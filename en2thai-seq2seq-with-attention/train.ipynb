{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e12a715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83f3363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0007b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef825dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwannaphong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2022-05-23 04:22:47.357376: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/project/thai_transliterate/cs-en-th-seq2seq-pythai/wandb/run-20220523_042244-1b6p9i4s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wannaphong/cs-en-th-seq2seq_attention/runs/1b6p9i4s\" target=\"_blank\">woven-microwave-3</a></strong> to <a href=\"https://wandb.ai/wannaphong/cs-en-th-seq2seq_attention\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/wannaphong/cs-en-th-seq2seq_attention/runs/1b6p9i4s?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4db2744d90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new experiment\n",
    "wandb.init(project=\"cs-en-th-seq2seq_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0ecb2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join('..','dataset','cs')\n",
    "\n",
    "train_filepaths = os.path.join(path_dataset,'train.tsv')\n",
    "dev_filepaths = os.path.join(path_dataset,'dev.tsv')\n",
    "test_filepaths = os.path.join(path_dataset,'test.tsv')\n",
    "\n",
    "train_df = pd.read_csv(train_filepaths,sep=\"\\t\")\n",
    "dev_df = pd.read_csv(dev_filepaths,sep=\"\\t\")\n",
    "test_df = pd.read_csv(test_filepaths,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e52b9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    input_texts = [str(i) for i in list(data_path['word'])]\n",
    "    target_texts =[str(i) for i in  list(data_path['roman'])]\n",
    "    return target_texts,input_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98778e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special characters\n",
    "UNK_token = '<UNK>'\n",
    "PAD_token = '<PAD>'\n",
    "START_token = '<start>'\n",
    "END_token = '<end>'\n",
    "MAX_LENGTH = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72a0e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self, name,char2index={},index2char={}, is_input=False):\n",
    "        self.name = name\n",
    "        self.characters = set()\n",
    "        self.n_chars = 0\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "\n",
    "        if is_input == True:\n",
    "            self.index2char = { 0: PAD_token, 1: UNK_token, 2: START_token, 3: END_token }\n",
    "            self.char2index = { ch:i for i, ch in self.index2char.items() } #reverse dictionary\n",
    "            self.n_chars = 4\n",
    "        else:\n",
    "            self.index2char = { 0: PAD_token, 1: START_token, 2: END_token }\n",
    "            self.char2index = { ch:i for i, ch in self.index2char.items() } #reverse dictionary\n",
    "            self.n_chars = 3\n",
    "        if char2index != {} and index2char != {}:\n",
    "            print(\"cat!!!\")\n",
    "            self.char2index = char2index\n",
    "            self.index2char = index2char\n",
    "            self.characters = set(list(self.char2index.keys()))\n",
    "\n",
    "    def addText(self, text):\n",
    "        for character in text:\n",
    "            self.addCharacter(character)\n",
    "    \n",
    "    def addCharacter(self, character):\n",
    "        if character not in self.char2index.keys():\n",
    "            self.char2index[character] = self.n_chars\n",
    "            self.index2char[self.n_chars] = character\n",
    "            self.n_chars += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f89b3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromText(lang, text):\n",
    "    \"\"\"returns indexes for all character given the text in the specified language\"\"\"\n",
    "    return [lang.char2index[char] for char in text]\n",
    "\n",
    "def tensorFromText(lang, text):\n",
    "    \"\"\"construct a tensor given the text in the specified language\"\"\"\n",
    "    indexes = indexesFromText(lang, text)\n",
    "    indexes.append(lang.char2index[END_token])\n",
    "    \n",
    "    no_padded_seq_length = len(indexes) # Number of characters in the text (including <END> token)\n",
    "    # Add padding token to make all tensors in the same length\n",
    "    for i in range(len(indexes), MAX_LENGTH): # padding\n",
    "        indexes.append(lang.char2index[PAD_token])\n",
    "        \n",
    "    return torch.tensor(indexes, dtype=torch.long), no_padded_seq_length\n",
    "\n",
    "def filterPair(p1, p2):\n",
    "    \"\"\"filter for the pair the both texts has length less than `MAX_LENGTH`\"\"\"\n",
    "    return len(p1) < MAX_LENGTH and len(p2) < MAX_LENGTH\n",
    "\n",
    "def tensorsFromPair(pair, lang1, lang2):\n",
    "    \"\"\"construct two tensors from a pair of source and target text specified by source and target language\"\"\"\n",
    "    input_tensor, input_length = tensorFromText(lang1, pair[0])\n",
    "    target_tensor, target_length = tensorFromText(lang2, pair[1])\n",
    "    return input_tensor, target_tensor, input_length, target_length\n",
    "\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        input_text, target_text, lang_th, lang_th_romanized = sample['input_text'], sample['target_text'],                                                               sample['lang_th'], sample['lang_th_romanized']\n",
    "\n",
    "        input_tensor, target_tensor, input_length, target_length = tensorsFromPair([input_text, target_text], \n",
    "                                                                                   lang_th, \n",
    "                                                                                   lang_th_romanized)\n",
    "        \n",
    "        return {\n",
    "                'input_text': input_text,\n",
    "                'target_text': target_text,\n",
    "                'input_length': input_length,\n",
    "                'target_length': target_length,\n",
    "                'input_tensor': input_tensor,\n",
    "                'target_tensor': target_tensor\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fae6eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThaiRomanizationDataset(Dataset):\n",
    "    \"\"\"Thai Romanization Dataset class\"\"\"\n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 config = {},\n",
    "                 transform=transforms.Compose([ ToTensor() ])):\n",
    "\n",
    "        input_texts, target_texts = load_data(data_path)\n",
    "        \n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.transform = transform\n",
    "        self.config=config\n",
    "        self.lang_th = Language('th', self.config['char_to_ix'],self.config['ix_to_char'],is_input=True)\n",
    "        self.lang_th_romanized = Language('th_romanized', self.config['target_char_to_ix'],self.config['ix_to_target_char'], is_input=False)\n",
    "        self.counter = Counter()\n",
    "        self.pairs = []\n",
    "        self.prepareData()\n",
    "\n",
    "    def prepareData(self):\n",
    "        for i in range(len(self.input_texts)):\n",
    "            \n",
    "            input_text = str(self.input_texts[i])\n",
    "            target_text = str(self.target_texts[i])\n",
    "            \n",
    "            # Count the number of input and target sequences with length `x`\n",
    "            self.counter.update({ \n",
    "                                  'len_input_{}'.format(len(input_text)): 1, \n",
    "                                  'len_target_{}'.format(len(target_text)): 1 \n",
    "                                })\n",
    "            \n",
    "            if filterPair(input_text, target_text):\n",
    "                self.pairs.append((input_text, target_text))\n",
    "                self.lang_th.addText(input_text)\n",
    "                self.lang_th_romanized.addText(target_text)    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = dict()\n",
    "        sample['input_text'] = self.pairs[idx][0]\n",
    "        sample['target_text'] = self.pairs[idx][1]\n",
    "        \n",
    "        sample['lang_th'] = self.lang_th\n",
    "        sample['lang_th_romanized'] = self.lang_th_romanized\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c4b07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    return torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ecbda480",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_model(\"thai2rom-pytorch-15.attn.v6.best_epoch-15.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e0de6fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: 'เ',\n",
       " 4: 'ก',\n",
       " 5: 'ษ',\n",
       " 6: 'ม',\n",
       " 7: ' ',\n",
       " 8: 'บ',\n",
       " 9: 'ุ',\n",
       " 10: 'ญ',\n",
       " 11: 'า',\n",
       " 12: 'ห',\n",
       " 13: 'ั',\n",
       " 14: 'น',\n",
       " 15: 'ิ',\n",
       " 16: 'โ',\n",
       " 17: 'ร',\n",
       " 18: 'ธ',\n",
       " 19: 'ศ',\n",
       " 20: 'ล',\n",
       " 21: 'พ',\n",
       " 22: '่',\n",
       " 23: 'ท',\n",
       " 24: 'ย',\n",
       " 25: '์',\n",
       " 26: 'ี',\n",
       " 27: 'ต',\n",
       " 28: 'อ',\n",
       " 29: '็',\n",
       " 30: 'ง',\n",
       " 31: 'แ',\n",
       " 32: 'ซ',\n",
       " 33: 'ส',\n",
       " 34: 'ว',\n",
       " 35: 'ะ',\n",
       " 36: 'ำ',\n",
       " 37: 'ด',\n",
       " 38: 'จ',\n",
       " 39: 'ค',\n",
       " 40: 'ณ',\n",
       " 41: 'ฑ',\n",
       " 42: 'ึ',\n",
       " 43: '้',\n",
       " 44: 'ู',\n",
       " 45: 'ฤ',\n",
       " 46: 'ฐ',\n",
       " 47: 'ข',\n",
       " 48: 'ฒ',\n",
       " 49: 'ฎ',\n",
       " 50: 'ป',\n",
       " 51: 'ไ',\n",
       " 52: 'ช',\n",
       " 53: 'ฏ',\n",
       " 54: 'ภ',\n",
       " 55: 'ฆ',\n",
       " 56: 'ฟ',\n",
       " 57: 'ฮ',\n",
       " 58: 'ื',\n",
       " 59: 'ผ',\n",
       " 60: '๋',\n",
       " 61: 'ใ',\n",
       " 62: '๊',\n",
       " 63: 'ถ',\n",
       " 64: 'ฌ',\n",
       " 65: 'ฉ',\n",
       " 66: 'ฝ',\n",
       " 67: 'ฬ',\n",
       " 68: '.',\n",
       " 69: 'ฦ',\n",
       " 70: '(',\n",
       " 71: ')',\n",
       " 72: 'ฯ',\n",
       " 73: '-',\n",
       " 74: 'ฃ',\n",
       " 75: 'ๆ',\n",
       " 76: '2',\n",
       " 77: 'ๅ',\n",
       " 78: 'ฅ',\n",
       " 79: 'ฺ',\n",
       " 80: 'ํ',\n",
       " 81: '5',\n",
       " 82: '3',\n",
       " 83: '6',\n",
       " 84: '4',\n",
       " 85: '7',\n",
       " 86: '1',\n",
       " 87: '0',\n",
       " 88: '!',\n",
       " 89: '9',\n",
       " 90: '8',\n",
       " 91: '\"',\n",
       " 92: '๙'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ix_to_target_char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94ce7f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat!!!\n",
      "cat!!!\n"
     ]
    }
   ],
   "source": [
    "thai_romanization_dataset = ThaiRomanizationDataset(train_df,config=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f05da935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<start>', 2: '<end>', 3: 'เ', 4: 'ก', 5: 'ษ', 6: 'ม', 7: ' ', 8: 'บ', 9: 'ุ', 10: 'ญ', 11: 'า', 12: 'ห', 13: 'ั', 14: 'น', 15: 'ิ', 16: 'โ', 17: 'ร', 18: 'ธ', 19: 'ศ', 20: 'ล', 21: 'พ', 22: '่', 23: 'ท', 24: 'ย', 25: '์', 26: 'ี', 27: 'ต', 28: 'อ', 29: '็', 30: 'ง', 31: 'แ', 32: 'ซ', 33: 'ส', 34: 'ว', 35: 'ะ', 36: 'ำ', 37: 'ด', 38: 'จ', 39: 'ค', 40: 'ณ', 41: 'ฑ', 42: 'ึ', 43: '้', 44: 'ู', 45: 'ฤ', 46: 'ฐ', 47: 'ข', 48: 'ฒ', 49: 'ฎ', 50: 'ป', 51: 'ไ', 52: 'ช', 53: 'ฏ', 54: 'ภ', 55: 'ฆ', 56: 'ฟ', 57: 'ฮ', 58: 'ื', 59: 'ผ', 60: '๋', 61: 'ใ', 62: '๊', 63: 'ถ', 64: 'ฌ', 65: 'ฉ', 66: 'ฝ', 67: 'ฬ', 68: '.', 69: 'ฦ', 70: '(', 71: ')', 72: 'ฯ', 73: '-', 74: 'ฃ', 75: 'ๆ', 76: '2', 77: 'ๅ', 78: 'ฅ', 79: 'ฺ', 80: 'ํ', 81: '5', 82: '3', 83: '6', 84: '4', 85: '7', 86: '1', 87: '0', 88: '!', 89: '9', 90: '8', 91: '\"', 92: '๙'}\n"
     ]
    }
   ],
   "source": [
    "print(thai_romanization_dataset.lang_th_romanized.index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd04ba2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<UNK>', 2: '<start>', 3: '<end>', 4: 'v', 5: 'x', 6: 'z', 7: 'é', 8: 'q', 9: \"'\", 10: 'j', 11: 'ฺ', 12: 'è', 13: '岸', 14: '田', 15: '文', 16: '雄', 17: '.', 18: '–', 19: 'g', 20: 'w', 21: 'l', 22: 'd', 23: 'c', 24: 'y', 25: 'f', 26: '1', 27: '(', 28: ')', 29: '2', 30: '5', 31: '3', 32: '6', 33: '4', 34: '7', 35: '0', 36: '!', 37: '9', 38: '8', 39: '\"'}\n"
     ]
    }
   ],
   "source": [
    "print(thai_romanization_dataset.lang_th.index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0e8ece70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  3037\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "N = len(thai_romanization_dataset)\n",
    "\n",
    "print('Number of samples: ', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85bbd12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_loader = torch.utils.data.DataLoader(\n",
    "                                             thai_romanization_dataset,\n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faa7fd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train mini-batches 12\n"
     ]
    }
   ],
   "source": [
    "print('Number of train mini-batches', len(train_dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "65961776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Seq2Seq Model architecture\n",
    "\n",
    "# ## 1. Encoder\n",
    "\n",
    "# Encoder \n",
    "#     - Embedding layer :(vocaburay_size, embedding_size) \n",
    "#         Input: (batch_size, sequence_length)\n",
    "#         Output: (batch_size, sequence_length, embebeding_size)\n",
    "#       \n",
    "#     - Bi-LSTM layer : (input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         Input: (input=(batch_size, seq_len, embebeding_size),  hidden)\n",
    "#         Output: (output=(batch_size, seq_len, hidden_size),\n",
    "#                  (h_n, c_n))\n",
    "#      \n",
    "#      \n",
    "# __Steps:__\n",
    "# \n",
    "# 1. Receives a batch of source sequences (batch_size, MAX_LENGTH) and a 1-D array of the length for each sequence (batch_size).\n",
    "#      \n",
    "# 2. Sort sequences in the batch by sequence length (number of tokens in the sequence where <PAD> token is excluded).\n",
    "# \n",
    "# 3. Feed the batch of sorted sequences into the Embedding Layer to maps source character indices into vectors. (batch_size,  sequence_length, embebeding_size)\n",
    "# \n",
    "# 4. Use `pack_padded_sequence` to let LSTM packed input with same length at time step $t$ together. This will reduce time required for training by avoid feeding `<PAD>` token to the LSTMs.\n",
    "# \n",
    "# \n",
    "# 5. Returns LSTM outputs in the unsorted order, and the LSTM hidden state vectors.\n",
    "#      \n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size, dropout=0.5):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.character_embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, \n",
    "                            hidden_size=hidden_size // 2, \n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, sequences, sequences_lengths):\n",
    "        batch_size = sequences.size(0)\n",
    "        self.hidden = self.init_hidden(batch_size) # batch_size\n",
    "\n",
    "        # sequences :(batch_size, sequence_length=MAX_LENGTH)\n",
    "        # sequences_lengths: (batch_size)  # an 1-D indicating length of each sequence (excluded <PAD> token) in `seq`\n",
    "        \n",
    "        # 1. Firstly we sort `sequences_lengths` according to theirs values and keep list of indexes to perform sorting\n",
    "        sequences_lengths = np.sort(sequences_lengths)[::-1] # sort in ascending order and reverse it\n",
    "        index_sorted = np.argsort(-sequences_lengths) # use negation in sort in descending order\n",
    "        index_unsort = np.argsort(index_sorted) # to unsorted sequence\n",
    "        \n",
    "        \n",
    "        # 2. Then, we change position of sequence in `sequences` according to `index_sorted`\n",
    "        index_sorted = torch.from_numpy(index_sorted)\n",
    "        sequences = sequences.index_select(0, index_sorted)\n",
    "        \n",
    "        # 3. Feed to Embedding Layer\n",
    "        \n",
    "        sequences = self.character_embedding(sequences)\n",
    "        sequences = self.dropout(sequences)\n",
    "        \n",
    "#         print('sequences',sequences.size(), sequences)\n",
    "            \n",
    "        # 3. Use function: pack_padded_sequence to let LSTM packed input with same length at time step T together\n",
    "        \n",
    "        # Quick fix: Use seq_len.copy(), instead of seq_len to fix `Torch.from_numpy not support negative strides`\n",
    "        # ndarray.copy() will alocate new memory for numpy array which make it normal, I mean the stride is not negative any more.\n",
    "\n",
    "        sequences_packed = nn.utils.rnn.pack_padded_sequence(sequences,\n",
    "                                                             sequences_lengths.copy(),\n",
    "                                                             batch_first=True)\n",
    "#         print('sequences_packed', sequences_packed)\n",
    "\n",
    "        # 4. Feed to LSTM\n",
    "        sequences_output, self.hidden = self.lstm(sequences_packed, self.hidden)\n",
    "        \n",
    "        # 5. Unpack\n",
    "        sequences_output, _ = nn.utils.rnn.pad_packed_sequence(sequences_output, batch_first=True)\n",
    "\n",
    "        # 6. Un-sort by length\n",
    "        index_unsort = torch.from_numpy(index_unsort)\n",
    "        sequences_output = sequences_output.index_select(0, Variable(index_unsort))\n",
    "\n",
    "#         print('hidden shape', self.hidden[0].shape, self.hidden[0], self.hidden[1].shape, self.hidden[1])\n",
    "        return sequences_output, self.hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros([2, batch_size, self.hidden_size // 2], requires_grad=True)\n",
    "        c_0 = torch.zeros([2, batch_size, self.hidden_size // 2], requires_grad=True)\n",
    "        \n",
    "        return (h_0, c_0)\n",
    "    \n",
    "def save_model(name, epoch, loss, model):\n",
    "    print('Save model at epoch ', epoch)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'char_to_ix': thai_romanization_dataset.lang_th.char2index,\n",
    "        'ix_to_char': thai_romanization_dataset.lang_th.index2char,\n",
    "        'target_char_to_ix': thai_romanization_dataset.lang_th_romanized.char2index,\n",
    "        'ix_to_target_char':thai_romanization_dataset.lang_th_romanized.index2char,\n",
    "        'encoder_params': (INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, ENC_DROPOUT), \n",
    "        'decoder_params': (OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "        \n",
    "    }, \"{}.best_epoch-{}.tar\".format(name, epoch))\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "# ## Decoder\n",
    "\n",
    "#    \n",
    "# Decoder architecture\n",
    "# \n",
    "#     - Embedding layer :(vocabulary_size, embebeding_size)\n",
    "#         Input: (batch_size, sequence_length=1)\n",
    "#         Output: (batch_size, sequence_length=1, embebeding_size)\n",
    "#     - RNN layer :input_size=embebeding_size, hidden_size, num_layers, batch_first=True)\n",
    "#         Input: (input=(batch_size, input_size=embedding_dimension), hidden:tuple=encoder_hidden\n",
    "#         Output: (batch_size, seq_len, hidden_size), (h_n, c_n)\n",
    "#     - Attention Layer: (in_features=hidden_size, out_features=hidden_size, bias=True)\n",
    "#     - Linear Layer: (in_features, out_features=vocabulary_size)\n",
    "#         Input: (batch_size, hidden_size)\n",
    "#         Output: (batch_size, vocabulary_size)\n",
    "#     \n",
    "#     - Softmax layer\n",
    "#         Input: (batch_size, vocabulary_size)\n",
    "#         Output: (batch_size, vocabulary_size)\n",
    "# \n",
    "# \n",
    "# \n",
    "# For the Attention mechanishm in the Decoder, Luong-style attention [[Luong et. al (2015)](https://arxiv.org/abs/1508.04025)] is used. \n",
    "# \n",
    "# \n",
    "# \n",
    "# __Steps:__\n",
    "# \n",
    "# 1. Receives a batch of <start> token (batch_size, 1) and a batch of Encoder's hidden state.\n",
    "#      \n",
    "# 2. Embed input into vectors.\n",
    "# \n",
    "# 3. Feed vectors from (2) to the LSTM.\n",
    "# \n",
    "# 4. Feed the output of LSTM at time step $t_1$ and Encoder output to the Attention Layer.\n",
    "# \n",
    "# 5. Attention layer, returns weights for Encoder's hidden states in every time step (masked out the time step with <PAD> token), then multiply with Encoder's hidden states to obtain a context vector\n",
    "#     \n",
    "# 6. Concatenate both decoder hidden state and the context vector, feed to a linear layer, and return its output.\n",
    "# \n",
    "# 7. Decoder then returns, final output, decoder's hidden state, attention weights, and context vector at time step $t$\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: B x 1 x h ; \n",
    "        # encoder_outputs: B x S x h\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        if self.method == 'dot':\n",
    "            attn_energies = torch.bmm(encoder_outputs, hidden.transpose(1, 2)).squeeze(2)  # B x S\n",
    "        elif self.method == 'general':\n",
    "            attn_energies = self.attn(encoder_outputs.view(-1, encoder_outputs.size(-1)))  # (B * S) x h\n",
    "            attn_energies = torch.bmm(attn_energies.view(*encoder_outputs.size()),\n",
    "                                      hidden.transpose(1, 2)).squeeze(2)  # B x S\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.attn(\n",
    "                torch.cat((hidden.expand(*encoder_outputs.size()), encoder_outputs), 2))  # B x S x h\n",
    "            attn_energies = torch.bmm(attn_energies,\n",
    "                                      self.other.unsqueeze(0).expand(*hidden.size()).transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        attn_energies = attn_energies.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1\n",
    "        return F.softmax(attn_energies, 1)\n",
    "\n",
    "class AttentionDecoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size, dropout=0.5):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.character_embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size + self.hidden_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            bidirectional=False,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.attn = Attn(method=\"general\", hidden_size=self.hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size * 2, vocabulary_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, input, last_hidden, last_context, encoder_outputs, mask):\n",
    "        \"\"\"\"Defines the forward computation of the decoder\"\"\"\n",
    "        # input: (B, 1) ,\n",
    "        # last_hidden: (num_layers * num_directions, B, hidden_dim)\n",
    "        # last_context: (B, 1, hidden_dim)\n",
    "        # encoder_outputs: (B, S, hidden_dim)\n",
    "        \n",
    "        embedded = self.character_embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # embedded: (batch_size, emb_dim)\n",
    "        rnn_input = torch.cat((embedded, last_context), 2)\n",
    "\n",
    "        output, hidden = self.lstm(rnn_input, last_hidden)        \n",
    "        attn_weights = self.attn(output, encoder_outputs, mask)  # B x S\n",
    "    \n",
    "        #  context = (B, 1, S) x (B, S, hidden_dim)\n",
    "        #  context = (B, 1, hidden_dim)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)  \n",
    "        \n",
    "        output = torch.cat((context.squeeze(1), output.squeeze(1)), 1)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, hidden, context, attn_weights\n",
    "\n",
    "\n",
    "# ## Seq2Seq model\n",
    "# \n",
    "# This class encapsulate _Decoder_ and _Encoder_ class.\n",
    "# \n",
    "# __Steps:__\n",
    "# \n",
    "# 1. The input sequcence $X$ is fed into the encoder to receive one hidden state vector.\n",
    "# \n",
    "# 2. The initial decoder hidden state is set to be the hidden state vector of the encoder\n",
    "# \n",
    "# 3. Add a batch of `<start>` tokens (batch_size, 1) as the first input $y_1$\n",
    "#     \n",
    "# 4. Then, decode within a loop:\n",
    "#     - Inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector $z$ into the decoder\n",
    "#     - Receiveing a prediction $\\hat{y}$ and a new hidden state $s_t$\n",
    "#     - Then, either use teacher forcing to let groundtruth target character as the input for the decoder at time step $t+1$, or let the result from decoder as the input for the next time step.\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module): \n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = 0\n",
    "\n",
    "        assert encoder.hidden_size == decoder.hidden_size\n",
    "    \n",
    "    def create_mask(self, source_seq):\n",
    "        mask = (source_seq != self.pad_idx)\n",
    "        return mask\n",
    "        \n",
    "  \n",
    "    def forward(self, source_seq, source_seq_len, target_seq, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                - source_seq: (batch_size x MAX_LENGTH) \n",
    "                - source_seq_len: (batch_size x 1)\n",
    "                - target_seq: (batch_size x MAX_LENGTH)\n",
    "\n",
    "            Returns\n",
    "        \"\"\"\n",
    "        batch_size = source_seq.size(0)\n",
    "        start_token = thai_romanization_dataset.lang_th_romanized.char2index[\"<start>\"]\n",
    "        end_token = thai_romanization_dataset.lang_th_romanized.char2index[\"<end>\"]\n",
    "        max_len = MAX_LENGTH\n",
    "        target_vocab_size = self.decoder.vocabulary_size\n",
    "\n",
    "        # init a tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, target_vocab_size)\n",
    "        \n",
    "        if target_seq is None:\n",
    "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
    "            inference = True\n",
    "        else:\n",
    "            inference = False\n",
    "\n",
    "    \n",
    "        # feed mini-batch source sequences into the `Encoder`\n",
    "        encoder_outputs, encoder_hidden = self.encoder(source_seq, source_seq_len)\n",
    "\n",
    "        # create a Tensor of first input for the decoder\n",
    "        decoder_input = torch.tensor([[start_token] * batch_size]).view(batch_size, 1)\n",
    "        \n",
    "        # Initiate decoder output as the last state encoder's hidden state\n",
    "        decoder_hidden_0 = torch.cat([encoder_hidden[0][0], encoder_hidden[0][1]], dim=1).unsqueeze(dim=0)\n",
    "        decoder_hidden_1 = torch.cat([encoder_hidden[1][0], encoder_hidden[1][1]], dim=1).unsqueeze(dim=0)\n",
    "        decoder_hidden = (decoder_hidden_0, decoder_hidden_1) # (hidden state, cell state)\n",
    "\n",
    "        # define a context vector\n",
    "        decoder_context = Variable(torch.zeros(encoder_outputs.size(0), encoder_outputs.size(2))).unsqueeze(1)\n",
    "        \n",
    "        max_source_len = encoder_outputs.size(1)\n",
    "        mask = self.create_mask(source_seq[:, 0:max_source_len])\n",
    "            \n",
    "       \n",
    "        for di in range(max_len):\n",
    "            decoder_output, decoder_hidden, decoder_context, attn_weights = self.decoder(decoder_input,\n",
    "                                                                                    decoder_hidden,\n",
    "                                                                                    decoder_context,\n",
    "                                                                                    encoder_outputs,\n",
    "                                                                                    mask)\n",
    "            # decoder_output: (batch_size, target_vocab_size)\n",
    "\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            outputs[di] = decoder_output\n",
    "    \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "\n",
    "            decoder_input = target_seq[:, di].reshape(batch_size, 1) if teacher_force else topi.detach() \n",
    "\n",
    "            if inference and decoder_input == end_token:\n",
    "                return outputs[:di]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9663d4d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c588f0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, ENC_DROPOUT = data['encoder_params']\n",
    "OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, DEC_DROPOUT = data['decoder_params']\n",
    "\n",
    "_encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM)\n",
    "_decoder = AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM)\n",
    "\n",
    "#model = Seq2Seq(_encoder, _decoder)\n",
    "model = Seq2Seq(_encoder, _decoder)\n",
    "model.load_state_dict(data['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f15b8ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 15,\n",
       " 'loss': 0.7513151974250109,\n",
       " 'model_state_dict': OrderedDict([('encoder.character_embedding.weight',\n",
       "               tensor([[ 0.0586,  0.0341, -0.0138,  ..., -0.0127, -0.0296, -0.0112],\n",
       "                       [-0.0092,  0.0035,  0.0047,  ..., -0.0093, -0.0133,  0.0025],\n",
       "                       [ 0.0036, -0.0039,  0.0066,  ..., -0.0067, -0.0026, -0.0011],\n",
       "                       ...,\n",
       "                       [ 0.0500,  0.1516, -0.0418,  ..., -0.0193,  0.0725, -0.0079],\n",
       "                       [ 0.1959,  0.0318, -0.1099,  ..., -0.0908,  0.1820, -0.0528],\n",
       "                       [-0.0464,  0.1692, -0.1842,  ..., -0.1329, -0.0389,  0.0242]])),\n",
       "              ('encoder.lstm.weight_ih_l0',\n",
       "               tensor([[ 0.0177, -0.0891, -0.1113,  ...,  0.1338,  0.0081,  0.2163],\n",
       "                       [-0.1308,  0.0723,  0.1978,  ..., -0.0037,  0.2551, -0.0477],\n",
       "                       [ 0.0582, -0.1368, -0.1186,  ...,  0.0625, -0.0146,  0.0463],\n",
       "                       ...,\n",
       "                       [ 0.0461,  0.0103,  0.1761,  ..., -0.0824, -0.0349, -0.0083],\n",
       "                       [-0.0257,  0.0140,  0.1017,  ..., -0.0048, -0.0319, -0.0026],\n",
       "                       [-0.0184,  0.0333, -0.0416,  ..., -0.0780,  0.1329, -0.0825]])),\n",
       "              ('encoder.lstm.weight_hh_l0',\n",
       "               tensor([[ 0.1145,  0.2389,  0.0856,  ..., -0.0963, -0.0206, -0.0539],\n",
       "                       [-0.0819,  0.7140, -0.1650,  ..., -0.0571, -0.0093, -0.1533],\n",
       "                       [ 0.0562, -0.0553,  0.1464,  ..., -0.0775,  0.0510, -0.2568],\n",
       "                       ...,\n",
       "                       [ 0.0971,  0.2018,  0.0209,  ...,  0.0713, -0.2285, -0.2664],\n",
       "                       [-0.1598,  0.4074, -0.1513,  ...,  0.2715, -0.3143, -0.3559],\n",
       "                       [ 0.0382,  0.0745,  0.0278,  ..., -0.0732, -0.1378,  0.0644]])),\n",
       "              ('encoder.lstm.bias_ih_l0',\n",
       "               tensor([-0.1315, -0.0932, -0.2019, -0.1767,  0.0740, -0.1134, -0.1368,  0.0873,\n",
       "                       -0.0665, -0.2649, -0.1350, -0.1631, -0.0469, -0.0634, -0.2888, -0.1323,\n",
       "                       -0.1158, -0.0534, -0.4383, -0.2089,  0.0018,  0.0258,  0.0369, -0.0258,\n",
       "                       -0.0839, -0.0399,  0.2697, -0.2755, -0.0787,  0.0260, -0.2866, -0.2492,\n",
       "                       -0.2199, -0.0585, -0.1548,  0.2995, -0.1065, -0.0926, -0.2232, -0.0264,\n",
       "                       -0.1499, -0.1445, -0.0193, -0.0445, -0.1254, -0.0556, -0.1525, -0.2523,\n",
       "                        0.1750, -0.2981, -0.1394, -0.0513, -0.3908, -0.0625, -0.2508, -0.3411,\n",
       "                       -0.2797, -0.1036, -0.0802, -0.2193, -0.2923,  0.0577, -0.1135, -0.2049,\n",
       "                       -0.2248, -0.0907, -0.4160, -0.0184, -0.0728, -0.4075, -0.1739, -0.2716,\n",
       "                       -0.3152, -0.2445, -0.0585, -0.1817,  0.0474, -0.2509, -0.1806, -0.0246,\n",
       "                        0.0134, -0.2224, -0.0340, -0.0703,  0.2546, -0.2201, -0.1201, -0.1278,\n",
       "                        0.0584, -0.0339, -0.3317, -0.1510, -0.2157, -0.1164, -0.2840, -0.0202,\n",
       "                        0.1121, -0.1623,  0.0175,  0.2204, -0.2898, -0.1028, -0.2479, -0.2771,\n",
       "                       -0.0989, -0.2178, -0.1977, -0.0159,  0.0215, -0.0638,  0.0402, -0.0217,\n",
       "                       -0.2625, -0.2135,  0.0804,  0.0152, -0.0647, -0.1459, -0.4486, -0.0786,\n",
       "                       -0.0977, -0.1885, -0.1211, -0.1093,  0.1399, -0.3877,  0.1165, -0.1627,\n",
       "                       -0.1010, -0.0941,  0.0060,  0.0160,  0.1892,  0.0042, -0.2302,  0.0979,\n",
       "                        0.1815,  0.1241,  0.2962, -0.1493, -0.0587,  0.1663,  0.0470, -0.0147,\n",
       "                       -0.1496,  0.0559, -0.2166, -0.0320, -0.1513,  0.1748, -0.2577,  0.0036,\n",
       "                       -0.3056,  0.1396, -0.0300,  0.2159,  0.0483, -0.2405, -0.0582, -0.1786,\n",
       "                        0.0063, -0.0566,  0.0880, -0.0023,  0.1306,  0.1023,  0.1290, -0.0291,\n",
       "                        0.0836, -0.1026, -0.0401, -0.1083,  0.2685, -0.1194, -0.1374, -0.1056,\n",
       "                       -0.3539, -0.0396,  0.0272,  0.0972, -0.0334,  0.1024, -0.0626, -0.1177,\n",
       "                        0.1270, -0.0734, -0.1255,  0.1571,  0.0825,  0.0798,  0.0456, -0.0379,\n",
       "                        0.0511, -0.1184, -0.0906, -0.0475,  0.0397, -0.0967, -0.0719, -0.0089,\n",
       "                       -0.0425,  0.0429,  0.1117,  0.0095,  0.0358,  0.1785, -0.0116,  0.0568,\n",
       "                       -0.1145, -0.0341,  0.0372,  0.0620, -0.0584,  0.2590, -0.0943,  0.0470,\n",
       "                       -0.0722,  0.1307,  0.0023,  0.0066, -0.0123, -0.0695, -0.1388,  0.2146,\n",
       "                       -0.2286, -0.1023,  0.0201, -0.1485,  0.1048,  0.0078, -0.0506,  0.0771,\n",
       "                       -0.1881,  0.0674,  0.0798, -0.0029, -0.0599, -0.0228, -0.1277,  0.0323,\n",
       "                       -0.0681, -0.1158, -0.2336,  0.0560,  0.0905, -0.0567, -0.0540, -0.1187,\n",
       "                       -0.1746, -0.0298, -0.1193, -0.0768,  0.2457, -0.0486, -0.0278,  0.1227,\n",
       "                        0.1072, -0.2086,  0.0340, -0.2076, -0.0472,  0.1446, -0.1422, -0.1708,\n",
       "                        0.3696, -0.1677, -0.2050,  0.1859, -0.1153, -0.1022, -0.0558,  0.0849,\n",
       "                       -0.0273,  0.1501,  0.0153, -0.2146,  0.1398,  0.1165, -0.0041, -0.0629,\n",
       "                        0.5721,  0.0682, -0.0280,  0.1227, -0.2317,  0.1769, -0.1168, -0.0074,\n",
       "                        0.0503,  0.1026,  0.1718,  0.0540, -0.0057,  0.0957,  0.0545,  0.2833,\n",
       "                       -0.0552,  0.1898,  0.0439,  0.1140,  0.1340,  0.2848,  0.1160,  0.0828,\n",
       "                       -0.0016, -0.0763, -0.1189,  0.1486,  0.2613, -0.0268,  0.0112,  0.0068,\n",
       "                        0.1384, -0.0537, -0.0489, -0.1380,  0.0219, -0.1518,  0.0233,  0.2338,\n",
       "                        0.0716,  0.0673, -0.1602, -0.2045, -0.1442, -0.3752, -0.1039,  0.0695,\n",
       "                        0.0045,  0.0300, -0.0353, -0.0372,  0.2972,  0.0843, -0.1026, -0.2278,\n",
       "                        0.0714, -0.0128, -0.2102,  0.1571, -0.0404, -0.1118, -0.2682, -0.0012,\n",
       "                        0.0087, -0.1222, -0.0147, -0.1406,  0.0104, -0.0913, -0.0204, -0.1840,\n",
       "                       -0.0119,  0.1960, -0.0376, -0.2016, -0.0615,  0.2164,  0.0318, -0.0632,\n",
       "                       -0.0113, -0.1434,  0.0845,  0.1870, -0.2575,  0.1016,  0.0118, -0.0708,\n",
       "                       -0.0320,  0.2923,  0.6722, -0.1762, -0.1365,  0.0534, -0.0962,  0.3303,\n",
       "                       -0.0259,  0.0970,  0.3057,  0.1688,  0.0980, -0.0254,  0.2047, -0.1218,\n",
       "                       -0.2327, -0.2673, -0.2563,  0.1828,  0.1522, -0.2728, -0.0113,  0.0837,\n",
       "                       -0.0530, -0.0559,  0.0948, -0.2218, -0.5592, -0.3814, -0.0744, -0.2926,\n",
       "                       -0.1619, -0.3022, -0.3712,  0.0544,  0.0385, -0.6334, -0.1702,  0.0881,\n",
       "                       -0.0772,  0.0466,  0.2265, -0.0371, -0.1287,  0.3164, -0.3202, -0.0068,\n",
       "                       -0.6247, -0.3762, -0.0418,  0.2541, -0.3179, -0.3672, -0.1364, -0.0451,\n",
       "                       -0.3911, -0.1432, -0.2767,  0.0937,  0.2409, -0.0669, -0.1263, -0.3100,\n",
       "                       -0.0739, -0.1705, -0.0101,  0.0610,  0.1542, -0.3138, -0.4164, -0.4075,\n",
       "                       -0.1168,  0.0348, -0.2163, -0.1265, -0.2603, -0.3366, -0.4652, -0.2186,\n",
       "                       -0.2293, -0.0383, -0.4097,  0.2862, -0.3255,  0.0079, -0.3406, -0.3679,\n",
       "                       -0.2071, -0.2336, -0.2712, -0.3019,  0.1528, -0.1232, -0.1928, -0.1247,\n",
       "                        0.0368, -0.2760,  0.0402, -0.0740,  0.0893,  0.0428, -0.1598, -0.2012,\n",
       "                        0.0417, -0.0329, -0.2563, -0.5327, -0.3786, -0.2797,  0.1352, -0.0166,\n",
       "                        0.3629, -0.2754,  0.0204,  0.3072,  0.0235, -0.3481, -0.3230, -0.3151,\n",
       "                        0.1244, -0.1324, -0.1267, -0.0847,  0.2874,  0.0254, -0.1692, -0.0533,\n",
       "                       -0.3431, -0.2247, -0.0175,  0.0094,  0.2535, -0.0135,  0.2826, -0.0417,\n",
       "                        0.2505, -0.1251,  0.2642, -0.0434, -0.4974, -0.4455, -0.6387, -0.2463])),\n",
       "              ('encoder.lstm.bias_hh_l0',\n",
       "               tensor([-0.1315, -0.0932, -0.2019, -0.1767,  0.0740, -0.1134, -0.1368,  0.0873,\n",
       "                       -0.0665, -0.2649, -0.1350, -0.1631, -0.0469, -0.0634, -0.2888, -0.1323,\n",
       "                       -0.1158, -0.0534, -0.4383, -0.2089,  0.0018,  0.0258,  0.0369, -0.0258,\n",
       "                       -0.0839, -0.0399,  0.2697, -0.2755, -0.0787,  0.0260, -0.2866, -0.2492,\n",
       "                       -0.2199, -0.0585, -0.1548,  0.2995, -0.1065, -0.0926, -0.2232, -0.0264,\n",
       "                       -0.1499, -0.1445, -0.0193, -0.0445, -0.1254, -0.0556, -0.1525, -0.2523,\n",
       "                        0.1750, -0.2981, -0.1394, -0.0513, -0.3908, -0.0625, -0.2508, -0.3411,\n",
       "                       -0.2797, -0.1036, -0.0802, -0.2193, -0.2923,  0.0577, -0.1135, -0.2049,\n",
       "                       -0.2248, -0.0907, -0.4160, -0.0184, -0.0728, -0.4075, -0.1739, -0.2716,\n",
       "                       -0.3152, -0.2445, -0.0585, -0.1817,  0.0474, -0.2509, -0.1806, -0.0246,\n",
       "                        0.0134, -0.2224, -0.0340, -0.0703,  0.2546, -0.2201, -0.1201, -0.1278,\n",
       "                        0.0584, -0.0339, -0.3317, -0.1510, -0.2157, -0.1164, -0.2840, -0.0202,\n",
       "                        0.1121, -0.1623,  0.0175,  0.2204, -0.2898, -0.1028, -0.2479, -0.2771,\n",
       "                       -0.0989, -0.2178, -0.1977, -0.0159,  0.0215, -0.0638,  0.0402, -0.0217,\n",
       "                       -0.2625, -0.2135,  0.0804,  0.0152, -0.0647, -0.1459, -0.4486, -0.0786,\n",
       "                       -0.0977, -0.1885, -0.1211, -0.1093,  0.1399, -0.3877,  0.1165, -0.1627,\n",
       "                       -0.1010, -0.0941,  0.0060,  0.0160,  0.1892,  0.0042, -0.2302,  0.0979,\n",
       "                        0.1815,  0.1241,  0.2962, -0.1493, -0.0587,  0.1663,  0.0470, -0.0147,\n",
       "                       -0.1496,  0.0559, -0.2166, -0.0320, -0.1513,  0.1748, -0.2577,  0.0036,\n",
       "                       -0.3056,  0.1396, -0.0300,  0.2159,  0.0483, -0.2405, -0.0582, -0.1786,\n",
       "                        0.0063, -0.0566,  0.0880, -0.0023,  0.1306,  0.1023,  0.1290, -0.0291,\n",
       "                        0.0836, -0.1026, -0.0401, -0.1083,  0.2685, -0.1194, -0.1374, -0.1056,\n",
       "                       -0.3539, -0.0396,  0.0272,  0.0972, -0.0334,  0.1024, -0.0626, -0.1177,\n",
       "                        0.1270, -0.0734, -0.1255,  0.1571,  0.0825,  0.0798,  0.0456, -0.0379,\n",
       "                        0.0511, -0.1184, -0.0906, -0.0475,  0.0397, -0.0967, -0.0719, -0.0089,\n",
       "                       -0.0425,  0.0429,  0.1117,  0.0095,  0.0358,  0.1785, -0.0116,  0.0568,\n",
       "                       -0.1145, -0.0341,  0.0372,  0.0620, -0.0584,  0.2590, -0.0943,  0.0470,\n",
       "                       -0.0722,  0.1307,  0.0023,  0.0066, -0.0123, -0.0695, -0.1388,  0.2146,\n",
       "                       -0.2286, -0.1023,  0.0201, -0.1485,  0.1048,  0.0078, -0.0506,  0.0771,\n",
       "                       -0.1881,  0.0674,  0.0798, -0.0029, -0.0599, -0.0228, -0.1277,  0.0323,\n",
       "                       -0.0681, -0.1158, -0.2336,  0.0560,  0.0905, -0.0567, -0.0540, -0.1187,\n",
       "                       -0.1746, -0.0298, -0.1193, -0.0768,  0.2457, -0.0486, -0.0278,  0.1227,\n",
       "                        0.1072, -0.2086,  0.0340, -0.2076, -0.0472,  0.1446, -0.1422, -0.1708,\n",
       "                        0.3696, -0.1677, -0.2050,  0.1859, -0.1153, -0.1022, -0.0558,  0.0849,\n",
       "                       -0.0273,  0.1501,  0.0153, -0.2146,  0.1398,  0.1165, -0.0041, -0.0629,\n",
       "                        0.5721,  0.0682, -0.0280,  0.1227, -0.2317,  0.1769, -0.1168, -0.0074,\n",
       "                        0.0503,  0.1026,  0.1718,  0.0540, -0.0057,  0.0957,  0.0545,  0.2833,\n",
       "                       -0.0552,  0.1898,  0.0439,  0.1140,  0.1340,  0.2848,  0.1160,  0.0828,\n",
       "                       -0.0016, -0.0763, -0.1189,  0.1486,  0.2613, -0.0268,  0.0112,  0.0068,\n",
       "                        0.1384, -0.0537, -0.0489, -0.1380,  0.0219, -0.1518,  0.0233,  0.2338,\n",
       "                        0.0716,  0.0673, -0.1602, -0.2045, -0.1442, -0.3752, -0.1039,  0.0695,\n",
       "                        0.0045,  0.0300, -0.0353, -0.0372,  0.2972,  0.0843, -0.1026, -0.2278,\n",
       "                        0.0714, -0.0128, -0.2102,  0.1571, -0.0404, -0.1118, -0.2682, -0.0012,\n",
       "                        0.0087, -0.1222, -0.0147, -0.1406,  0.0104, -0.0913, -0.0204, -0.1840,\n",
       "                       -0.0119,  0.1960, -0.0376, -0.2016, -0.0615,  0.2164,  0.0318, -0.0632,\n",
       "                       -0.0113, -0.1434,  0.0845,  0.1870, -0.2575,  0.1016,  0.0118, -0.0708,\n",
       "                       -0.0320,  0.2923,  0.6722, -0.1762, -0.1365,  0.0534, -0.0962,  0.3303,\n",
       "                       -0.0259,  0.0970,  0.3057,  0.1688,  0.0980, -0.0254,  0.2047, -0.1218,\n",
       "                       -0.2327, -0.2673, -0.2563,  0.1828,  0.1522, -0.2728, -0.0113,  0.0837,\n",
       "                       -0.0530, -0.0559,  0.0948, -0.2218, -0.5592, -0.3814, -0.0744, -0.2926,\n",
       "                       -0.1619, -0.3022, -0.3712,  0.0544,  0.0385, -0.6334, -0.1702,  0.0881,\n",
       "                       -0.0772,  0.0466,  0.2265, -0.0371, -0.1287,  0.3164, -0.3202, -0.0068,\n",
       "                       -0.6247, -0.3762, -0.0418,  0.2541, -0.3179, -0.3672, -0.1364, -0.0451,\n",
       "                       -0.3911, -0.1432, -0.2767,  0.0937,  0.2409, -0.0669, -0.1263, -0.3100,\n",
       "                       -0.0739, -0.1705, -0.0101,  0.0610,  0.1542, -0.3138, -0.4164, -0.4075,\n",
       "                       -0.1168,  0.0348, -0.2163, -0.1265, -0.2603, -0.3366, -0.4652, -0.2186,\n",
       "                       -0.2293, -0.0383, -0.4097,  0.2862, -0.3255,  0.0079, -0.3406, -0.3679,\n",
       "                       -0.2071, -0.2336, -0.2712, -0.3019,  0.1528, -0.1232, -0.1928, -0.1247,\n",
       "                        0.0368, -0.2760,  0.0402, -0.0740,  0.0893,  0.0428, -0.1598, -0.2012,\n",
       "                        0.0417, -0.0329, -0.2563, -0.5327, -0.3786, -0.2797,  0.1352, -0.0166,\n",
       "                        0.3629, -0.2754,  0.0204,  0.3072,  0.0235, -0.3481, -0.3230, -0.3151,\n",
       "                        0.1244, -0.1324, -0.1267, -0.0847,  0.2874,  0.0254, -0.1692, -0.0533,\n",
       "                       -0.3431, -0.2247, -0.0175,  0.0094,  0.2535, -0.0135,  0.2826, -0.0417,\n",
       "                        0.2505, -0.1251,  0.2642, -0.0434, -0.4974, -0.4455, -0.6387, -0.2463])),\n",
       "              ('encoder.lstm.weight_ih_l0_reverse',\n",
       "               tensor([[-0.0189, -0.1047, -0.1573,  ...,  0.0293,  0.0983,  0.0035],\n",
       "                       [-0.1447, -0.0140,  0.1292,  ..., -0.1112, -0.2343,  0.1428],\n",
       "                       [-0.0084,  0.0280, -0.1534,  ..., -0.0070,  0.2161,  0.0396],\n",
       "                       ...,\n",
       "                       [ 0.2717, -0.1202,  0.0213,  ...,  0.1556,  0.1276, -0.0647],\n",
       "                       [ 0.2916, -0.1016,  0.0263,  ...,  0.2276,  0.0677,  0.0899],\n",
       "                       [ 0.0241,  0.0643,  0.1433,  ..., -0.0175,  0.3334, -0.2047]])),\n",
       "              ('encoder.lstm.weight_hh_l0_reverse',\n",
       "               tensor([[ 0.1468,  0.1686,  0.1093,  ..., -0.0065,  0.5394,  0.1835],\n",
       "                       [ 0.7037, -0.1062,  0.2929,  ..., -0.3321,  0.1758, -0.2298],\n",
       "                       [-0.2182,  0.0167,  0.0369,  ...,  0.5342,  0.3388, -0.0184],\n",
       "                       ...,\n",
       "                       [ 0.0032,  0.6117, -0.0264,  ..., -0.4342,  0.2837, -0.5798],\n",
       "                       [ 0.2858, -0.0265,  0.5967,  ...,  0.2494,  0.5482,  0.4632],\n",
       "                       [-0.2446, -0.1438, -0.1200,  ...,  0.2559, -0.2690,  0.4526]])),\n",
       "              ('encoder.lstm.bias_ih_l0_reverse',\n",
       "               tensor([ 2.0890e-02,  1.0313e-01,  6.3520e-02, -9.6633e-03,  2.0491e-01,\n",
       "                        2.9446e-01, -2.0072e-01,  1.0779e-03, -1.1497e-02,  5.8916e-02,\n",
       "                       -8.9038e-02, -2.9884e-02, -1.0865e-01, -9.9001e-02,  2.4205e-01,\n",
       "                       -1.7481e-01,  8.6157e-02,  1.2510e+00,  3.4441e-01, -6.3870e-02,\n",
       "                        3.3685e-02, -1.7889e-01, -1.4068e-01,  4.1266e-01, -6.0196e-02,\n",
       "                        1.2623e-01,  2.0733e-01, -3.9439e-01,  4.0800e-01,  3.0500e-01,\n",
       "                        6.6546e-01, -1.1420e-01, -7.1177e-02, -6.3351e-02,  3.0432e-02,\n",
       "                       -3.2334e-01,  2.5904e-02, -2.9859e-01,  3.1546e-02,  1.8542e-01,\n",
       "                       -1.3994e-01, -3.5722e-01,  4.3605e-01, -4.6528e-02, -2.3308e-01,\n",
       "                        1.7312e-01, -2.1807e-03,  1.9290e-01, -5.6234e-01,  1.2008e-01,\n",
       "                       -2.4309e-01, -1.3509e-01,  3.3184e-01,  2.0908e-01,  4.8515e-01,\n",
       "                       -2.4755e-02,  1.1423e-01, -1.5157e-01, -3.7905e-02,  2.9310e-01,\n",
       "                        1.7394e-01,  2.7573e-01, -3.7044e-03, -4.8818e-01,  2.6736e-01,\n",
       "                        1.0054e-01, -2.5265e-01,  1.5634e-01,  4.4509e-01, -4.5745e-01,\n",
       "                        5.1028e-01, -1.0651e-01, -1.9746e-01, -2.1820e-02, -1.5622e-01,\n",
       "                        6.5942e-01, -2.1687e-01,  1.9387e-01,  1.8615e-01,  1.1730e-01,\n",
       "                       -1.4098e-01,  3.2050e-01, -4.2212e-01, -2.4055e-03,  1.2563e-01,\n",
       "                        1.6255e-01, -3.0510e-03, -1.4592e-01, -6.7919e-02, -1.3806e-01,\n",
       "                        3.0074e-01, -5.4023e-02, -2.2179e-01, -2.4981e-01, -1.5519e-01,\n",
       "                        2.2997e-01, -4.0647e-01,  5.2505e-01,  3.2302e-01, -2.2785e-01,\n",
       "                        3.4899e-01,  3.1415e-01,  3.0666e-01, -2.6897e-01,  1.6665e-01,\n",
       "                       -9.1639e-02,  4.4713e-01,  1.0179e-01,  5.0635e-02,  5.3918e-02,\n",
       "                        2.9642e-01, -2.9436e-02,  2.2055e-01, -7.2145e-02, -3.5058e-01,\n",
       "                        9.5837e-03, -2.7487e-01,  4.7107e-03, -1.1497e-01,  5.2619e-03,\n",
       "                       -6.6634e-02, -2.2353e-02, -7.5226e-02,  1.1109e-01, -3.4361e-02,\n",
       "                       -1.3465e-01,  4.7384e-02,  1.4981e-01, -4.6939e-01, -3.5492e-01,\n",
       "                       -3.6713e-02, -3.4706e-02, -2.0787e-01, -4.1909e-01,  2.4212e-01,\n",
       "                        1.6807e-01, -4.5328e-02,  1.4706e-02, -6.0356e-02, -7.3278e-02,\n",
       "                       -3.4860e-02,  5.0910e-03, -9.3877e-02, -2.6326e-01, -1.3146e-01,\n",
       "                        2.0253e-01, -9.3990e-02, -9.0286e-02, -2.5383e-01, -3.5095e-02,\n",
       "                       -2.0973e-01, -2.0020e-01,  3.0623e-01, -1.1792e-01, -1.0796e-02,\n",
       "                       -2.7142e-02, -2.5929e-01, -2.5554e-01, -8.1528e-02, -2.1411e-02,\n",
       "                       -5.1725e-02,  2.1683e-02, -2.4483e-01, -3.9019e-02, -6.8928e-02,\n",
       "                       -1.0522e-01, -1.4073e-01, -2.4765e-02,  2.2542e-01, -2.1179e-01,\n",
       "                       -8.8449e-02,  6.5294e-02, -7.2075e-02, -1.6341e-01, -2.8824e-01,\n",
       "                        4.2697e-02, -2.7122e-01, -3.4409e-02, -1.3692e-01,  4.7165e-02,\n",
       "                       -4.2956e-02, -3.8354e-01, -1.6753e-01, -1.7302e-02, -1.1617e-01,\n",
       "                       -1.3177e-02, -1.3664e-01, -1.5966e-01, -2.0270e-01, -3.1024e-01,\n",
       "                       -6.8906e-02,  2.3783e-01,  1.9876e-01,  5.7461e-02, -2.1281e-01,\n",
       "                       -6.7923e-02,  4.5750e-02, -9.4415e-02, -2.9979e-01, -4.4657e-03,\n",
       "                       -1.0084e-01, -9.6397e-02,  2.0857e-01,  1.8601e-01, -5.0897e-02,\n",
       "                       -7.8792e-03, -2.3465e-01, -1.1755e-02, -2.4352e-01, -6.9624e-02,\n",
       "                       -7.4817e-03,  3.8351e-04,  6.5387e-02, -2.2526e-01, -1.5451e-01,\n",
       "                       -2.2197e-02,  3.6517e-02, -2.9243e-01, -2.3300e-01, -5.4257e-02,\n",
       "                        2.2995e-01,  2.1903e-01, -1.7235e-03, -7.0402e-02, -1.3660e-01,\n",
       "                        4.9157e-02, -3.1505e-01, -3.4500e-01,  1.7927e-02, -9.1883e-02,\n",
       "                       -3.6978e-01, -9.4942e-02, -2.7557e-01, -1.6357e-01, -1.8567e-01,\n",
       "                       -1.2845e-01, -1.6869e-01, -1.6270e-01,  1.0081e-01, -5.1464e-02,\n",
       "                        1.1537e-01, -1.5573e-02, -3.4380e-02, -1.9639e-01, -1.5682e-02,\n",
       "                       -1.5318e-01,  1.2767e-02, -2.3081e-01, -3.5457e-03, -1.0069e-01,\n",
       "                        2.0651e-02, -5.2444e-02, -1.8743e-01, -3.0735e-01, -1.2484e-01,\n",
       "                       -1.7363e-01, -3.2578e-01,  1.6650e-01,  1.6723e-01,  1.5422e-01,\n",
       "                        6.8171e-02, -3.3123e-01,  1.5353e-01,  1.9462e-01,  5.0927e-01,\n",
       "                        1.4764e-01, -8.9648e-02, -1.0657e-01,  5.9754e-02, -1.5786e-01,\n",
       "                       -2.1659e-01, -2.5396e-01,  3.6846e-01,  3.6146e-01,  3.5034e-01,\n",
       "                       -1.5796e-03,  1.3884e-01,  1.5812e-01,  1.3762e-01,  3.7242e-02,\n",
       "                       -2.5969e-01,  5.7104e-03,  2.6989e-01,  8.5336e-02,  1.6900e-01,\n",
       "                       -2.7463e-01, -4.4725e-01, -3.3510e-02,  1.0336e-01,  3.0105e-01,\n",
       "                       -5.7322e-02,  1.8596e-01, -1.4254e-01,  7.6831e-02,  5.1083e-01,\n",
       "                        1.6697e-02, -9.3732e-03,  6.0002e-01, -4.3671e-01, -1.5118e-01,\n",
       "                        7.9171e-02,  6.0130e-03,  6.0213e-01,  8.4732e-02, -2.7230e-01,\n",
       "                       -1.9264e-01,  1.4542e-01, -8.1699e-02,  5.9501e-01, -1.5203e-01,\n",
       "                       -3.6495e-01,  1.7108e-01, -1.2239e-01,  3.0818e-01,  1.2737e-01,\n",
       "                       -3.3867e-01,  2.2245e-02, -4.5695e-01, -1.0982e-01, -1.0275e-01,\n",
       "                       -1.9316e-01, -5.8541e-02, -8.6132e-02, -7.0309e-02, -1.1001e-01,\n",
       "                        2.3583e-02,  1.3772e-01,  4.6734e-01,  9.0358e-02, -2.4945e-02,\n",
       "                        1.7087e-01,  2.0833e-01, -8.3035e-02, -1.3282e-02,  4.8094e-02,\n",
       "                        1.5369e-01,  1.0226e-01,  8.1464e-02,  2.1485e-01,  6.3882e-02,\n",
       "                        7.2677e-02,  3.5395e-03, -5.3823e-01, -8.3536e-02,  1.8302e-01,\n",
       "                        6.9989e-02, -1.7529e-01,  2.7931e-02,  1.4535e-01,  1.8557e-01,\n",
       "                       -1.3292e-01,  2.7812e-01, -9.8916e-02, -2.2931e-01,  1.4173e-01,\n",
       "                       -1.4614e-01, -1.9678e-01, -3.4954e-01, -2.3611e-01, -7.2009e-02,\n",
       "                       -3.1038e-01, -5.4390e-01, -2.1709e-01,  2.0900e-01, -1.4357e-01,\n",
       "                       -9.9645e-02, -9.5415e-02, -1.8932e-01,  2.1483e-01, -3.0550e-01,\n",
       "                       -2.5753e-01,  1.5945e-01,  1.1733e-01, -2.3213e-03,  5.6708e-02,\n",
       "                       -1.5917e-01, -1.7396e-02, -1.6324e-01,  5.6243e-02, -5.0014e-02,\n",
       "                       -2.9104e-01,  1.7294e-01, -4.9338e-02,  1.4924e-02,  4.7367e-02,\n",
       "                       -9.2498e-03,  2.4126e-01,  7.2115e-02,  9.5548e-02,  3.6186e-01,\n",
       "                        1.8617e-01,  2.9522e-01, -1.3676e-01,  7.1175e-02,  1.8325e-01,\n",
       "                        7.8402e-02,  1.0730e-01, -9.2352e-02,  9.1650e-02, -8.8952e-02,\n",
       "                       -6.1486e-02,  3.1659e-01,  5.2451e-01, -1.3889e-01,  1.3888e-01,\n",
       "                       -5.3115e-02,  1.3742e-01, -9.9732e-02,  1.8675e-01,  2.8335e-01,\n",
       "                        2.2791e-01,  4.0415e-02,  2.4441e-02,  5.0250e-01,  8.1433e-01,\n",
       "                       -1.5661e-01,  1.0722e-02,  1.4948e-01, -6.3383e-03,  2.0466e-01,\n",
       "                       -1.7852e-02, -3.9929e-01,  2.2653e-01,  9.5519e-02, -1.1754e-02,\n",
       "                        2.6607e-01,  1.4100e-01,  1.2384e-01, -3.6094e-01,  1.8199e-01,\n",
       "                        1.8808e-01, -3.2451e-01,  1.7711e-01,  1.9397e-01, -6.0594e-02,\n",
       "                       -7.0526e-02,  3.6315e-01,  3.4784e-01,  3.5492e-01,  1.6140e-01,\n",
       "                        2.4046e-01,  5.7343e-02, -6.8027e-02,  1.1258e-02,  1.7623e-01,\n",
       "                        9.5890e-02,  7.7182e-02,  1.1251e-01,  1.5391e-01,  1.2903e-01,\n",
       "                       -2.6578e-01,  1.0979e-01, -1.5587e-01, -7.2138e-02,  3.1736e-01,\n",
       "                       -1.0115e-01, -1.6944e-03,  1.0653e-01,  9.3953e-02,  1.0732e-01,\n",
       "                        8.9849e-02,  1.4149e-01, -2.6633e-02,  3.2952e-01,  1.6158e-02,\n",
       "                       -9.1141e-02, -2.0538e-01, -8.2228e-02,  1.4435e-01, -1.7662e-02,\n",
       "                       -3.5416e-02, -3.3435e-02,  1.6186e-02,  2.3064e-02,  3.2011e-01,\n",
       "                       -8.8000e-02,  7.5484e-02, -1.1116e-02, -1.3887e-01,  1.4032e-02,\n",
       "                       -1.0573e-01,  3.4528e-01,  6.4274e-02, -2.8223e-02,  2.5191e-01,\n",
       "                        2.1286e-01,  2.3749e-01,  1.8340e-01,  7.7935e-02, -1.6950e-01,\n",
       "                        5.4926e-01,  3.1254e-01,  1.0625e-01, -3.2394e-01,  1.4343e-01,\n",
       "                        1.4260e-01,  1.1314e-01, -1.5657e-01, -1.2400e-01,  2.7325e-01,\n",
       "                       -1.9478e-01,  1.3698e-01,  1.1368e-01, -2.3820e-02,  2.2737e-01,\n",
       "                        2.5286e-01, -6.2861e-02, -1.3508e-01,  3.8362e-01, -1.5330e-01,\n",
       "                        3.6882e-02,  2.1418e-01])),\n",
       "              ('encoder.lstm.bias_hh_l0_reverse',\n",
       "               tensor([ 2.0890e-02,  1.0313e-01,  6.3520e-02, -9.6631e-03,  2.0491e-01,\n",
       "                        2.9446e-01, -2.0072e-01,  1.0779e-03, -1.1497e-02,  5.8916e-02,\n",
       "                       -8.9038e-02, -2.9884e-02, -1.0865e-01, -9.9001e-02,  2.4205e-01,\n",
       "                       -1.7481e-01,  8.6157e-02,  1.2510e+00,  3.4441e-01, -6.3870e-02,\n",
       "                        3.3685e-02, -1.7889e-01, -1.4068e-01,  4.1266e-01, -6.0196e-02,\n",
       "                        1.2623e-01,  2.0733e-01, -3.9439e-01,  4.0800e-01,  3.0500e-01,\n",
       "                        6.6546e-01, -1.1420e-01, -7.1177e-02, -6.3351e-02,  3.0432e-02,\n",
       "                       -3.2334e-01,  2.5904e-02, -2.9859e-01,  3.1546e-02,  1.8542e-01,\n",
       "                       -1.3994e-01, -3.5722e-01,  4.3605e-01, -4.6528e-02, -2.3308e-01,\n",
       "                        1.7312e-01, -2.1806e-03,  1.9289e-01, -5.6234e-01,  1.2008e-01,\n",
       "                       -2.4309e-01, -1.3509e-01,  3.3184e-01,  2.0908e-01,  4.8515e-01,\n",
       "                       -2.4756e-02,  1.1423e-01, -1.5157e-01, -3.7905e-02,  2.9310e-01,\n",
       "                        1.7394e-01,  2.7573e-01, -3.7044e-03, -4.8818e-01,  2.6736e-01,\n",
       "                        1.0054e-01, -2.5265e-01,  1.5634e-01,  4.4509e-01, -4.5745e-01,\n",
       "                        5.1028e-01, -1.0651e-01, -1.9746e-01, -2.1820e-02, -1.5622e-01,\n",
       "                        6.5942e-01, -2.1687e-01,  1.9387e-01,  1.8615e-01,  1.1730e-01,\n",
       "                       -1.4098e-01,  3.2050e-01, -4.2212e-01, -2.4057e-03,  1.2563e-01,\n",
       "                        1.6255e-01, -3.0512e-03, -1.4592e-01, -6.7919e-02, -1.3806e-01,\n",
       "                        3.0074e-01, -5.4023e-02, -2.2179e-01, -2.4981e-01, -1.5519e-01,\n",
       "                        2.2997e-01, -4.0647e-01,  5.2505e-01,  3.2302e-01, -2.2785e-01,\n",
       "                        3.4899e-01,  3.1415e-01,  3.0666e-01, -2.6897e-01,  1.6665e-01,\n",
       "                       -9.1639e-02,  4.4713e-01,  1.0179e-01,  5.0635e-02,  5.3918e-02,\n",
       "                        2.9642e-01, -2.9436e-02,  2.2055e-01, -7.2145e-02, -3.5058e-01,\n",
       "                        9.5836e-03, -2.7487e-01,  4.7108e-03, -1.1497e-01,  5.2619e-03,\n",
       "                       -6.6634e-02, -2.2353e-02, -7.5226e-02,  1.1109e-01, -3.4361e-02,\n",
       "                       -1.3465e-01,  4.7384e-02,  1.4981e-01, -4.6939e-01, -3.5492e-01,\n",
       "                       -3.6713e-02, -3.4706e-02, -2.0787e-01, -4.1909e-01,  2.4212e-01,\n",
       "                        1.6807e-01, -4.5328e-02,  1.4706e-02, -6.0356e-02, -7.3278e-02,\n",
       "                       -3.4860e-02,  5.0911e-03, -9.3877e-02, -2.6326e-01, -1.3146e-01,\n",
       "                        2.0253e-01, -9.3990e-02, -9.0286e-02, -2.5383e-01, -3.5095e-02,\n",
       "                       -2.0973e-01, -2.0020e-01,  3.0623e-01, -1.1792e-01, -1.0796e-02,\n",
       "                       -2.7142e-02, -2.5929e-01, -2.5554e-01, -8.1528e-02, -2.1411e-02,\n",
       "                       -5.1725e-02,  2.1683e-02, -2.4483e-01, -3.9019e-02, -6.8928e-02,\n",
       "                       -1.0522e-01, -1.4073e-01, -2.4765e-02,  2.2542e-01, -2.1179e-01,\n",
       "                       -8.8449e-02,  6.5294e-02, -7.2075e-02, -1.6341e-01, -2.8824e-01,\n",
       "                        4.2697e-02, -2.7122e-01, -3.4409e-02, -1.3692e-01,  4.7165e-02,\n",
       "                       -4.2956e-02, -3.8354e-01, -1.6753e-01, -1.7302e-02, -1.1617e-01,\n",
       "                       -1.3177e-02, -1.3664e-01, -1.5966e-01, -2.0270e-01, -3.1024e-01,\n",
       "                       -6.8906e-02,  2.3783e-01,  1.9876e-01,  5.7461e-02, -2.1281e-01,\n",
       "                       -6.7923e-02,  4.5750e-02, -9.4415e-02, -2.9979e-01, -4.4658e-03,\n",
       "                       -1.0084e-01, -9.6397e-02,  2.0857e-01,  1.8601e-01, -5.0897e-02,\n",
       "                       -7.8792e-03, -2.3465e-01, -1.1755e-02, -2.4352e-01, -6.9624e-02,\n",
       "                       -7.4817e-03,  3.8353e-04,  6.5387e-02, -2.2526e-01, -1.5451e-01,\n",
       "                       -2.2197e-02,  3.6517e-02, -2.9243e-01, -2.3300e-01, -5.4257e-02,\n",
       "                        2.2995e-01,  2.1903e-01, -1.7235e-03, -7.0402e-02, -1.3660e-01,\n",
       "                        4.9157e-02, -3.1505e-01, -3.4500e-01,  1.7927e-02, -9.1883e-02,\n",
       "                       -3.6978e-01, -9.4942e-02, -2.7557e-01, -1.6357e-01, -1.8567e-01,\n",
       "                       -1.2845e-01, -1.6869e-01, -1.6270e-01,  1.0081e-01, -5.1464e-02,\n",
       "                        1.1537e-01, -1.5573e-02, -3.4380e-02, -1.9639e-01, -1.5682e-02,\n",
       "                       -1.5318e-01,  1.2768e-02, -2.3081e-01, -3.5456e-03, -1.0069e-01,\n",
       "                        2.0651e-02, -5.2444e-02, -1.8743e-01, -3.0735e-01, -1.2484e-01,\n",
       "                       -1.7364e-01, -3.2578e-01,  1.6650e-01,  1.6723e-01,  1.5422e-01,\n",
       "                        6.8171e-02, -3.3123e-01,  1.5353e-01,  1.9462e-01,  5.0927e-01,\n",
       "                        1.4764e-01, -8.9648e-02, -1.0657e-01,  5.9755e-02, -1.5786e-01,\n",
       "                       -2.1659e-01, -2.5396e-01,  3.6846e-01,  3.6146e-01,  3.5034e-01,\n",
       "                       -1.5796e-03,  1.3884e-01,  1.5812e-01,  1.3762e-01,  3.7242e-02,\n",
       "                       -2.5969e-01,  5.7103e-03,  2.6989e-01,  8.5336e-02,  1.6900e-01,\n",
       "                       -2.7463e-01, -4.4725e-01, -3.3510e-02,  1.0336e-01,  3.0105e-01,\n",
       "                       -5.7322e-02,  1.8596e-01, -1.4254e-01,  7.6831e-02,  5.1083e-01,\n",
       "                        1.6697e-02, -9.3731e-03,  6.0002e-01, -4.3671e-01, -1.5118e-01,\n",
       "                        7.9171e-02,  6.0130e-03,  6.0213e-01,  8.4732e-02, -2.7230e-01,\n",
       "                       -1.9264e-01,  1.4542e-01, -8.1699e-02,  5.9501e-01, -1.5203e-01,\n",
       "                       -3.6495e-01,  1.7108e-01, -1.2239e-01,  3.0818e-01,  1.2737e-01,\n",
       "                       -3.3867e-01,  2.2245e-02, -4.5695e-01, -1.0982e-01, -1.0275e-01,\n",
       "                       -1.9316e-01, -5.8541e-02, -8.6132e-02, -7.0309e-02, -1.1001e-01,\n",
       "                        2.3583e-02,  1.3772e-01,  4.6734e-01,  9.0358e-02, -2.4945e-02,\n",
       "                        1.7087e-01,  2.0833e-01, -8.3035e-02, -1.3282e-02,  4.8094e-02,\n",
       "                        1.5369e-01,  1.0226e-01,  8.1464e-02,  2.1485e-01,  6.3882e-02,\n",
       "                        7.2677e-02,  3.5394e-03, -5.3823e-01, -8.3536e-02,  1.8302e-01,\n",
       "                        6.9989e-02, -1.7529e-01,  2.7931e-02,  1.4535e-01,  1.8557e-01,\n",
       "                       -1.3292e-01,  2.7812e-01, -9.8916e-02, -2.2931e-01,  1.4174e-01,\n",
       "                       -1.4614e-01, -1.9678e-01, -3.4954e-01, -2.3611e-01, -7.2009e-02,\n",
       "                       -3.1038e-01, -5.4390e-01, -2.1709e-01,  2.0900e-01, -1.4357e-01,\n",
       "                       -9.9645e-02, -9.5415e-02, -1.8932e-01,  2.1483e-01, -3.0550e-01,\n",
       "                       -2.5753e-01,  1.5945e-01,  1.1733e-01, -2.3213e-03,  5.6708e-02,\n",
       "                       -1.5917e-01, -1.7396e-02, -1.6324e-01,  5.6243e-02, -5.0014e-02,\n",
       "                       -2.9104e-01,  1.7294e-01, -4.9338e-02,  1.4924e-02,  4.7367e-02,\n",
       "                       -9.2499e-03,  2.4126e-01,  7.2115e-02,  9.5548e-02,  3.6186e-01,\n",
       "                        1.8617e-01,  2.9522e-01, -1.3676e-01,  7.1175e-02,  1.8325e-01,\n",
       "                        7.8402e-02,  1.0730e-01, -9.2352e-02,  9.1649e-02, -8.8952e-02,\n",
       "                       -6.1486e-02,  3.1659e-01,  5.2451e-01, -1.3889e-01,  1.3888e-01,\n",
       "                       -5.3115e-02,  1.3742e-01, -9.9732e-02,  1.8675e-01,  2.8335e-01,\n",
       "                        2.2791e-01,  4.0415e-02,  2.4441e-02,  5.0250e-01,  8.1433e-01,\n",
       "                       -1.5661e-01,  1.0722e-02,  1.4948e-01, -6.3384e-03,  2.0466e-01,\n",
       "                       -1.7852e-02, -3.9929e-01,  2.2653e-01,  9.5519e-02, -1.1754e-02,\n",
       "                        2.6607e-01,  1.4100e-01,  1.2384e-01, -3.6094e-01,  1.8199e-01,\n",
       "                        1.8808e-01, -3.2452e-01,  1.7711e-01,  1.9397e-01, -6.0594e-02,\n",
       "                       -7.0526e-02,  3.6315e-01,  3.4784e-01,  3.5492e-01,  1.6140e-01,\n",
       "                        2.4046e-01,  5.7343e-02, -6.8027e-02,  1.1258e-02,  1.7623e-01,\n",
       "                        9.5890e-02,  7.7182e-02,  1.1251e-01,  1.5391e-01,  1.2903e-01,\n",
       "                       -2.6578e-01,  1.0979e-01, -1.5587e-01, -7.2138e-02,  3.1736e-01,\n",
       "                       -1.0115e-01, -1.6943e-03,  1.0653e-01,  9.3953e-02,  1.0732e-01,\n",
       "                        8.9849e-02,  1.4149e-01, -2.6633e-02,  3.2952e-01,  1.6158e-02,\n",
       "                       -9.1141e-02, -2.0538e-01, -8.2228e-02,  1.4435e-01, -1.7662e-02,\n",
       "                       -3.5416e-02, -3.3435e-02,  1.6186e-02,  2.3064e-02,  3.2011e-01,\n",
       "                       -8.8000e-02,  7.5484e-02, -1.1116e-02, -1.3887e-01,  1.4032e-02,\n",
       "                       -1.0573e-01,  3.4528e-01,  6.4274e-02, -2.8223e-02,  2.5191e-01,\n",
       "                        2.1286e-01,  2.3749e-01,  1.8340e-01,  7.7935e-02, -1.6950e-01,\n",
       "                        5.4926e-01,  3.1254e-01,  1.0625e-01, -3.2394e-01,  1.4343e-01,\n",
       "                        1.4260e-01,  1.1314e-01, -1.5657e-01, -1.2400e-01,  2.7325e-01,\n",
       "                       -1.9478e-01,  1.3698e-01,  1.1368e-01, -2.3820e-02,  2.2737e-01,\n",
       "                        2.5286e-01, -6.2861e-02, -1.3508e-01,  3.8362e-01, -1.5330e-01,\n",
       "                        3.6882e-02,  2.1418e-01])),\n",
       "              ('decoder.character_embedding.weight',\n",
       "               tensor([[ 1.2343e-02,  7.9405e-03,  6.9107e-03,  ..., -2.2478e-02,\n",
       "                         1.1492e-02,  7.6763e-03],\n",
       "                       [-2.8204e-02,  4.7435e-02, -3.4650e-02,  ...,  9.8588e-02,\n",
       "                         3.5150e-02,  4.5370e-02],\n",
       "                       [ 2.1884e-02, -8.0780e-03,  3.5129e-02,  ...,  6.0114e-02,\n",
       "                        -1.8479e-04,  6.2917e-03],\n",
       "                       ...,\n",
       "                       [ 1.4099e-01,  9.1053e-02, -2.9185e-01,  ...,  1.2284e-01,\n",
       "                         1.3043e-01,  5.1166e-02],\n",
       "                       [-1.1789e-01, -5.3044e-02, -6.5607e-02,  ...,  3.8286e-02,\n",
       "                        -1.9067e-03,  1.0572e-01],\n",
       "                       [-1.1616e-01,  4.1859e-03, -5.9919e-02,  ..., -2.1994e-02,\n",
       "                         1.3715e-01, -6.1688e-02]])),\n",
       "              ('decoder.lstm.weight_ih_l0',\n",
       "               tensor([[-6.7655e-02, -1.3392e-01,  2.7366e-01,  ...,  3.9105e-01,\n",
       "                         3.5909e-01, -8.4148e-02],\n",
       "                       [ 6.9466e-02,  3.2142e-01, -9.4193e-02,  ...,  3.8349e-01,\n",
       "                        -5.4042e-01, -6.0436e-02],\n",
       "                       [ 2.2796e-01,  1.0873e-01, -9.9672e-02,  ...,  3.0005e-01,\n",
       "                        -4.9832e-03,  3.1751e-01],\n",
       "                       ...,\n",
       "                       [ 3.4137e-02, -1.8194e-01,  3.9692e-01,  ...,  4.8336e-01,\n",
       "                         3.8396e-01,  3.8755e-01],\n",
       "                       [-1.5495e-01,  9.9557e-02, -1.3277e-01,  ..., -4.4239e-01,\n",
       "                        -1.6625e-01, -3.4000e-04],\n",
       "                       [ 3.1534e-01, -4.3736e-01,  1.6816e-01,  ..., -2.9624e-01,\n",
       "                         4.0556e-01, -2.3609e-01]])),\n",
       "              ('decoder.lstm.weight_hh_l0',\n",
       "               tensor([[-0.1395, -0.4017, -0.0336,  ..., -0.4566, -0.5189, -0.0920],\n",
       "                       [-0.0311,  0.2884,  0.0996,  ..., -0.2147, -0.2450,  0.0669],\n",
       "                       [-0.2144,  0.0601,  0.0424,  ..., -0.1270, -0.1646,  0.4135],\n",
       "                       ...,\n",
       "                       [ 0.1985,  0.0484,  0.0332,  ..., -0.3265, -0.1430,  0.4775],\n",
       "                       [-0.5374, -0.1113, -0.4428,  ..., -0.2662,  0.0625, -0.1079],\n",
       "                       [ 0.2915,  0.1110,  0.0158,  ..., -0.2406,  0.3166,  0.0503]])),\n",
       "              ('decoder.lstm.bias_ih_l0',\n",
       "               tensor([-0.0747, -0.1017,  0.1078,  ..., -0.0615, -0.1858, -0.2287])),\n",
       "              ('decoder.lstm.bias_hh_l0',\n",
       "               tensor([-0.0747, -0.1017,  0.1078,  ..., -0.0615, -0.1858, -0.2287])),\n",
       "              ('decoder.attn.attn.weight',\n",
       "               tensor([[-0.0717, -0.0258, -0.2815,  ...,  0.0466,  0.0154, -0.0876],\n",
       "                       [ 0.1447,  0.3374,  0.0299,  ..., -0.0147, -0.0815, -0.1175],\n",
       "                       [ 0.0241,  0.1192,  0.0221,  ..., -0.0416,  0.0542, -0.0221],\n",
       "                       ...,\n",
       "                       [-0.0573, -0.0504,  0.0090,  ...,  0.0647, -0.0141,  0.0043],\n",
       "                       [-0.0014, -0.0903,  0.0061,  ...,  0.0644,  0.0709,  0.0161],\n",
       "                       [-0.0411,  0.1032,  0.0342,  ...,  0.1086,  0.0852,  0.1203]])),\n",
       "              ('decoder.attn.attn.bias',\n",
       "               tensor([ 8.8998e-03, -1.3124e-02, -7.2919e-03,  3.9719e-02,  1.8628e-02,\n",
       "                        1.1081e-02,  1.4587e-03,  5.1826e-02, -1.0961e-01,  2.8836e-02,\n",
       "                        1.8772e-02, -2.6886e-02, -8.0573e-02,  8.3812e-03,  3.7653e-02,\n",
       "                        3.0661e-03, -6.0835e-02,  1.1990e-02,  6.9650e-02,  3.9255e-02,\n",
       "                       -1.7141e-01, -1.3935e-02,  3.9831e-02,  2.2972e-02, -8.8199e-02,\n",
       "                        9.6867e-03, -3.9419e-02,  5.2582e-03,  2.1995e-02, -1.0717e-02,\n",
       "                        2.7676e-02, -3.7472e-02, -6.9346e-02, -4.1025e-02,  1.2514e-02,\n",
       "                        1.2170e-01,  8.1891e-02, -1.6786e-02, -9.8790e-03, -5.8465e-02,\n",
       "                       -4.8959e-02, -1.1821e-02,  1.1812e-01, -9.9865e-03, -6.6441e-02,\n",
       "                       -1.7761e-02, -2.5437e-02, -4.0044e-02,  9.1268e-02, -2.2193e-02,\n",
       "                        6.1273e-07,  2.1765e-02,  2.1714e-02,  1.7528e-02, -2.2886e-02,\n",
       "                        3.1198e-03,  2.3759e-02, -3.1310e-02, -1.0899e-01,  3.1481e-02,\n",
       "                        1.2698e-02,  6.7105e-03,  3.8856e-02,  2.4801e-02,  2.4619e-02,\n",
       "                        2.1009e-03,  1.5466e-02, -3.1223e-02, -2.7420e-02, -6.4950e-02,\n",
       "                        1.0864e-02, -1.0868e-01, -2.6616e-02, -5.3417e-02,  4.5659e-02,\n",
       "                        1.1513e-01,  8.1555e-04, -1.2752e-02, -2.3081e-02, -5.1404e-02,\n",
       "                        1.6251e-02, -8.1291e-02,  1.0133e-02,  9.5629e-02, -4.6400e-02,\n",
       "                        3.6598e-03, -2.1508e-03,  1.1004e-02, -1.2342e-01,  1.8565e-02,\n",
       "                       -1.0615e-02, -9.8644e-02, -5.1286e-02,  4.4768e-02,  1.2702e-01,\n",
       "                       -9.5620e-03,  2.9568e-02, -9.9991e-03, -4.8687e-02,  1.9831e-02,\n",
       "                        3.4390e-03,  8.5503e-03,  1.3352e-02, -2.9214e-03, -7.7715e-03,\n",
       "                        2.1600e-03,  2.7121e-02,  1.1630e-01, -1.2188e-03,  2.5274e-03,\n",
       "                       -3.4541e-02,  1.0520e-02, -1.1472e-02, -5.8605e-02,  8.2589e-03,\n",
       "                        6.5329e-02, -5.4934e-03, -9.9112e-03, -2.3867e-03,  1.1385e-02,\n",
       "                       -5.2226e-02, -3.2463e-02,  3.3229e-02, -5.5638e-02, -1.0324e-01,\n",
       "                       -5.7211e-02, -9.4273e-02, -7.4026e-02, -1.1465e-02,  4.4040e-02,\n",
       "                        7.0265e-02,  5.6176e-02, -3.3025e-02,  3.2511e-02, -2.0577e-02,\n",
       "                       -3.6299e-02,  5.7356e-03, -1.7451e-03,  4.4096e-02,  7.2280e-02,\n",
       "                       -1.6507e-02,  3.2563e-02,  2.5312e-04, -4.7747e-02,  1.9519e-02,\n",
       "                       -5.6719e-02,  9.2712e-02,  1.9443e-02,  1.7089e-02, -9.4765e-02,\n",
       "                       -7.0116e-02, -1.1334e-01,  2.5143e-02,  5.6488e-02, -1.7090e-02,\n",
       "                        8.0890e-03,  1.5551e-02,  2.3280e-02, -2.1426e-02,  8.7046e-02,\n",
       "                        1.0677e-02, -3.0170e-02,  2.3596e-03,  3.6652e-02, -3.3626e-03,\n",
       "                       -2.6819e-02, -3.0577e-02, -1.5796e-02,  5.1872e-02,  4.7566e-03,\n",
       "                       -3.0513e-02,  1.0093e-01,  1.6237e-02, -1.7642e-02, -1.1055e-01,\n",
       "                        1.0968e-02, -7.2636e-02, -5.8684e-03,  1.6314e-03, -2.4957e-03,\n",
       "                       -7.0677e-02,  2.9332e-02,  1.3111e-02, -8.9897e-02, -6.2201e-02,\n",
       "                       -6.8569e-03, -2.0479e-02, -9.3783e-02, -4.3646e-02,  2.3518e-03,\n",
       "                       -1.5546e-02,  4.0186e-02,  2.8238e-02,  2.1021e-02, -1.7020e-02,\n",
       "                       -2.6206e-02,  1.9655e-02,  4.9017e-02, -9.4959e-03, -7.6732e-03,\n",
       "                       -9.6508e-03, -2.4399e-02, -3.7382e-02,  3.4686e-02,  1.0895e-02,\n",
       "                       -8.3265e-02, -6.8091e-02, -6.9445e-03, -2.7911e-02,  3.5376e-03,\n",
       "                        2.9918e-02, -7.9916e-03,  7.5201e-02, -7.2941e-03,  7.4106e-02,\n",
       "                       -3.3858e-02, -1.2550e-02,  2.7973e-02,  9.1144e-02, -1.4314e-02,\n",
       "                        1.2865e-01, -4.1830e-02, -1.0284e-01,  3.2327e-02, -7.4836e-02,\n",
       "                       -3.4822e-03, -3.5443e-02, -3.7662e-02, -7.3482e-03, -6.7337e-02,\n",
       "                       -7.0276e-03,  9.6027e-02, -1.1117e-01, -1.7065e-02, -1.3463e-02,\n",
       "                        8.5068e-03,  3.3378e-02,  4.1717e-02,  3.9127e-02,  5.2686e-03,\n",
       "                       -2.9052e-02, -1.6201e-02,  2.9699e-03, -1.6947e-04,  2.0825e-02,\n",
       "                       -8.0995e-03, -1.1298e-01,  1.8920e-02, -6.3020e-03, -7.5190e-02,\n",
       "                        7.5628e-03, -3.7140e-02, -1.2806e-01,  7.8221e-03,  9.0839e-03,\n",
       "                       -3.8641e-02])),\n",
       "              ('decoder.linear.weight',\n",
       "               tensor([[-0.2612,  0.2292,  0.2175,  ...,  0.3232,  0.4195, -0.3814],\n",
       "                       [-0.2365,  0.2492,  0.2338,  ...,  0.3435,  0.4038, -0.3868],\n",
       "                       [ 0.0787, -0.2565, -0.1474,  ..., -0.5050, -0.0566, -0.0451],\n",
       "                       ...,\n",
       "                       [-0.4235, -0.1085,  0.3287,  ...,  0.1408,  0.4187, -1.0261],\n",
       "                       [-0.0985,  0.3056,  0.0399,  ...,  0.4477,  0.0761, -0.1960],\n",
       "                       [-0.6416,  0.3345,  0.4855,  ...,  0.0235,  0.4273, -0.0538]])),\n",
       "              ('decoder.linear.bias',\n",
       "               tensor([-0.2672, -0.2598, -0.0845,  0.1753,  0.1744, -0.0120,  0.0215, -0.0134,\n",
       "                       -0.0370,  0.0891, -0.0751,  0.1222,  0.0546,  0.1669,  0.0928,  0.1309,\n",
       "                       -0.0442,  0.0999, -0.0179,  0.0144,  0.0743,  0.0493, -0.0465,  0.1384,\n",
       "                        0.0376, -0.2398, -0.1289,  0.1469,  0.1320, -0.2245, -0.1748, -0.0926,\n",
       "                       -0.0612,  0.1488,  0.1238, -0.0850, -0.1393,  0.0218,  0.1382,  0.1421,\n",
       "                       -0.0320, -0.3203, -0.1631,  0.0460, -0.0806, -0.1827, -0.0192,  0.0260,\n",
       "                       -0.2182, -0.0570,  0.0424, -0.0856,  0.1029, -0.1471, -0.0474, -0.1649,\n",
       "                       -0.0578, -0.0291, -0.2256, -0.0688, -0.1208, -0.2030, -0.1164, -0.0834,\n",
       "                       -0.1248, -0.0513, -0.1495, -0.2181, -0.1073, -0.1300, -0.0890, -0.0464,\n",
       "                       -0.1413, -0.2393, -0.1358, -0.0505, -0.2166, -0.0327, -0.1887, -0.0573,\n",
       "                       -0.0504, -0.1317, -0.1787, -0.1185, -0.1493, -0.1387, -0.1508, -0.1061,\n",
       "                       -0.0775, -0.0744, -0.1134, -0.0220, -0.0985]))]),\n",
       " 'optimizer_state_dict': {'state': {0: {'step': 23400,\n",
       "    'exp_avg': tensor([[ 2.3517e-04,  5.9861e-04, -9.2815e-05,  ...,  3.2294e-04,\n",
       "              2.6957e-04, -6.5783e-04],\n",
       "            [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            ...,\n",
       "            [ 1.1176e-22, -9.2658e-08,  4.6671e-08,  ...,  7.9678e-25,\n",
       "             -7.4554e-08,  1.1109e-08],\n",
       "            [-7.5175e-09, -7.7322e-10, -1.1630e-10,  ...,  1.5123e-17,\n",
       "              2.1278e-08, -2.5689e-08],\n",
       "            [ 5.6052e-45,  5.0542e-22,  5.6052e-45,  ..., -6.9422e-22,\n",
       "             -5.6052e-45,  5.6052e-45]]),\n",
       "    'exp_avg_sq': tensor([[4.5590e-07, 2.9057e-07, 1.7192e-07,  ..., 2.2069e-07, 3.3955e-07,\n",
       "             3.0559e-07],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            ...,\n",
       "            [1.8511e-10, 7.7283e-10, 5.7433e-10,  ..., 6.1350e-12, 3.1069e-10,\n",
       "             3.4323e-11],\n",
       "            [8.4391e-10, 1.5811e-09, 7.7669e-11,  ..., 5.6938e-10, 1.8452e-10,\n",
       "             1.3238e-10],\n",
       "            [1.7787e-13, 7.4626e-12, 2.3614e-13,  ..., 1.2907e-11, 2.7099e-13,\n",
       "             9.7460e-13]])},\n",
       "   1: {'step': 23400,\n",
       "    'exp_avg': tensor([[-2.2477e-05,  1.9011e-05,  1.6946e-05,  ...,  2.9434e-06,\n",
       "             -8.5370e-06, -9.8382e-06],\n",
       "            [ 7.6751e-07, -4.8307e-06, -6.4974e-06,  ..., -1.8989e-05,\n",
       "              7.6722e-06,  2.2953e-05],\n",
       "            [ 3.1601e-06,  2.5773e-05,  2.2443e-05,  ..., -3.6432e-06,\n",
       "              6.3226e-05,  1.0915e-05],\n",
       "            ...,\n",
       "            [ 6.8507e-06, -2.6690e-06, -2.4466e-05,  ...,  1.0906e-05,\n",
       "             -1.0734e-05,  2.4623e-06],\n",
       "            [-4.4400e-06, -7.7377e-06, -1.8729e-05,  ...,  5.6781e-06,\n",
       "              8.4426e-06,  1.4564e-05],\n",
       "            [-8.7131e-06,  8.2045e-05, -1.1621e-05,  ...,  2.5619e-05,\n",
       "              2.4958e-07,  3.4550e-06]]),\n",
       "    'exp_avg_sq': tensor([[2.2046e-09, 1.8946e-09, 1.9248e-09,  ..., 4.3373e-09, 2.1977e-09,\n",
       "             1.0516e-09],\n",
       "            [1.2339e-09, 1.6305e-09, 1.1460e-09,  ..., 1.2558e-09, 2.3874e-09,\n",
       "             1.4855e-09],\n",
       "            [1.2357e-09, 2.0774e-09, 2.4000e-09,  ..., 1.2171e-09, 5.2816e-09,\n",
       "             2.6716e-09],\n",
       "            ...,\n",
       "            [3.0586e-10, 4.3180e-10, 5.4024e-10,  ..., 1.0406e-09, 5.1461e-10,\n",
       "             3.6810e-10],\n",
       "            [1.7677e-09, 1.9441e-09, 1.2705e-09,  ..., 1.5188e-09, 1.5540e-09,\n",
       "             9.3890e-10],\n",
       "            [3.3467e-08, 2.2428e-08, 1.7301e-08,  ..., 1.8042e-08, 1.9007e-08,\n",
       "             1.1410e-08]])},\n",
       "   2: {'step': 23400,\n",
       "    'exp_avg': tensor([[-1.0818e-05, -5.8774e-06,  4.6020e-06,  ...,  1.3932e-06,\n",
       "             -5.3513e-06,  3.8562e-06],\n",
       "            [ 8.5545e-06,  8.8858e-06,  1.6428e-06,  ..., -1.5987e-06,\n",
       "              3.3270e-06, -7.0853e-06],\n",
       "            [-5.7621e-06,  3.7841e-06, -4.2573e-06,  ..., -6.4386e-07,\n",
       "             -2.8930e-06, -4.4878e-08],\n",
       "            ...,\n",
       "            [ 7.2689e-06, -2.3385e-06, -9.3392e-07,  ..., -1.1651e-06,\n",
       "             -5.2661e-07,  5.3196e-06],\n",
       "            [ 1.2160e-05,  5.1669e-06, -2.0397e-06,  ..., -3.3670e-06,\n",
       "              7.4984e-06, -9.7262e-06],\n",
       "            [-5.4194e-05,  3.4410e-05,  4.4627e-06,  ...,  1.6206e-05,\n",
       "             -1.6363e-05,  5.4576e-05]]),\n",
       "    'exp_avg_sq': tensor([[3.9874e-10, 5.9112e-10, 1.2831e-10,  ..., 1.8576e-11, 8.6213e-11,\n",
       "             1.2300e-09],\n",
       "            [6.5986e-10, 1.0372e-09, 3.0381e-10,  ..., 3.5157e-11, 4.0642e-11,\n",
       "             8.3202e-10],\n",
       "            [7.0981e-10, 8.1577e-10, 3.2578e-10,  ..., 3.0259e-11, 8.1657e-11,\n",
       "             1.3416e-09],\n",
       "            ...,\n",
       "            [4.5975e-10, 1.0257e-10, 9.3901e-11,  ..., 5.6454e-11, 2.2398e-11,\n",
       "             2.1008e-10],\n",
       "            [4.3825e-10, 3.1890e-10, 1.4544e-10,  ..., 1.6627e-11, 8.2285e-11,\n",
       "             5.9975e-10],\n",
       "            [6.4666e-09, 8.1530e-09, 4.1852e-09,  ..., 4.6221e-10, 5.2047e-10,\n",
       "             1.8630e-08]])},\n",
       "   3: {'step': 23400,\n",
       "    'exp_avg': tensor([-1.1771e-04,  6.6987e-05, -4.3232e-05,  2.0164e-04, -4.5426e-04,\n",
       "            -4.6520e-05, -1.8663e-04, -3.2553e-04, -7.3302e-05, -1.8543e-04,\n",
       "            -1.4446e-04,  3.2208e-06,  1.0368e-04, -7.1020e-05, -7.1268e-05,\n",
       "             6.9124e-06, -2.0231e-04,  1.6239e-05,  1.3825e-05, -4.6110e-04,\n",
       "             1.2031e-05,  1.3220e-05, -4.4176e-05, -1.8117e-04, -1.9175e-04,\n",
       "            -2.3719e-04, -7.0279e-04, -2.4076e-04,  8.5683e-05,  4.7316e-04,\n",
       "             2.9764e-04,  6.7865e-05,  4.3312e-06,  3.6705e-05, -2.1103e-05,\n",
       "            -2.8731e-04, -2.1100e-05, -4.6028e-05, -1.3470e-04, -1.1030e-04,\n",
       "            -1.2481e-05, -9.1878e-05,  4.5155e-05,  1.1516e-05, -6.2644e-04,\n",
       "            -3.2111e-04, -1.7422e-04,  1.3830e-05, -8.3324e-04, -1.7632e-04,\n",
       "            -8.0709e-05,  5.8021e-05, -4.0004e-04,  7.5753e-06,  9.8720e-07,\n",
       "            -6.2639e-05, -4.8543e-04,  2.9628e-05,  7.0320e-05, -1.5185e-04,\n",
       "            -8.8271e-05, -6.3715e-05,  1.9188e-05, -8.9740e-06, -1.9120e-04,\n",
       "            -7.2019e-05,  5.1277e-05, -1.0066e-04,  2.5725e-05,  1.8051e-04,\n",
       "             9.8145e-06, -1.9386e-05,  1.3184e-04,  3.0730e-06, -1.8016e-04,\n",
       "            -4.1114e-05,  1.2259e-04, -9.8292e-04, -4.5447e-04, -1.3867e-04,\n",
       "            -3.2232e-05, -7.0620e-05, -7.5799e-06, -4.2390e-04, -1.4677e-04,\n",
       "            -7.0595e-05,  3.9389e-04,  2.2273e-05,  1.1093e-04, -2.3960e-04,\n",
       "             4.2487e-05, -2.9762e-04, -1.3454e-04,  8.4563e-05,  3.5783e-07,\n",
       "            -2.1579e-04,  4.3992e-05, -1.0462e-04, -1.7642e-04, -6.6633e-04,\n",
       "            -2.2069e-05, -5.5064e-05,  1.6031e-06, -6.4416e-05, -3.1806e-04,\n",
       "             9.2437e-05,  1.1182e-05, -9.2789e-06,  1.5968e-04, -2.1395e-04,\n",
       "             3.7111e-05, -4.0746e-05, -2.8635e-05, -1.0790e-04,  3.5899e-04,\n",
       "            -1.1197e-04,  2.9769e-04, -3.8622e-04, -2.1496e-04,  1.3520e-04,\n",
       "            -5.5999e-05,  2.8673e-04,  6.3884e-04,  2.9850e-04, -2.6511e-04,\n",
       "             3.6556e-06, -6.3768e-04, -4.1363e-04, -2.9063e-05, -2.3458e-05,\n",
       "             1.4382e-05,  3.0691e-05, -2.0916e-09,  2.2302e-05, -6.7180e-09,\n",
       "            -4.3964e-05,  3.4776e-06,  3.4432e-05, -2.5657e-05,  3.1201e-05,\n",
       "             8.4885e-06, -1.9538e-05, -6.9341e-05,  1.5249e-05, -5.5882e-05,\n",
       "            -3.9602e-05,  1.0952e-05,  1.5425e-04,  1.8911e-04, -7.2572e-05,\n",
       "             2.8137e-05,  4.1905e-05,  5.5471e-05, -2.5506e-05,  1.4875e-04,\n",
       "             1.0710e-04,  8.8844e-05,  1.0189e-04,  1.3198e-04,  1.4153e-05,\n",
       "             6.8537e-06,  8.8787e-05, -8.9356e-05, -8.5881e-05,  2.5170e-05,\n",
       "            -3.6433e-05, -4.1412e-05,  5.0620e-05, -6.2715e-06,  1.5487e-05,\n",
       "            -1.8308e-05, -1.3548e-05,  3.1442e-04,  1.1135e-04, -4.8183e-06,\n",
       "             4.0039e-05, -7.4880e-05, -3.2362e-05, -1.5791e-04,  8.3215e-06,\n",
       "             8.7835e-05,  1.5747e-05, -8.1276e-06,  3.5975e-06, -1.9173e-04,\n",
       "             1.7553e-04,  1.3812e-05,  4.9500e-05,  4.9271e-05,  1.8448e-04,\n",
       "            -6.4056e-05,  5.6549e-05, -5.1410e-05, -5.8997e-06,  3.7389e-06,\n",
       "             1.9112e-04,  1.5337e-06, -4.1187e-05,  4.3873e-05,  3.7774e-06,\n",
       "             6.9175e-05,  7.1387e-06, -3.7702e-05, -2.9701e-05,  6.6600e-05,\n",
       "            -4.1360e-04, -2.7610e-04,  5.9746e-06,  3.7751e-05,  3.2245e-05,\n",
       "            -4.5316e-05, -4.3326e-05,  2.2204e-05, -1.0988e-05,  2.1376e-04,\n",
       "             9.2924e-05, -1.6045e-05, -1.2383e-04,  3.9419e-05, -2.1161e-04,\n",
       "            -1.1129e-05,  1.0233e-04,  4.5545e-05,  4.2795e-05,  6.8488e-05,\n",
       "            -6.1065e-05, -6.2988e-06,  4.2076e-05,  6.4292e-05, -3.4302e-05,\n",
       "            -7.2392e-05,  1.8532e-05,  3.8967e-05,  1.1613e-04, -6.0898e-05,\n",
       "            -1.3225e-05,  4.5903e-05,  7.8230e-05, -7.2348e-07, -5.8372e-05,\n",
       "             3.7583e-05,  2.4441e-05,  5.8360e-05, -6.9700e-05, -4.2785e-05,\n",
       "             1.5414e-04, -1.7874e-04, -3.8133e-05,  1.8745e-04,  3.4156e-04,\n",
       "             7.3843e-05,  1.1746e-04, -7.7122e-04, -3.9373e-06, -1.1086e-03,\n",
       "            -1.7631e-05, -5.2384e-04, -2.6004e-04, -8.8779e-04,  5.5473e-04,\n",
       "            -2.6008e-04, -1.0887e-03, -3.6521e-04,  1.4426e-03, -4.5246e-04,\n",
       "             8.4465e-04,  3.3066e-04,  1.0978e-03, -4.1913e-04,  2.9382e-04,\n",
       "             1.6485e-04,  1.5557e-04,  1.7148e-03,  1.3670e-04,  4.4794e-05,\n",
       "             5.5598e-04,  9.2127e-04,  1.3136e-04, -1.7793e-03, -1.8981e-04,\n",
       "            -3.1256e-05,  7.6748e-04,  1.4661e-03, -5.6689e-04, -5.4857e-04,\n",
       "             6.6316e-04, -2.6700e-04, -3.9309e-04,  3.3872e-04,  2.5269e-04,\n",
       "             4.4587e-04,  4.9659e-04,  4.8693e-04,  5.1331e-04,  5.2506e-04,\n",
       "             6.4046e-04,  3.4968e-05,  2.7223e-04, -6.6226e-04, -3.1039e-04,\n",
       "             1.1355e-03,  2.9000e-04, -3.5720e-04, -6.5248e-05,  1.2933e-04,\n",
       "            -3.7307e-04,  4.4361e-04, -3.9562e-04, -8.5571e-04, -1.5765e-03,\n",
       "             2.2819e-05,  5.5205e-05, -7.2593e-04, -8.6615e-04, -1.7500e-04,\n",
       "             2.9144e-04, -2.7043e-04, -2.9126e-04,  1.6398e-04, -1.5050e-04,\n",
       "             5.8882e-04,  1.0633e-03, -8.1012e-05, -4.3940e-04,  1.1811e-04,\n",
       "            -6.7800e-04,  1.6986e-04, -2.3481e-04, -6.1133e-04, -2.0849e-04,\n",
       "             8.8455e-04, -2.6703e-04,  3.0874e-04,  2.1330e-03,  1.2393e-03,\n",
       "             7.2457e-05, -4.8887e-04, -4.1367e-04, -3.2673e-04, -1.0392e-03,\n",
       "             1.8053e-03, -1.2973e-03, -6.9971e-04,  1.4137e-04, -2.8039e-04,\n",
       "             9.7005e-05, -2.2101e-04,  4.3873e-04, -8.3439e-05,  3.2153e-04,\n",
       "            -7.1326e-04, -3.7908e-04,  6.0086e-04,  2.5339e-04, -6.2464e-04,\n",
       "             4.0282e-04, -2.9636e-04,  1.3877e-04, -9.4497e-05,  3.9090e-04,\n",
       "            -1.1500e-03,  1.8513e-03, -3.5314e-04,  2.4391e-04, -3.3831e-04,\n",
       "             5.8212e-04,  1.3038e-04,  5.1427e-04,  6.2068e-04, -1.1433e-04,\n",
       "             2.4652e-04, -1.0080e-06, -1.0681e-04,  2.8144e-04, -4.5799e-05,\n",
       "             4.1000e-04,  5.3191e-04,  4.1102e-04,  7.6920e-04,  3.1899e-04,\n",
       "            -6.4134e-04,  5.0861e-05, -6.2832e-05,  2.1168e-04, -3.2086e-04,\n",
       "             9.1682e-05, -3.3036e-05,  2.3947e-04, -1.3248e-04, -1.1351e-04,\n",
       "            -2.1740e-04, -3.4491e-04, -3.3686e-04, -1.8638e-04, -1.4005e-04,\n",
       "            -1.0663e-04,  2.3754e-04, -2.4790e-04, -1.0721e-04, -1.5986e-06,\n",
       "            -1.2385e-04,  2.8849e-06,  8.6775e-06, -2.5174e-04,  1.2788e-04,\n",
       "             2.1636e-04, -1.1712e-04, -1.3675e-04, -1.6979e-04, -2.5627e-04,\n",
       "            -1.3832e-03, -1.4261e-04, -3.9162e-06,  1.2905e-04,  2.3979e-04,\n",
       "             8.7072e-05, -1.8349e-05, -7.2893e-05,  5.0450e-05, -5.1185e-04,\n",
       "             4.2243e-05,  7.5383e-05, -9.4200e-05,  2.0909e-04, -9.6403e-06,\n",
       "            -8.9552e-05, -2.2045e-05, -3.3135e-05, -2.4919e-04, -1.4518e-04,\n",
       "            -1.7308e-04, -1.5664e-06,  3.8735e-04, -2.8778e-04, -1.0815e-04,\n",
       "             2.0214e-04, -2.5160e-04, -2.1990e-05,  1.1173e-05, -8.5791e-05,\n",
       "            -2.4210e-04, -4.2975e-05,  2.0024e-04, -8.5269e-05, -1.5587e-05,\n",
       "            -1.8130e-04,  1.1287e-04,  2.8953e-05, -1.2686e-04, -1.1333e-04,\n",
       "            -1.1496e-05, -1.1798e-04,  1.5043e-04,  4.6474e-05,  3.5896e-05,\n",
       "            -2.1255e-05,  5.5863e-05,  3.1751e-05, -2.7514e-04, -2.7298e-05,\n",
       "            -5.1856e-05, -2.8170e-04, -3.7039e-04, -1.3826e-04,  1.0309e-05,\n",
       "             4.7996e-05,  6.8787e-05, -5.9960e-04, -3.3718e-04, -1.6672e-04,\n",
       "             4.6920e-04,  6.9649e-05, -1.0860e-04, -1.0136e-04,  1.0944e-04,\n",
       "            -1.2762e-04, -2.3882e-04,  1.4691e-04, -1.9697e-04, -3.5313e-04,\n",
       "             7.6508e-04, -1.3809e-04, -2.2948e-04, -6.2693e-04, -6.6922e-05,\n",
       "            -1.0652e-04, -2.7557e-05, -6.7598e-05, -5.0081e-04,  2.6773e-04,\n",
       "            -3.3973e-05, -1.1743e-04, -1.0315e-04, -1.1069e-04,  1.7357e-04,\n",
       "            -1.2983e-04, -1.4025e-05, -1.6812e-04,  3.2179e-04, -1.7023e-04,\n",
       "             3.1880e-04, -2.8939e-04, -6.5332e-05, -2.2521e-05, -1.4335e-04,\n",
       "             2.3760e-04, -6.9425e-05,  2.1891e-04, -5.4267e-05,  1.8407e-05,\n",
       "             7.4587e-05, -2.8806e-04]),\n",
       "    'exp_avg_sq': tensor([3.9725e-08, 3.6270e-08, 3.6814e-08, 1.0375e-07, 2.9960e-07, 3.6226e-08,\n",
       "            5.6453e-08, 4.9379e-08, 4.5074e-08, 6.7302e-08, 2.9174e-07, 5.8942e-08,\n",
       "            2.0015e-08, 1.5763e-08, 6.4097e-08, 2.8940e-08, 4.4244e-08, 4.2892e-08,\n",
       "            3.9349e-09, 1.2275e-07, 6.7738e-08, 1.1142e-08, 6.6715e-08, 6.2088e-08,\n",
       "            1.3225e-07, 1.1282e-07, 2.2784e-07, 1.0832e-06, 3.7499e-08, 2.3202e-07,\n",
       "            2.3437e-08, 4.0495e-08, 7.6621e-09, 3.7623e-08, 4.8224e-08, 3.4546e-07,\n",
       "            4.5614e-08, 2.6115e-08, 8.0799e-08, 2.7280e-08, 1.8949e-08, 6.9703e-08,\n",
       "            3.3683e-08, 7.2153e-08, 2.0401e-06, 7.8764e-08, 4.3318e-08, 1.7108e-08,\n",
       "            1.4654e-06, 3.5447e-08, 9.3797e-08, 1.1971e-07, 1.8750e-07, 1.0628e-07,\n",
       "            7.3205e-09, 8.6030e-09, 1.4244e-07, 4.1464e-08, 3.9640e-08, 9.7284e-08,\n",
       "            4.7592e-08, 1.1400e-08, 1.3202e-08, 2.8581e-08, 5.9059e-08, 4.6893e-08,\n",
       "            1.3265e-08, 1.3692e-07, 3.4003e-08, 1.6243e-07, 2.6701e-08, 2.1662e-08,\n",
       "            5.1236e-08, 3.0126e-08, 4.4982e-08, 3.3163e-08, 8.0344e-08, 3.2510e-07,\n",
       "            5.6753e-08, 5.0754e-08, 9.0587e-08, 2.4662e-08, 1.2464e-07, 5.8908e-08,\n",
       "            1.4407e-07, 9.1344e-07, 1.1975e-07, 4.0484e-08, 1.1693e-07, 3.3290e-07,\n",
       "            4.7575e-08, 1.1235e-08, 2.8678e-08, 3.1393e-08, 6.4511e-08, 1.4256e-07,\n",
       "            1.3338e-07, 2.6394e-08, 1.1308e-07, 1.3794e-07, 3.3313e-08, 2.4901e-08,\n",
       "            3.3958e-08, 2.2183e-08, 7.2740e-08, 5.5683e-08, 3.4472e-08, 7.8382e-08,\n",
       "            1.8380e-07, 4.0737e-08, 7.9373e-08, 9.1452e-08, 3.4034e-08, 2.5135e-08,\n",
       "            1.2814e-07, 6.4375e-08, 1.2865e-07, 1.7500e-07, 7.2360e-08, 4.2340e-08,\n",
       "            8.0288e-08, 4.2505e-08, 1.8976e-07, 5.9722e-08, 4.1955e-07, 7.7956e-09,\n",
       "            2.5065e-07, 9.4765e-08, 2.4849e-08, 8.6565e-09, 1.5083e-08, 1.0557e-07,\n",
       "            2.4612e-08, 1.1709e-08, 1.3527e-08, 7.8938e-09, 1.9389e-08, 5.3589e-08,\n",
       "            1.4776e-07, 3.1984e-08, 1.0426e-08, 1.3861e-08, 3.8922e-08, 4.3890e-08,\n",
       "            1.4628e-08, 2.4568e-08, 5.7939e-10, 5.2988e-08, 3.7714e-08, 5.0789e-08,\n",
       "            1.3813e-08, 1.1796e-08, 6.6782e-09, 2.2975e-08, 1.8993e-07, 2.8400e-07,\n",
       "            3.2807e-08, 1.2980e-07, 5.1736e-09, 1.3743e-08, 5.3919e-09, 3.2974e-08,\n",
       "            8.6376e-08, 5.0096e-07, 3.0409e-08, 7.5780e-08, 4.8640e-08, 3.5946e-08,\n",
       "            1.1273e-08, 1.5621e-08, 3.1613e-08, 1.9843e-08, 3.5524e-07, 1.3920e-08,\n",
       "            1.1698e-08, 2.4279e-08, 3.3383e-08, 2.9346e-08, 8.1812e-08, 9.9124e-08,\n",
       "            1.0860e-07, 1.2459e-07, 1.4669e-09, 1.3178e-09, 4.4293e-08, 4.2657e-08,\n",
       "            1.7120e-08, 4.1556e-08, 5.3678e-08, 2.3218e-08, 1.7058e-08, 3.4836e-09,\n",
       "            2.2320e-08, 3.2206e-08, 4.0269e-09, 1.1376e-07, 3.8675e-08, 1.1555e-07,\n",
       "            1.2269e-08, 3.6493e-08, 1.5716e-08, 2.1308e-08, 3.9088e-08, 1.1100e-08,\n",
       "            4.8656e-08, 1.4150e-07, 1.7357e-08, 1.2127e-08, 4.9480e-08, 1.0240e-08,\n",
       "            3.4633e-08, 6.2652e-08, 1.1142e-07, 8.5875e-08, 2.7290e-08, 3.6450e-08,\n",
       "            6.5016e-08, 1.4761e-07, 1.6573e-08, 8.6550e-09, 1.4148e-08, 1.2860e-08,\n",
       "            4.9766e-08, 6.8286e-08, 7.4688e-08, 1.3680e-08, 4.2160e-08, 1.4883e-07,\n",
       "            1.4473e-07, 1.5292e-08, 3.5779e-08, 1.8425e-08, 6.5811e-08, 5.9319e-08,\n",
       "            1.7105e-08, 8.7002e-08, 1.0149e-07, 1.8527e-08, 5.9168e-08, 3.8650e-08,\n",
       "            1.2083e-08, 2.9090e-09, 3.3858e-08, 7.7459e-09, 6.8435e-08, 1.1763e-07,\n",
       "            1.1322e-07, 8.3851e-09, 8.7324e-08, 4.8786e-08, 8.4657e-08, 1.2507e-08,\n",
       "            3.8295e-06, 2.6502e-09, 1.3633e-06, 2.9138e-08, 5.2584e-07, 1.1331e-07,\n",
       "            6.0346e-07, 3.8550e-07, 2.4877e-06, 4.2568e-07, 5.4576e-07, 2.6682e-07,\n",
       "            2.1478e-07, 1.4022e-06, 2.6662e-06, 1.7274e-07, 5.4092e-08, 2.4592e-07,\n",
       "            5.5940e-07, 1.8549e-07, 1.1491e-06, 3.7243e-07, 3.2446e-08, 4.6899e-07,\n",
       "            1.3755e-06, 2.5972e-08, 1.5652e-06, 2.7944e-07, 6.3901e-08, 4.1519e-07,\n",
       "            5.0677e-07, 2.5805e-06, 2.7717e-07, 2.4860e-07, 6.3673e-08, 4.6730e-07,\n",
       "            2.8119e-08, 2.8472e-07, 3.5459e-07, 6.9730e-07, 6.1980e-07, 1.8840e-07,\n",
       "            7.5029e-07, 2.0422e-07, 2.3908e-07, 1.3984e-07, 5.8249e-07, 7.4535e-07,\n",
       "            6.1638e-06, 3.7486e-07, 5.0725e-07, 2.0730e-07, 4.5753e-07, 5.1258e-07,\n",
       "            3.6306e-07, 2.5476e-07, 4.7339e-07, 7.3936e-07, 6.2770e-08, 8.5867e-08,\n",
       "            1.1438e-06, 1.2760e-06, 3.8607e-07, 9.4215e-07, 6.1249e-07, 1.0451e-07,\n",
       "            8.4176e-08, 5.2542e-08, 4.0463e-07, 1.1468e-06, 3.1408e-08, 6.8329e-07,\n",
       "            1.8124e-07, 3.0993e-07, 1.7690e-07, 2.1143e-07, 5.5272e-07, 3.7209e-07,\n",
       "            4.9881e-07, 5.8278e-07, 1.5304e-07, 2.0665e-06, 4.0949e-07, 7.8060e-07,\n",
       "            1.0255e-06, 3.4561e-07, 4.3840e-07, 9.2807e-07, 7.7270e-07, 4.1724e-06,\n",
       "            2.9076e-07, 4.5808e-07, 5.7620e-07, 1.8940e-06, 2.8130e-07, 3.1369e-08,\n",
       "            1.8146e-07, 6.1992e-07, 1.2225e-06, 1.7169e-06, 6.1526e-07, 1.0551e-07,\n",
       "            4.8663e-07, 1.9936e-07, 4.2988e-07, 5.1125e-08, 2.4810e-07, 3.3678e-07,\n",
       "            3.1087e-07, 1.3951e-06, 5.4467e-07, 9.2814e-07, 7.4365e-07, 3.8431e-07,\n",
       "            5.3990e-07, 9.9587e-07, 2.9813e-07, 9.0032e-08, 5.0114e-08, 1.2443e-07,\n",
       "            1.3358e-07, 1.2082e-07, 2.6540e-07, 1.9370e-07, 4.9035e-07, 3.2581e-07,\n",
       "            5.6475e-07, 1.8004e-07, 6.1809e-07, 6.6839e-08, 4.0729e-08, 4.2854e-07,\n",
       "            5.9488e-08, 6.1119e-08, 3.1695e-08, 7.5184e-08, 5.2981e-07, 3.8404e-08,\n",
       "            4.1300e-08, 6.3828e-08, 3.0905e-07, 7.3147e-08, 2.7465e-07, 7.7450e-08,\n",
       "            4.8436e-08, 6.9610e-08, 9.0951e-08, 7.0592e-08, 4.3413e-08, 1.2588e-07,\n",
       "            4.6657e-09, 7.9814e-08, 1.3185e-07, 3.6366e-08, 5.7969e-08, 5.0561e-08,\n",
       "            1.7487e-07, 1.6922e-07, 3.5088e-07, 3.3831e-07, 8.3394e-08, 8.7564e-08,\n",
       "            3.4443e-08, 3.5601e-08, 9.0236e-09, 1.3403e-07, 1.5705e-07, 1.2808e-06,\n",
       "            1.0273e-07, 1.4559e-07, 6.2291e-08, 1.0427e-07, 7.7582e-08, 8.9429e-08,\n",
       "            5.7767e-08, 8.6372e-08, 1.0909e-07, 7.0765e-08, 5.5000e-08, 3.0432e-08,\n",
       "            3.3430e-07, 6.7567e-08, 1.0514e-07, 2.0129e-07, 9.3170e-08, 1.1094e-07,\n",
       "            1.4359e-08, 1.6155e-08, 9.4668e-08, 3.0804e-08, 6.5956e-08, 8.1981e-08,\n",
       "            3.4370e-08, 1.5167e-07, 3.0217e-08, 3.4580e-08, 7.6050e-08, 5.6736e-08,\n",
       "            1.9911e-08, 1.7917e-07, 1.2558e-07, 1.4555e-07, 6.5325e-08, 5.6636e-08,\n",
       "            4.3739e-08, 5.3332e-08, 7.0798e-08, 4.5200e-08, 1.7382e-07, 1.2147e-07,\n",
       "            8.8025e-08, 4.7809e-08, 1.0255e-07, 3.3427e-08, 8.3539e-08, 9.0740e-08,\n",
       "            1.6854e-07, 2.9594e-07, 1.4890e-07, 1.1722e-07, 8.8325e-08, 7.2182e-07,\n",
       "            3.3160e-08, 2.0167e-08, 1.0788e-07, 5.7293e-08, 7.6268e-08, 8.7814e-07,\n",
       "            1.2978e-07, 5.9786e-08, 2.5308e-07, 2.1403e-07, 1.8505e-07, 1.0686e-07,\n",
       "            6.1742e-08, 3.2575e-08, 1.6398e-07, 1.2745e-07, 3.3927e-08, 1.0734e-07,\n",
       "            1.1233e-07, 3.5835e-08, 6.5611e-08, 5.8281e-08, 2.8088e-08, 4.1971e-08,\n",
       "            2.2209e-07, 7.6363e-08, 1.8857e-07, 9.1319e-08, 7.5954e-08, 7.5039e-08,\n",
       "            9.5107e-08, 5.8883e-08, 1.2232e-07, 7.0494e-08, 2.2085e-08, 9.0463e-09,\n",
       "            2.2366e-08, 3.2933e-07])},\n",
       "   4: {'step': 23400,\n",
       "    'exp_avg': tensor([-1.1771e-04,  6.6987e-05, -4.3232e-05,  2.0164e-04, -4.5426e-04,\n",
       "            -4.6520e-05, -1.8663e-04, -3.2553e-04, -7.3302e-05, -1.8543e-04,\n",
       "            -1.4446e-04,  3.2208e-06,  1.0368e-04, -7.1020e-05, -7.1269e-05,\n",
       "             6.9124e-06, -2.0231e-04,  1.6239e-05,  1.3825e-05, -4.6110e-04,\n",
       "             1.2031e-05,  1.3220e-05, -4.4176e-05, -1.8117e-04, -1.9175e-04,\n",
       "            -2.3719e-04, -7.0279e-04, -2.4076e-04,  8.5683e-05,  4.7316e-04,\n",
       "             2.9764e-04,  6.7865e-05,  4.3312e-06,  3.6705e-05, -2.1103e-05,\n",
       "            -2.8731e-04, -2.1100e-05, -4.6028e-05, -1.3470e-04, -1.1030e-04,\n",
       "            -1.2481e-05, -9.1878e-05,  4.5155e-05,  1.1516e-05, -6.2644e-04,\n",
       "            -3.2111e-04, -1.7422e-04,  1.3830e-05, -8.3324e-04, -1.7632e-04,\n",
       "            -8.0709e-05,  5.8021e-05, -4.0004e-04,  7.5753e-06,  9.8720e-07,\n",
       "            -6.2639e-05, -4.8543e-04,  2.9628e-05,  7.0320e-05, -1.5185e-04,\n",
       "            -8.8271e-05, -6.3715e-05,  1.9188e-05, -8.9740e-06, -1.9120e-04,\n",
       "            -7.2019e-05,  5.1277e-05, -1.0066e-04,  2.5725e-05,  1.8051e-04,\n",
       "             9.8145e-06, -1.9386e-05,  1.3184e-04,  3.0730e-06, -1.8016e-04,\n",
       "            -4.1114e-05,  1.2259e-04, -9.8292e-04, -4.5447e-04, -1.3867e-04,\n",
       "            -3.2232e-05, -7.0620e-05, -7.5799e-06, -4.2390e-04, -1.4677e-04,\n",
       "            -7.0595e-05,  3.9389e-04,  2.2273e-05,  1.1093e-04, -2.3960e-04,\n",
       "             4.2487e-05, -2.9762e-04, -1.3454e-04,  8.4563e-05,  3.5781e-07,\n",
       "            -2.1579e-04,  4.3992e-05, -1.0462e-04, -1.7642e-04, -6.6633e-04,\n",
       "            -2.2069e-05, -5.5064e-05,  1.6031e-06, -6.4416e-05, -3.1806e-04,\n",
       "             9.2437e-05,  1.1182e-05, -9.2789e-06,  1.5968e-04, -2.1395e-04,\n",
       "             3.7111e-05, -4.0746e-05, -2.8635e-05, -1.0790e-04,  3.5899e-04,\n",
       "            -1.1197e-04,  2.9769e-04, -3.8622e-04, -2.1496e-04,  1.3520e-04,\n",
       "            -5.5999e-05,  2.8673e-04,  6.3884e-04,  2.9850e-04, -2.6511e-04,\n",
       "             3.6556e-06, -6.3768e-04, -4.1363e-04, -2.9063e-05, -2.3458e-05,\n",
       "             1.4382e-05,  3.0691e-05, -2.0885e-09,  2.2302e-05, -6.7100e-09,\n",
       "            -4.3964e-05,  3.4776e-06,  3.4432e-05, -2.5657e-05,  3.1201e-05,\n",
       "             8.4885e-06, -1.9538e-05, -6.9341e-05,  1.5249e-05, -5.5882e-05,\n",
       "            -3.9602e-05,  1.0952e-05,  1.5425e-04,  1.8911e-04, -7.2572e-05,\n",
       "             2.8137e-05,  4.1905e-05,  5.5471e-05, -2.5506e-05,  1.4875e-04,\n",
       "             1.0710e-04,  8.8844e-05,  1.0189e-04,  1.3198e-04,  1.4153e-05,\n",
       "             6.8537e-06,  8.8787e-05, -8.9356e-05, -8.5881e-05,  2.5170e-05,\n",
       "            -3.6433e-05, -4.1412e-05,  5.0620e-05, -6.2715e-06,  1.5487e-05,\n",
       "            -1.8308e-05, -1.3548e-05,  3.1442e-04,  1.1135e-04, -4.8183e-06,\n",
       "             4.0039e-05, -7.4880e-05, -3.2362e-05, -1.5791e-04,  8.3215e-06,\n",
       "             8.7835e-05,  1.5747e-05, -8.1276e-06,  3.5975e-06, -1.9173e-04,\n",
       "             1.7553e-04,  1.3812e-05,  4.9500e-05,  4.9271e-05,  1.8448e-04,\n",
       "            -6.4056e-05,  5.6549e-05, -5.1410e-05, -5.8997e-06,  3.7389e-06,\n",
       "             1.9112e-04,  1.5337e-06, -4.1187e-05,  4.3873e-05,  3.7774e-06,\n",
       "             6.9175e-05,  7.1387e-06, -3.7702e-05, -2.9701e-05,  6.6600e-05,\n",
       "            -4.1360e-04, -2.7610e-04,  5.9746e-06,  3.7751e-05,  3.2245e-05,\n",
       "            -4.5316e-05, -4.3326e-05,  2.2204e-05, -1.0988e-05,  2.1376e-04,\n",
       "             9.2924e-05, -1.6045e-05, -1.2383e-04,  3.9419e-05, -2.1161e-04,\n",
       "            -1.1129e-05,  1.0233e-04,  4.5545e-05,  4.2795e-05,  6.8488e-05,\n",
       "            -6.1065e-05, -6.2988e-06,  4.2076e-05,  6.4292e-05, -3.4302e-05,\n",
       "            -7.2392e-05,  1.8532e-05,  3.8967e-05,  1.1613e-04, -6.0898e-05,\n",
       "            -1.3225e-05,  4.5903e-05,  7.8230e-05, -7.2347e-07, -5.8372e-05,\n",
       "             3.7583e-05,  2.4441e-05,  5.8360e-05, -6.9700e-05, -4.2785e-05,\n",
       "             1.5414e-04, -1.7874e-04, -3.8133e-05,  1.8745e-04,  3.4156e-04,\n",
       "             7.3843e-05,  1.1746e-04, -7.7122e-04, -3.9373e-06, -1.1086e-03,\n",
       "            -1.7631e-05, -5.2384e-04, -2.6004e-04, -8.8779e-04,  5.5473e-04,\n",
       "            -2.6008e-04, -1.0887e-03, -3.6521e-04,  1.4426e-03, -4.5246e-04,\n",
       "             8.4465e-04,  3.3066e-04,  1.0978e-03, -4.1913e-04,  2.9382e-04,\n",
       "             1.6485e-04,  1.5557e-04,  1.7148e-03,  1.3670e-04,  4.4794e-05,\n",
       "             5.5598e-04,  9.2127e-04,  1.3136e-04, -1.7793e-03, -1.8981e-04,\n",
       "            -3.1256e-05,  7.6748e-04,  1.4661e-03, -5.6689e-04, -5.4857e-04,\n",
       "             6.6316e-04, -2.6700e-04, -3.9309e-04,  3.3872e-04,  2.5269e-04,\n",
       "             4.4587e-04,  4.9659e-04,  4.8693e-04,  5.1331e-04,  5.2506e-04,\n",
       "             6.4046e-04,  3.4968e-05,  2.7223e-04, -6.6226e-04, -3.1039e-04,\n",
       "             1.1355e-03,  2.9000e-04, -3.5720e-04, -6.5248e-05,  1.2933e-04,\n",
       "            -3.7307e-04,  4.4361e-04, -3.9562e-04, -8.5571e-04, -1.5765e-03,\n",
       "             2.2819e-05,  5.5205e-05, -7.2593e-04, -8.6615e-04, -1.7500e-04,\n",
       "             2.9144e-04, -2.7043e-04, -2.9126e-04,  1.6398e-04, -1.5050e-04,\n",
       "             5.8882e-04,  1.0633e-03, -8.1012e-05, -4.3940e-04,  1.1811e-04,\n",
       "            -6.7800e-04,  1.6986e-04, -2.3481e-04, -6.1133e-04, -2.0849e-04,\n",
       "             8.8455e-04, -2.6703e-04,  3.0874e-04,  2.1330e-03,  1.2393e-03,\n",
       "             7.2457e-05, -4.8887e-04, -4.1367e-04, -3.2673e-04, -1.0392e-03,\n",
       "             1.8053e-03, -1.2973e-03, -6.9971e-04,  1.4137e-04, -2.8039e-04,\n",
       "             9.7005e-05, -2.2101e-04,  4.3873e-04, -8.3439e-05,  3.2153e-04,\n",
       "            -7.1326e-04, -3.7908e-04,  6.0086e-04,  2.5339e-04, -6.2464e-04,\n",
       "             4.0282e-04, -2.9636e-04,  1.3877e-04, -9.4497e-05,  3.9090e-04,\n",
       "            -1.1500e-03,  1.8513e-03, -3.5314e-04,  2.4391e-04, -3.3831e-04,\n",
       "             5.8212e-04,  1.3038e-04,  5.1427e-04,  6.2068e-04, -1.1433e-04,\n",
       "             2.4652e-04, -1.0080e-06, -1.0681e-04,  2.8144e-04, -4.5799e-05,\n",
       "             4.1000e-04,  5.3191e-04,  4.1102e-04,  7.6920e-04,  3.1899e-04,\n",
       "            -6.4134e-04,  5.0861e-05, -6.2832e-05,  2.1168e-04, -3.2086e-04,\n",
       "             9.1682e-05, -3.3036e-05,  2.3947e-04, -1.3248e-04, -1.1351e-04,\n",
       "            -2.1740e-04, -3.4491e-04, -3.3686e-04, -1.8638e-04, -1.4005e-04,\n",
       "            -1.0663e-04,  2.3754e-04, -2.4790e-04, -1.0721e-04, -1.5986e-06,\n",
       "            -1.2385e-04,  2.8849e-06,  8.6775e-06, -2.5174e-04,  1.2788e-04,\n",
       "             2.1636e-04, -1.1712e-04, -1.3675e-04, -1.6979e-04, -2.5627e-04,\n",
       "            -1.3832e-03, -1.4261e-04, -3.9161e-06,  1.2905e-04,  2.3979e-04,\n",
       "             8.7072e-05, -1.8349e-05, -7.2893e-05,  5.0451e-05, -5.1185e-04,\n",
       "             4.2243e-05,  7.5383e-05, -9.4200e-05,  2.0909e-04, -9.6403e-06,\n",
       "            -8.9552e-05, -2.2045e-05, -3.3135e-05, -2.4919e-04, -1.4518e-04,\n",
       "            -1.7308e-04, -1.5664e-06,  3.8735e-04, -2.8778e-04, -1.0815e-04,\n",
       "             2.0214e-04, -2.5160e-04, -2.1990e-05,  1.1173e-05, -8.5791e-05,\n",
       "            -2.4210e-04, -4.2975e-05,  2.0024e-04, -8.5269e-05, -1.5587e-05,\n",
       "            -1.8130e-04,  1.1287e-04,  2.8953e-05, -1.2686e-04, -1.1333e-04,\n",
       "            -1.1496e-05, -1.1798e-04,  1.5043e-04,  4.6474e-05,  3.5896e-05,\n",
       "            -2.1255e-05,  5.5863e-05,  3.1751e-05, -2.7514e-04, -2.7298e-05,\n",
       "            -5.1856e-05, -2.8170e-04, -3.7039e-04, -1.3826e-04,  1.0309e-05,\n",
       "             4.7996e-05,  6.8787e-05, -5.9960e-04, -3.3718e-04, -1.6672e-04,\n",
       "             4.6920e-04,  6.9649e-05, -1.0860e-04, -1.0136e-04,  1.0944e-04,\n",
       "            -1.2762e-04, -2.3882e-04,  1.4691e-04, -1.9697e-04, -3.5313e-04,\n",
       "             7.6508e-04, -1.3809e-04, -2.2948e-04, -6.2693e-04, -6.6922e-05,\n",
       "            -1.0652e-04, -2.7557e-05, -6.7598e-05, -5.0081e-04,  2.6773e-04,\n",
       "            -3.3973e-05, -1.1743e-04, -1.0315e-04, -1.1069e-04,  1.7357e-04,\n",
       "            -1.2983e-04, -1.4025e-05, -1.6812e-04,  3.2179e-04, -1.7023e-04,\n",
       "             3.1880e-04, -2.8939e-04, -6.5332e-05, -2.2521e-05, -1.4335e-04,\n",
       "             2.3760e-04, -6.9425e-05,  2.1891e-04, -5.4267e-05,  1.8407e-05,\n",
       "             7.4587e-05, -2.8806e-04]),\n",
       "    'exp_avg_sq': tensor([3.9725e-08, 3.6270e-08, 3.6814e-08, 1.0375e-07, 2.9960e-07, 3.6226e-08,\n",
       "            5.6453e-08, 4.9379e-08, 4.5074e-08, 6.7302e-08, 2.9174e-07, 5.8942e-08,\n",
       "            2.0015e-08, 1.5763e-08, 6.4097e-08, 2.8940e-08, 4.4244e-08, 4.2892e-08,\n",
       "            3.9349e-09, 1.2275e-07, 6.7738e-08, 1.1142e-08, 6.6715e-08, 6.2088e-08,\n",
       "            1.3225e-07, 1.1282e-07, 2.2783e-07, 1.0832e-06, 3.7499e-08, 2.3202e-07,\n",
       "            2.3437e-08, 4.0495e-08, 7.6621e-09, 3.7623e-08, 4.8224e-08, 3.4546e-07,\n",
       "            4.5614e-08, 2.6115e-08, 8.0799e-08, 2.7280e-08, 1.8949e-08, 6.9703e-08,\n",
       "            3.3683e-08, 7.2153e-08, 2.0401e-06, 7.8764e-08, 4.3318e-08, 1.7108e-08,\n",
       "            1.4654e-06, 3.5447e-08, 9.3797e-08, 1.1971e-07, 1.8750e-07, 1.0628e-07,\n",
       "            7.3205e-09, 8.6030e-09, 1.4244e-07, 4.1464e-08, 3.9640e-08, 9.7284e-08,\n",
       "            4.7592e-08, 1.1400e-08, 1.3202e-08, 2.8581e-08, 5.9059e-08, 4.6893e-08,\n",
       "            1.3265e-08, 1.3692e-07, 3.4003e-08, 1.6243e-07, 2.6701e-08, 2.1662e-08,\n",
       "            5.1236e-08, 3.0126e-08, 4.4982e-08, 3.3163e-08, 8.0344e-08, 3.2510e-07,\n",
       "            5.6753e-08, 5.0754e-08, 9.0587e-08, 2.4662e-08, 1.2464e-07, 5.8908e-08,\n",
       "            1.4407e-07, 9.1344e-07, 1.1975e-07, 4.0484e-08, 1.1693e-07, 3.3290e-07,\n",
       "            4.7575e-08, 1.1235e-08, 2.8678e-08, 3.1393e-08, 6.4511e-08, 1.4256e-07,\n",
       "            1.3338e-07, 2.6394e-08, 1.1308e-07, 1.3794e-07, 3.3313e-08, 2.4901e-08,\n",
       "            3.3958e-08, 2.2183e-08, 7.2740e-08, 5.5683e-08, 3.4472e-08, 7.8382e-08,\n",
       "            1.8380e-07, 4.0737e-08, 7.9373e-08, 9.1452e-08, 3.4034e-08, 2.5135e-08,\n",
       "            1.2814e-07, 6.4375e-08, 1.2865e-07, 1.7500e-07, 7.2360e-08, 4.2340e-08,\n",
       "            8.0288e-08, 4.2505e-08, 1.8976e-07, 5.9722e-08, 4.1955e-07, 7.7956e-09,\n",
       "            2.5065e-07, 9.4765e-08, 2.4849e-08, 8.6565e-09, 1.5083e-08, 1.0557e-07,\n",
       "            2.4612e-08, 1.1709e-08, 1.3527e-08, 7.8938e-09, 1.9389e-08, 5.3589e-08,\n",
       "            1.4776e-07, 3.1984e-08, 1.0426e-08, 1.3861e-08, 3.8922e-08, 4.3890e-08,\n",
       "            1.4628e-08, 2.4568e-08, 5.7939e-10, 5.2988e-08, 3.7714e-08, 5.0789e-08,\n",
       "            1.3813e-08, 1.1796e-08, 6.6782e-09, 2.2975e-08, 1.8993e-07, 2.8400e-07,\n",
       "            3.2807e-08, 1.2980e-07, 5.1736e-09, 1.3743e-08, 5.3919e-09, 3.2974e-08,\n",
       "            8.6376e-08, 5.0096e-07, 3.0409e-08, 7.5780e-08, 4.8640e-08, 3.5946e-08,\n",
       "            1.1273e-08, 1.5621e-08, 3.1613e-08, 1.9843e-08, 3.5524e-07, 1.3920e-08,\n",
       "            1.1698e-08, 2.4279e-08, 3.3383e-08, 2.9346e-08, 8.1812e-08, 9.9124e-08,\n",
       "            1.0860e-07, 1.2459e-07, 1.4669e-09, 1.3178e-09, 4.4293e-08, 4.2657e-08,\n",
       "            1.7120e-08, 4.1556e-08, 5.3678e-08, 2.3218e-08, 1.7058e-08, 3.4836e-09,\n",
       "            2.2320e-08, 3.2206e-08, 4.0269e-09, 1.1376e-07, 3.8675e-08, 1.1555e-07,\n",
       "            1.2269e-08, 3.6493e-08, 1.5716e-08, 2.1308e-08, 3.9088e-08, 1.1100e-08,\n",
       "            4.8656e-08, 1.4150e-07, 1.7357e-08, 1.2127e-08, 4.9480e-08, 1.0240e-08,\n",
       "            3.4633e-08, 6.2652e-08, 1.1142e-07, 8.5875e-08, 2.7290e-08, 3.6450e-08,\n",
       "            6.5016e-08, 1.4761e-07, 1.6573e-08, 8.6550e-09, 1.4148e-08, 1.2860e-08,\n",
       "            4.9766e-08, 6.8286e-08, 7.4688e-08, 1.3680e-08, 4.2160e-08, 1.4883e-07,\n",
       "            1.4473e-07, 1.5292e-08, 3.5779e-08, 1.8425e-08, 6.5811e-08, 5.9319e-08,\n",
       "            1.7105e-08, 8.7002e-08, 1.0149e-07, 1.8527e-08, 5.9168e-08, 3.8650e-08,\n",
       "            1.2083e-08, 2.9090e-09, 3.3858e-08, 7.7459e-09, 6.8435e-08, 1.1763e-07,\n",
       "            1.1322e-07, 8.3851e-09, 8.7324e-08, 4.8786e-08, 8.4657e-08, 1.2507e-08,\n",
       "            3.8295e-06, 2.6502e-09, 1.3633e-06, 2.9138e-08, 5.2584e-07, 1.1331e-07,\n",
       "            6.0346e-07, 3.8550e-07, 2.4877e-06, 4.2568e-07, 5.4576e-07, 2.6682e-07,\n",
       "            2.1478e-07, 1.4022e-06, 2.6662e-06, 1.7274e-07, 5.4092e-08, 2.4592e-07,\n",
       "            5.5940e-07, 1.8549e-07, 1.1491e-06, 3.7243e-07, 3.2446e-08, 4.6899e-07,\n",
       "            1.3755e-06, 2.5972e-08, 1.5652e-06, 2.7944e-07, 6.3901e-08, 4.1519e-07,\n",
       "            5.0677e-07, 2.5805e-06, 2.7717e-07, 2.4860e-07, 6.3673e-08, 4.6730e-07,\n",
       "            2.8119e-08, 2.8472e-07, 3.5459e-07, 6.9730e-07, 6.1980e-07, 1.8840e-07,\n",
       "            7.5029e-07, 2.0422e-07, 2.3907e-07, 1.3984e-07, 5.8249e-07, 7.4535e-07,\n",
       "            6.1638e-06, 3.7486e-07, 5.0725e-07, 2.0730e-07, 4.5753e-07, 5.1258e-07,\n",
       "            3.6306e-07, 2.5476e-07, 4.7339e-07, 7.3936e-07, 6.2770e-08, 8.5867e-08,\n",
       "            1.1438e-06, 1.2760e-06, 3.8607e-07, 9.4215e-07, 6.1249e-07, 1.0451e-07,\n",
       "            8.4176e-08, 5.2542e-08, 4.0463e-07, 1.1468e-06, 3.1408e-08, 6.8329e-07,\n",
       "            1.8124e-07, 3.0993e-07, 1.7690e-07, 2.1143e-07, 5.5272e-07, 3.7209e-07,\n",
       "            4.9881e-07, 5.8278e-07, 1.5304e-07, 2.0665e-06, 4.0949e-07, 7.8060e-07,\n",
       "            1.0255e-06, 3.4561e-07, 4.3840e-07, 9.2807e-07, 7.7270e-07, 4.1724e-06,\n",
       "            2.9076e-07, 4.5808e-07, 5.7620e-07, 1.8940e-06, 2.8130e-07, 3.1369e-08,\n",
       "            1.8146e-07, 6.1992e-07, 1.2225e-06, 1.7169e-06, 6.1526e-07, 1.0551e-07,\n",
       "            4.8663e-07, 1.9936e-07, 4.2988e-07, 5.1125e-08, 2.4810e-07, 3.3678e-07,\n",
       "            3.1087e-07, 1.3951e-06, 5.4467e-07, 9.2814e-07, 7.4365e-07, 3.8431e-07,\n",
       "            5.3990e-07, 9.9587e-07, 2.9813e-07, 9.0032e-08, 5.0114e-08, 1.2443e-07,\n",
       "            1.3358e-07, 1.2082e-07, 2.6540e-07, 1.9370e-07, 4.9035e-07, 3.2581e-07,\n",
       "            5.6475e-07, 1.8004e-07, 6.1809e-07, 6.6839e-08, 4.0729e-08, 4.2854e-07,\n",
       "            5.9488e-08, 6.1119e-08, 3.1695e-08, 7.5184e-08, 5.2981e-07, 3.8404e-08,\n",
       "            4.1300e-08, 6.3828e-08, 3.0905e-07, 7.3147e-08, 2.7465e-07, 7.7450e-08,\n",
       "            4.8436e-08, 6.9610e-08, 9.0951e-08, 7.0592e-08, 4.3413e-08, 1.2588e-07,\n",
       "            4.6657e-09, 7.9814e-08, 1.3185e-07, 3.6366e-08, 5.7969e-08, 5.0561e-08,\n",
       "            1.7487e-07, 1.6922e-07, 3.5088e-07, 3.3831e-07, 8.3394e-08, 8.7564e-08,\n",
       "            3.4443e-08, 3.5601e-08, 9.0236e-09, 1.3403e-07, 1.5705e-07, 1.2808e-06,\n",
       "            1.0273e-07, 1.4559e-07, 6.2291e-08, 1.0427e-07, 7.7582e-08, 8.9429e-08,\n",
       "            5.7767e-08, 8.6372e-08, 1.0909e-07, 7.0765e-08, 5.5000e-08, 3.0432e-08,\n",
       "            3.3430e-07, 6.7567e-08, 1.0514e-07, 2.0129e-07, 9.3170e-08, 1.1094e-07,\n",
       "            1.4359e-08, 1.6155e-08, 9.4668e-08, 3.0804e-08, 6.5956e-08, 8.1981e-08,\n",
       "            3.4370e-08, 1.5167e-07, 3.0217e-08, 3.4580e-08, 7.6050e-08, 5.6736e-08,\n",
       "            1.9911e-08, 1.7917e-07, 1.2558e-07, 1.4555e-07, 6.5325e-08, 5.6636e-08,\n",
       "            4.3739e-08, 5.3332e-08, 7.0798e-08, 4.5200e-08, 1.7382e-07, 1.2147e-07,\n",
       "            8.8025e-08, 4.7809e-08, 1.0255e-07, 3.3427e-08, 8.3539e-08, 9.0740e-08,\n",
       "            1.6854e-07, 2.9594e-07, 1.4890e-07, 1.1722e-07, 8.8325e-08, 7.2182e-07,\n",
       "            3.3160e-08, 2.0167e-08, 1.0788e-07, 5.7293e-08, 7.6268e-08, 8.7814e-07,\n",
       "            1.2978e-07, 5.9786e-08, 2.5308e-07, 2.1403e-07, 1.8505e-07, 1.0686e-07,\n",
       "            6.1742e-08, 3.2575e-08, 1.6398e-07, 1.2745e-07, 3.3927e-08, 1.0734e-07,\n",
       "            1.1233e-07, 3.5835e-08, 6.5611e-08, 5.8281e-08, 2.8088e-08, 4.1971e-08,\n",
       "            2.2209e-07, 7.6363e-08, 1.8857e-07, 9.1319e-08, 7.5954e-08, 7.5039e-08,\n",
       "            9.5107e-08, 5.8883e-08, 1.2232e-07, 7.0494e-08, 2.2085e-08, 9.0463e-09,\n",
       "            2.2366e-08, 3.2933e-07])},\n",
       "   5: {'step': 23400,\n",
       "    'exp_avg': tensor([[ 1.4704e-05, -4.0615e-05,  2.5589e-05,  ...,  6.5496e-05,\n",
       "             -5.5175e-06,  3.8080e-05],\n",
       "            [ 1.4712e-05, -5.4800e-05,  5.9841e-05,  ...,  2.4234e-05,\n",
       "              2.2791e-05,  3.9543e-05],\n",
       "            [ 7.9344e-06,  3.4707e-05,  1.2810e-05,  ...,  2.8646e-06,\n",
       "             -2.4297e-05, -1.3907e-05],\n",
       "            ...,\n",
       "            [ 3.2628e-05,  5.3305e-05, -1.5985e-06,  ...,  6.6486e-06,\n",
       "              4.9745e-05, -2.9935e-05],\n",
       "            [-2.2789e-05, -1.4547e-05, -4.0317e-05,  ...,  2.5468e-05,\n",
       "             -2.1719e-05, -1.9498e-06],\n",
       "            [ 1.6417e-05, -3.8949e-05, -3.9746e-05,  ...,  5.8726e-05,\n",
       "             -8.9389e-05,  2.5914e-05]]),\n",
       "    'exp_avg_sq': tensor([[1.1053e-08, 9.4158e-09, 9.2654e-09,  ..., 1.4587e-08, 5.3135e-09,\n",
       "             4.7827e-09],\n",
       "            [5.7973e-09, 8.6414e-09, 5.2232e-09,  ..., 1.5013e-08, 8.5105e-09,\n",
       "             9.5058e-09],\n",
       "            [4.5305e-09, 5.3592e-09, 4.1449e-09,  ..., 3.8939e-09, 4.7874e-09,\n",
       "             4.1765e-09],\n",
       "            ...,\n",
       "            [9.3489e-09, 6.3627e-09, 5.6355e-09,  ..., 2.2834e-09, 4.6197e-09,\n",
       "             3.5756e-09],\n",
       "            [3.3574e-09, 5.6246e-09, 5.0851e-09,  ..., 5.1584e-09, 7.7410e-09,\n",
       "             3.8543e-09],\n",
       "            [4.2619e-09, 1.2326e-08, 1.2056e-08,  ..., 1.5348e-08, 1.0949e-08,\n",
       "             8.6190e-09]])},\n",
       "   6: {'step': 23400,\n",
       "    'exp_avg': tensor([[-8.2541e-06,  2.1513e-05, -8.5827e-05,  ..., -1.0020e-04,\n",
       "             -9.1827e-05, -1.2534e-05],\n",
       "            [ 1.1221e-05,  1.2552e-04, -3.1234e-06,  ..., -5.9988e-05,\n",
       "             -5.1281e-05,  8.7452e-06],\n",
       "            [ 2.8529e-05,  1.3677e-05, -3.4547e-05,  ...,  5.0593e-07,\n",
       "             -3.2395e-06,  5.3642e-05],\n",
       "            ...,\n",
       "            [-1.9834e-05, -4.7244e-05,  2.8290e-05,  ..., -4.4740e-05,\n",
       "             -1.2419e-05,  6.0594e-05],\n",
       "            [ 1.3252e-05,  1.0374e-04, -5.3756e-05,  ..., -3.0765e-05,\n",
       "             -8.4337e-05,  3.5247e-06],\n",
       "            [ 8.7395e-05,  1.0328e-04, -4.4708e-05,  ..., -1.0358e-04,\n",
       "             -1.5798e-05,  1.2393e-04]]),\n",
       "    'exp_avg_sq': tensor([[4.3079e-08, 3.5571e-08, 1.6750e-08,  ..., 3.9297e-08, 1.8634e-08,\n",
       "             3.3149e-08],\n",
       "            [2.8925e-08, 3.8154e-08, 1.5537e-08,  ..., 4.1259e-08, 1.6978e-08,\n",
       "             3.4240e-08],\n",
       "            [1.6355e-08, 1.7189e-08, 9.9548e-09,  ..., 1.9995e-08, 6.8001e-09,\n",
       "             1.4771e-08],\n",
       "            ...,\n",
       "            [1.5400e-08, 1.9663e-08, 7.9183e-09,  ..., 2.5145e-08, 1.0551e-08,\n",
       "             2.5441e-08],\n",
       "            [1.5323e-08, 1.7312e-08, 6.2280e-09,  ..., 1.9313e-08, 8.0793e-09,\n",
       "             1.5323e-08],\n",
       "            [2.0133e-08, 2.0371e-08, 1.3691e-08,  ..., 2.6754e-08, 1.3779e-08,\n",
       "             2.1378e-08]])},\n",
       "   7: {'step': 23400,\n",
       "    'exp_avg': tensor([ 1.0640e-04,  8.8546e-05, -1.8000e-04, -1.8500e-04, -1.4342e-04,\n",
       "            -1.6117e-04, -4.3620e-04, -7.1779e-04, -1.5066e-05, -1.8443e-05,\n",
       "            -3.4711e-05,  3.3235e-04, -1.2607e-05, -6.0622e-05,  1.0595e-04,\n",
       "            -3.7757e-06,  7.7479e-05,  4.0443e-05, -5.7150e-04,  2.7325e-04,\n",
       "             3.6759e-04, -8.5756e-05,  1.2106e-04, -5.1435e-04,  7.1053e-04,\n",
       "            -1.3937e-04, -1.2763e-04,  1.2640e-04,  1.8842e-05,  8.5198e-05,\n",
       "            -1.1587e-04, -7.3727e-06,  2.3249e-04,  1.0117e-04,  3.7616e-04,\n",
       "             2.6371e-04, -5.9608e-06, -2.2507e-04, -2.3983e-04,  4.8022e-05,\n",
       "             2.4271e-04, -4.4898e-04, -2.7724e-04,  2.1039e-04, -1.3954e-04,\n",
       "            -9.8080e-05,  2.4741e-04, -4.0811e-04,  3.8723e-04,  1.4742e-04,\n",
       "            -5.6001e-05,  3.2808e-06, -1.4718e-04,  7.2330e-05, -8.6560e-05,\n",
       "             9.6799e-05, -1.5054e-04,  5.6963e-05, -4.1554e-05,  9.5746e-05,\n",
       "            -1.6235e-04, -4.6560e-04,  2.2238e-04, -3.7615e-04,  1.4515e-05,\n",
       "             1.9295e-04,  1.8365e-04, -1.4735e-04,  1.5041e-04,  1.5141e-04,\n",
       "            -4.2108e-04,  3.0745e-04,  1.9955e-04, -2.5746e-04, -2.0836e-04,\n",
       "            -4.7805e-05,  3.8066e-05,  6.0072e-05, -3.9590e-05,  7.4272e-05,\n",
       "             3.4018e-05,  4.1209e-05,  1.2890e-04,  1.6609e-05, -2.2219e-04,\n",
       "            -3.2968e-04, -8.3462e-04,  1.4876e-04, -1.2074e-04,  5.4632e-05,\n",
       "            -2.5683e-04, -2.8799e-05,  1.3037e-04, -2.5770e-04, -6.5509e-05,\n",
       "            -5.0441e-05,  1.7666e-04,  1.0919e-04,  6.3434e-05,  4.2508e-04,\n",
       "             3.2855e-05,  1.5068e-04,  2.1178e-04, -1.1729e-04, -4.6036e-05,\n",
       "            -7.1996e-05, -5.2950e-05,  9.6766e-05, -6.2894e-06, -1.5635e-04,\n",
       "            -1.4692e-04, -3.3969e-04,  3.0986e-05, -1.3798e-05,  1.9076e-04,\n",
       "            -2.9933e-04,  1.3024e-04, -5.1939e-05, -8.7978e-05, -2.5581e-04,\n",
       "            -1.6719e-04, -1.8841e-04, -8.5172e-05, -2.6263e-04, -6.1219e-05,\n",
       "            -7.0290e-07,  2.3891e-04,  4.2090e-04, -1.0652e-04, -4.4287e-05,\n",
       "             9.1005e-05,  4.5344e-04, -5.5759e-05, -2.1650e-04,  7.7219e-05,\n",
       "             7.5557e-04, -3.8354e-05,  1.8256e-05,  1.5574e-04,  5.5477e-05,\n",
       "             4.9532e-05,  1.8698e-04, -9.7821e-05,  9.8278e-05,  1.3434e-04,\n",
       "            -2.0499e-04, -1.6517e-05,  1.2521e-04,  3.9458e-05,  4.0891e-05,\n",
       "             4.0198e-05,  6.3517e-04,  1.3447e-04, -1.6544e-05, -7.0756e-05,\n",
       "             1.2172e-04, -2.7026e-04, -3.4013e-04, -1.9034e-04,  2.0075e-04,\n",
       "             2.2280e-04,  5.3623e-06,  1.4134e-04, -9.7492e-06,  1.6142e-07,\n",
       "            -1.1125e-04, -5.5354e-05,  4.6787e-04,  2.4515e-04,  7.2141e-05,\n",
       "             1.4454e-04, -2.2713e-04, -1.1548e-04,  1.6349e-05,  1.0683e-05,\n",
       "             1.0697e-04,  1.4223e-04,  2.0027e-04,  1.4154e-04,  3.4174e-04,\n",
       "             7.8960e-05, -1.2833e-04,  2.9522e-04,  7.1704e-05,  2.4343e-04,\n",
       "            -5.7530e-05, -2.5776e-05, -6.2615e-05, -2.4825e-04, -6.2195e-05,\n",
       "             1.1361e-04, -5.4833e-04, -1.4410e-04, -3.1027e-04, -7.0631e-05,\n",
       "             1.4272e-05, -3.4680e-04, -3.8649e-05,  2.3809e-04,  1.7313e-04,\n",
       "             8.6174e-05,  4.8481e-05,  8.2719e-04, -6.5434e-05,  7.2919e-05,\n",
       "            -4.8551e-05, -5.9630e-05, -3.2723e-04, -3.7686e-05,  2.0460e-04,\n",
       "            -5.6326e-05,  5.5921e-05, -1.1903e-04, -8.2579e-05,  3.1460e-04,\n",
       "            -1.1500e-04, -8.1810e-05,  3.1760e-05,  3.1156e-04,  1.5599e-04,\n",
       "             1.1035e-04,  2.7094e-04,  1.1367e-04,  1.5592e-04,  1.5465e-04,\n",
       "             2.2869e-05, -5.5971e-05,  7.8647e-05, -4.7893e-05, -3.5250e-04,\n",
       "             1.8310e-04,  2.6550e-05,  6.8868e-05, -7.8301e-05,  1.6403e-04,\n",
       "             6.3373e-05, -1.9303e-05, -1.0734e-04, -1.1000e-05, -6.2038e-05,\n",
       "            -4.8306e-05,  1.8627e-05, -8.3892e-04, -3.8781e-05,  2.0851e-04,\n",
       "             1.9151e-05,  1.5719e-04, -1.2491e-04, -1.2690e-04,  1.2275e-04,\n",
       "            -2.4370e-04, -2.3010e-04,  6.6461e-05,  3.2822e-04,  5.4198e-05,\n",
       "            -3.0355e-05,  3.5635e-04, -1.2594e-04, -1.2394e-04, -6.8097e-04,\n",
       "             5.8144e-05,  1.1654e-04,  3.1230e-04, -3.5185e-03, -1.1941e-04,\n",
       "             1.8331e-05, -2.6312e-04, -1.0929e-04,  5.0573e-04, -5.0290e-04,\n",
       "            -2.0249e-06, -1.0371e-04, -1.2987e-04, -1.8295e-04, -1.6490e-03,\n",
       "            -8.3458e-04,  5.8024e-04,  1.3470e-04,  4.3759e-04,  1.5243e-04,\n",
       "            -1.5187e-03,  1.3311e-03, -4.7586e-04,  6.3016e-05, -1.6506e-04,\n",
       "             4.0625e-04,  2.4423e-04, -5.7361e-05,  1.8452e-04,  4.6452e-04,\n",
       "             1.2697e-04,  6.4593e-04,  3.9108e-04,  4.9286e-04, -7.9064e-04,\n",
       "            -2.1004e-03,  3.1653e-04, -2.0013e-05,  1.2133e-04, -2.9613e-04,\n",
       "            -3.1953e-05,  2.4981e-04, -1.2436e-04, -4.8740e-05,  3.8196e-04,\n",
       "            -5.9502e-04, -1.1209e-04, -9.3135e-05, -5.2534e-05, -2.5238e-04,\n",
       "            -2.7823e-04,  3.7357e-04,  6.8817e-05, -1.1799e-03, -3.9640e-04,\n",
       "             4.3915e-04, -5.1458e-04,  2.4350e-04,  1.4893e-04, -1.0801e-03,\n",
       "            -9.4778e-04,  4.8863e-04, -6.5007e-04, -6.7279e-04,  1.0579e-03,\n",
       "            -9.6043e-05, -2.8394e-04,  5.0859e-04,  1.4055e-04, -1.1928e-05,\n",
       "             7.7137e-05, -8.7781e-04,  8.5434e-05, -4.2228e-04, -2.3380e-05,\n",
       "            -2.0156e-04, -4.6919e-04, -2.9345e-04, -1.0852e-04,  5.9288e-04,\n",
       "            -2.3562e-04, -1.6993e-04,  2.0903e-04, -4.6044e-04, -6.4682e-04,\n",
       "             1.1037e-03, -2.2219e-05, -1.1233e-03, -1.2837e-03, -1.2148e-04,\n",
       "            -1.0112e-04,  2.1231e-05,  6.6064e-04,  1.8889e-04,  5.4081e-04,\n",
       "             3.5901e-04,  1.8575e-05, -6.5365e-04, -1.1683e-03, -2.0759e-04,\n",
       "             5.4771e-04, -1.8053e-04, -1.5870e-04,  1.3123e-04,  3.0723e-04,\n",
       "            -3.4299e-04, -7.5176e-04, -1.5557e-04, -1.9363e-05,  6.3122e-04,\n",
       "             1.8163e-03, -8.9187e-04, -4.3696e-04,  5.0866e-05, -6.8057e-04,\n",
       "             3.1706e-04,  3.1837e-04,  4.7238e-04,  5.8785e-04,  8.1797e-06,\n",
       "            -2.1233e-04, -3.7790e-05, -8.3841e-04,  2.7904e-05,  2.4995e-04,\n",
       "             7.0887e-04,  2.8036e-05,  1.5090e-05,  2.1126e-05, -2.0928e-04,\n",
       "            -3.4744e-04, -3.2456e-04, -1.5519e-04, -6.2978e-06,  6.3259e-05,\n",
       "             2.6065e-04,  7.0498e-05,  1.0487e-04,  3.3947e-04,  3.3545e-04,\n",
       "             3.7759e-04, -1.0157e-03, -4.9680e-04,  1.5776e-04,  2.8175e-05,\n",
       "             1.6498e-04,  2.2623e-04,  7.1002e-05,  9.2708e-04, -1.3219e-04,\n",
       "            -2.7690e-04,  3.2301e-04, -7.7130e-05,  1.2452e-04,  1.4360e-05,\n",
       "            -9.6870e-05,  1.1520e-04,  5.1303e-05,  1.2323e-04,  4.6762e-04,\n",
       "             3.0387e-05, -9.4496e-05, -3.3356e-04, -7.0550e-05,  2.0489e-04,\n",
       "            -2.3720e-04, -2.0911e-04,  3.1848e-04, -2.1572e-04, -2.4573e-04,\n",
       "             4.1006e-04, -6.2096e-05,  2.3107e-04,  3.3475e-04,  3.8964e-05,\n",
       "             4.9533e-05,  1.0972e-04, -9.3680e-05, -6.9689e-04, -5.0182e-05,\n",
       "            -7.4321e-05, -2.4023e-05, -4.5434e-05,  5.8875e-06, -8.2067e-05,\n",
       "             4.1199e-05,  1.3350e-04, -5.3217e-04, -7.3550e-04, -1.4703e-05,\n",
       "             3.3599e-05, -3.6833e-05, -3.8482e-04,  3.3488e-04,  2.7911e-04,\n",
       "             1.2858e-04,  3.1285e-04,  3.0168e-04, -2.9388e-04, -1.4855e-03,\n",
       "             4.5667e-05,  6.9337e-06,  1.2970e-04, -2.4894e-04, -2.1649e-05,\n",
       "             1.6348e-04, -1.1926e-04,  1.7973e-04, -1.3875e-04, -4.4067e-04,\n",
       "             5.3384e-05, -1.4279e-04, -1.1296e-04, -3.3165e-05,  1.7105e-04,\n",
       "            -5.9309e-05, -1.5138e-05, -2.3325e-04, -1.0832e-04,  2.9848e-04,\n",
       "             1.1841e-04, -6.8337e-05,  2.1103e-04,  4.2657e-04, -2.4572e-05,\n",
       "             9.7118e-05, -3.2198e-04, -5.7500e-05, -7.2488e-05, -1.4163e-04,\n",
       "            -1.4520e-05,  3.6023e-04, -4.8036e-05, -2.8340e-05,  7.6812e-05,\n",
       "             1.5140e-04,  3.0148e-04,  1.5810e-04, -1.8888e-04, -4.2884e-04,\n",
       "             3.6893e-04,  8.4553e-05,  1.6363e-04, -6.6608e-05,  1.2473e-05,\n",
       "             2.9934e-05, -1.6769e-04, -8.6157e-05, -2.9410e-04, -2.5119e-04,\n",
       "             3.5815e-04,  2.8338e-04]),\n",
       "    'exp_avg_sq': tensor([1.5485e-07, 1.7599e-07, 6.6727e-08, 1.4534e-07, 1.3003e-07, 1.8578e-07,\n",
       "            6.0297e-07, 1.6511e-06, 7.4349e-08, 7.1406e-08, 1.4422e-07, 1.1784e-07,\n",
       "            8.5199e-08, 1.7293e-07, 1.3355e-07, 6.4512e-08, 2.2719e-07, 7.0470e-09,\n",
       "            2.4901e-07, 1.0063e-07, 1.9903e-07, 8.8271e-08, 9.7525e-08, 1.7929e-07,\n",
       "            9.8868e-07, 1.5734e-07, 8.6572e-08, 1.1037e-07, 1.5038e-07, 1.6639e-07,\n",
       "            1.3801e-07, 1.2720e-07, 1.6606e-07, 1.5359e-07, 1.4763e-07, 1.6527e-07,\n",
       "            1.1527e-07, 2.0246e-07, 9.6798e-08, 9.7416e-08, 4.1557e-07, 2.7207e-07,\n",
       "            1.3853e-07, 1.5467e-07, 9.8498e-08, 6.9421e-08, 2.1923e-07, 8.6947e-08,\n",
       "            1.6712e-07, 7.8913e-08, 2.2483e-07, 1.1389e-07, 1.3272e-07, 1.6759e-07,\n",
       "            6.1090e-08, 1.7759e-07, 1.1824e-07, 4.7275e-08, 1.2510e-07, 8.9943e-08,\n",
       "            1.7008e-07, 2.8296e-07, 1.0998e-07, 1.9478e-07, 6.0140e-08, 9.4830e-08,\n",
       "            1.1725e-07, 8.6832e-08, 7.6689e-08, 1.3118e-07, 1.0516e-07, 1.7074e-07,\n",
       "            1.1190e-07, 1.1298e-07, 6.6700e-07, 2.0557e-08, 1.0461e-07, 1.3307e-07,\n",
       "            1.6759e-07, 2.1678e-07, 7.9872e-08, 1.0009e-07, 1.0731e-07, 5.7011e-08,\n",
       "            5.2396e-08, 2.2306e-07, 3.7654e-07, 9.3674e-08, 1.4896e-07, 8.5707e-08,\n",
       "            1.1786e-07, 1.7818e-07, 3.5090e-07, 7.3074e-07, 1.6826e-07, 1.2429e-07,\n",
       "            1.3127e-07, 4.9254e-08, 4.0090e-08, 1.8900e-07, 2.4714e-07, 1.0573e-07,\n",
       "            9.7981e-08, 1.7429e-07, 2.5397e-07, 3.1395e-07, 1.5892e-07, 2.6886e-07,\n",
       "            7.9264e-08, 1.4727e-07, 4.6634e-08, 1.9090e-07, 7.9499e-08, 1.9230e-07,\n",
       "            7.9380e-08, 1.2959e-07, 1.2756e-07, 1.1901e-07, 5.9234e-08, 1.2823e-07,\n",
       "            3.4182e-07, 1.4945e-07, 1.2928e-07, 2.1168e-07, 2.3013e-07, 1.4403e-07,\n",
       "            1.1566e-07, 1.2441e-07, 6.5640e-08, 1.6139e-07, 9.3397e-08, 1.1125e-07,\n",
       "            1.0013e-07, 1.0057e-07, 7.4836e-07, 1.8591e-06, 6.4632e-08, 1.1605e-07,\n",
       "            1.2235e-07, 5.9610e-08, 7.9268e-08, 5.5339e-07, 1.9024e-07, 6.6675e-08,\n",
       "            2.0779e-07, 2.4522e-08, 7.4587e-08, 8.5620e-08, 1.2489e-07, 8.2307e-08,\n",
       "            1.5920e-07, 9.6337e-08, 7.4588e-08, 1.1972e-07, 5.0541e-08, 1.3759e-07,\n",
       "            3.9045e-07, 7.0350e-08, 4.9187e-08, 1.8313e-07, 1.2887e-07, 4.3047e-08,\n",
       "            6.0768e-08, 1.1326e-07, 7.9548e-08, 1.4542e-07, 1.0189e-07, 1.1248e-07,\n",
       "            2.6992e-07, 1.0628e-07, 1.7818e-07, 1.1992e-07, 1.1227e-07, 7.1499e-08,\n",
       "            5.8814e-08, 1.1761e-07, 1.2181e-07, 1.0361e-07, 7.1911e-08, 3.4759e-07,\n",
       "            4.0268e-08, 7.2781e-08, 7.2746e-08, 7.9683e-08, 1.3872e-07, 1.1198e-07,\n",
       "            9.4134e-08, 5.2537e-08, 1.9153e-07, 2.1448e-07, 7.3178e-08, 4.3197e-07,\n",
       "            2.4462e-08, 1.4619e-07, 7.2433e-08, 1.1160e-07, 1.4707e-07, 1.3409e-07,\n",
       "            1.0755e-07, 6.2962e-08, 4.9453e-08, 3.4335e-07, 9.4629e-07, 1.8175e-08,\n",
       "            9.5382e-08, 1.4206e-07, 1.7837e-07, 3.8020e-07, 1.0999e-07, 1.3961e-07,\n",
       "            2.7904e-08, 6.3190e-08, 1.2558e-07, 2.0417e-07, 1.5352e-07, 5.8302e-08,\n",
       "            1.6737e-07, 3.8689e-08, 1.2952e-07, 1.6931e-07, 1.7729e-07, 5.2504e-07,\n",
       "            9.0189e-08, 1.6071e-07, 7.2499e-08, 8.8759e-08, 6.0923e-08, 9.6056e-08,\n",
       "            1.1435e-07, 1.1014e-07, 9.5635e-08, 1.7438e-07, 1.0183e-07, 1.7160e-07,\n",
       "            9.3611e-08, 1.5938e-07, 6.3240e-08, 2.5303e-07, 9.0844e-08, 1.5399e-07,\n",
       "            1.1626e-07, 1.0994e-07, 1.7964e-07, 1.3879e-07, 1.3283e-07, 1.3599e-07,\n",
       "            1.2692e-07, 6.3836e-08, 1.5093e-07, 1.5436e-07, 2.4271e-07, 2.2488e-07,\n",
       "            1.1014e-07, 2.5227e-07, 4.9776e-08, 2.5802e-07, 2.0136e-07, 1.3188e-07,\n",
       "            1.7045e-06, 2.0715e-06, 9.6330e-07, 2.1568e-07, 7.2444e-06, 3.3436e-05,\n",
       "            9.8913e-08, 1.7107e-06, 1.3364e-06, 1.3512e-06, 1.2986e-06, 3.8082e-06,\n",
       "            8.7531e-07, 3.9445e-07, 2.3494e-07, 1.1401e-07, 1.6015e-06, 1.6669e-06,\n",
       "            1.1330e-06, 1.8773e-06, 4.8443e-07, 4.6057e-07, 1.9512e-06, 2.6248e-06,\n",
       "            1.7505e-06, 1.1563e-06, 3.3510e-07, 7.6142e-07, 1.2344e-07, 3.5749e-08,\n",
       "            9.8111e-07, 3.7507e-07, 1.5985e-06, 4.0157e-07, 1.4408e-06, 2.6782e-07,\n",
       "            3.0580e-07, 1.8035e-06, 2.9907e-06, 7.1056e-08, 2.1505e-07, 8.7371e-07,\n",
       "            4.5492e-07, 1.1125e-06, 1.0436e-07, 1.0033e-06, 2.8951e-07, 6.7577e-07,\n",
       "            1.0255e-06, 2.1095e-06, 2.6522e-08, 6.7536e-07, 1.8730e-07, 1.0583e-06,\n",
       "            7.0392e-08, 9.2761e-07, 2.7428e-06, 6.4445e-07, 1.1646e-06, 1.0245e-07,\n",
       "            1.2591e-06, 2.8420e-06, 1.6664e-06, 2.5782e-06, 5.8708e-07, 2.1904e-06,\n",
       "            7.7068e-07, 3.8740e-06, 2.0783e-07, 4.2810e-07, 2.0215e-06, 3.1861e-08,\n",
       "            7.0907e-06, 2.0784e-06, 1.3167e-06, 2.1089e-06, 1.3238e-07, 8.5102e-07,\n",
       "            1.1247e-06, 1.3988e-06, 6.8075e-07, 1.7438e-06, 1.0543e-06, 1.6307e-07,\n",
       "            6.1869e-08, 1.1085e-06, 2.9065e-06, 1.5646e-06, 8.0334e-07, 1.2449e-06,\n",
       "            4.2324e-06, 8.1610e-06, 1.6521e-06, 6.2169e-07, 1.7582e-06, 1.1746e-07,\n",
       "            1.9919e-06, 1.2684e-06, 3.4289e-06, 7.1380e-07, 4.4350e-06, 4.9612e-07,\n",
       "            5.7400e-07, 1.5913e-07, 9.9535e-07, 1.7375e-07, 9.6290e-07, 9.5353e-07,\n",
       "            1.4021e-06, 1.2853e-07, 8.3628e-07, 6.4713e-07, 9.0961e-07, 2.6636e-06,\n",
       "            8.7560e-07, 2.3755e-06, 2.6347e-06, 5.7920e-07, 3.7733e-07, 2.0538e-06,\n",
       "            1.0132e-06, 2.1091e-07, 5.5230e-07, 6.3335e-07, 1.2674e-06, 2.2498e-08,\n",
       "            1.9007e-07, 2.1240e-07, 7.3928e-08, 7.5226e-08, 1.9833e-07, 1.4407e-07,\n",
       "            2.1503e-07, 4.4501e-07, 1.7714e-07, 1.0136e-07, 3.7872e-08, 6.3752e-08,\n",
       "            1.0060e-07, 1.4625e-07, 2.3308e-07, 1.4078e-07, 2.0829e-07, 5.1695e-07,\n",
       "            1.3702e-07, 8.4921e-08, 8.1862e-08, 8.6318e-08, 7.5393e-08, 7.5555e-08,\n",
       "            6.6902e-07, 1.0763e-07, 2.9402e-07, 2.0986e-07, 8.0484e-08, 8.1342e-08,\n",
       "            1.1300e-07, 1.3200e-07, 1.1475e-07, 1.7213e-07, 4.9839e-08, 1.9908e-07,\n",
       "            1.3640e-07, 7.1176e-08, 9.8807e-08, 1.3135e-07, 5.8300e-07, 1.3438e-07,\n",
       "            1.3431e-07, 3.6359e-07, 1.9265e-07, 1.5323e-07, 1.3183e-07, 1.9271e-07,\n",
       "            9.8260e-08, 1.3412e-07, 9.3907e-08, 1.4431e-07, 2.6551e-07, 1.5358e-07,\n",
       "            1.8528e-07, 7.3042e-08, 2.0478e-07, 2.7233e-07, 9.4876e-08, 1.3841e-07,\n",
       "            2.3252e-07, 1.1053e-07, 7.9338e-08, 3.0835e-07, 1.1238e-06, 1.7669e-07,\n",
       "            9.5751e-08, 1.2901e-07, 1.9655e-07, 1.6431e-07, 2.2223e-07, 2.8637e-07,\n",
       "            7.8207e-08, 2.2557e-07, 2.3314e-07, 6.7899e-07, 1.8754e-07, 1.1295e-07,\n",
       "            1.6221e-07, 2.2141e-07, 1.1213e-07, 1.2460e-07, 1.4292e-07, 4.3538e-07,\n",
       "            2.7948e-07, 1.3226e-07, 1.9195e-07, 9.5356e-08, 9.5973e-08, 8.8734e-08,\n",
       "            1.1503e-07, 7.6212e-08, 1.6354e-07, 3.4122e-07, 4.1419e-08, 2.7709e-07,\n",
       "            5.9589e-08, 3.6545e-07, 2.3687e-07, 1.8780e-07, 2.0655e-07, 2.0032e-07,\n",
       "            1.8105e-07, 1.6496e-07, 1.8028e-07, 7.4736e-08, 8.1395e-08, 1.2073e-07,\n",
       "            9.2119e-08, 1.9617e-07, 1.9416e-07, 2.1103e-07, 1.8757e-07, 2.2033e-07,\n",
       "            2.0920e-07, 1.2834e-07, 2.5613e-07, 1.1309e-07, 1.5036e-07, 1.2303e-07,\n",
       "            6.2266e-08, 7.3379e-08, 1.4211e-07, 1.1143e-07, 1.3649e-07, 1.4924e-07,\n",
       "            9.8087e-08, 2.1021e-07])},\n",
       "   8: {'step': 23400,\n",
       "    'exp_avg': tensor([ 1.0640e-04,  8.8546e-05, -1.8000e-04, -1.8500e-04, -1.4342e-04,\n",
       "            -1.6117e-04, -4.3620e-04, -7.1779e-04, -1.5066e-05, -1.8443e-05,\n",
       "            -3.4711e-05,  3.3235e-04, -1.2607e-05, -6.0622e-05,  1.0595e-04,\n",
       "            -3.7757e-06,  7.7479e-05,  4.0443e-05, -5.7150e-04,  2.7325e-04,\n",
       "             3.6759e-04, -8.5756e-05,  1.2106e-04, -5.1435e-04,  7.1053e-04,\n",
       "            -1.3937e-04, -1.2763e-04,  1.2640e-04,  1.8842e-05,  8.5198e-05,\n",
       "            -1.1587e-04, -7.3727e-06,  2.3249e-04,  1.0117e-04,  3.7616e-04,\n",
       "             2.6371e-04, -5.9608e-06, -2.2507e-04, -2.3983e-04,  4.8022e-05,\n",
       "             2.4271e-04, -4.4898e-04, -2.7724e-04,  2.1039e-04, -1.3954e-04,\n",
       "            -9.8080e-05,  2.4741e-04, -4.0811e-04,  3.8723e-04,  1.4742e-04,\n",
       "            -5.6001e-05,  3.2808e-06, -1.4718e-04,  7.2330e-05, -8.6560e-05,\n",
       "             9.6799e-05, -1.5054e-04,  5.6963e-05, -4.1554e-05,  9.5746e-05,\n",
       "            -1.6235e-04, -4.6560e-04,  2.2239e-04, -3.7615e-04,  1.4515e-05,\n",
       "             1.9295e-04,  1.8365e-04, -1.4735e-04,  1.5041e-04,  1.5141e-04,\n",
       "            -4.2108e-04,  3.0745e-04,  1.9955e-04, -2.5746e-04, -2.0836e-04,\n",
       "            -4.7805e-05,  3.8066e-05,  6.0072e-05, -3.9590e-05,  7.4272e-05,\n",
       "             3.4018e-05,  4.1209e-05,  1.2890e-04,  1.6609e-05, -2.2219e-04,\n",
       "            -3.2968e-04, -8.3462e-04,  1.4876e-04, -1.2074e-04,  5.4632e-05,\n",
       "            -2.5683e-04, -2.8799e-05,  1.3037e-04, -2.5770e-04, -6.5509e-05,\n",
       "            -5.0441e-05,  1.7666e-04,  1.0919e-04,  6.3434e-05,  4.2508e-04,\n",
       "             3.2855e-05,  1.5068e-04,  2.1178e-04, -1.1729e-04, -4.6036e-05,\n",
       "            -7.1996e-05, -5.2950e-05,  9.6766e-05, -6.2894e-06, -1.5635e-04,\n",
       "            -1.4692e-04, -3.3969e-04,  3.0986e-05, -1.3798e-05,  1.9076e-04,\n",
       "            -2.9933e-04,  1.3024e-04, -5.1939e-05, -8.7978e-05, -2.5581e-04,\n",
       "            -1.6719e-04, -1.8841e-04, -8.5172e-05, -2.6263e-04, -6.1219e-05,\n",
       "            -7.0288e-07,  2.3891e-04,  4.2090e-04, -1.0652e-04, -4.4287e-05,\n",
       "             9.1005e-05,  4.5344e-04, -5.5759e-05, -2.1650e-04,  7.7219e-05,\n",
       "             7.5557e-04, -3.8354e-05,  1.8256e-05,  1.5574e-04,  5.5477e-05,\n",
       "             4.9532e-05,  1.8698e-04, -9.7821e-05,  9.8278e-05,  1.3434e-04,\n",
       "            -2.0499e-04, -1.6517e-05,  1.2521e-04,  3.9458e-05,  4.0891e-05,\n",
       "             4.0198e-05,  6.3517e-04,  1.3447e-04, -1.6544e-05, -7.0756e-05,\n",
       "             1.2172e-04, -2.7026e-04, -3.4013e-04, -1.9034e-04,  2.0075e-04,\n",
       "             2.2280e-04,  5.3623e-06,  1.4134e-04, -9.7492e-06,  1.6142e-07,\n",
       "            -1.1125e-04, -5.5354e-05,  4.6787e-04,  2.4515e-04,  7.2141e-05,\n",
       "             1.4454e-04, -2.2713e-04, -1.1548e-04,  1.6349e-05,  1.0683e-05,\n",
       "             1.0697e-04,  1.4223e-04,  2.0027e-04,  1.4154e-04,  3.4174e-04,\n",
       "             7.8960e-05, -1.2833e-04,  2.9522e-04,  7.1704e-05,  2.4343e-04,\n",
       "            -5.7530e-05, -2.5776e-05, -6.2615e-05, -2.4825e-04, -6.2195e-05,\n",
       "             1.1361e-04, -5.4833e-04, -1.4410e-04, -3.1027e-04, -7.0631e-05,\n",
       "             1.4272e-05, -3.4680e-04, -3.8649e-05,  2.3809e-04,  1.7313e-04,\n",
       "             8.6174e-05,  4.8481e-05,  8.2719e-04, -6.5434e-05,  7.2919e-05,\n",
       "            -4.8551e-05, -5.9630e-05, -3.2723e-04, -3.7686e-05,  2.0460e-04,\n",
       "            -5.6326e-05,  5.5921e-05, -1.1903e-04, -8.2579e-05,  3.1460e-04,\n",
       "            -1.1500e-04, -8.1810e-05,  3.1760e-05,  3.1156e-04,  1.5599e-04,\n",
       "             1.1035e-04,  2.7094e-04,  1.1367e-04,  1.5592e-04,  1.5465e-04,\n",
       "             2.2869e-05, -5.5971e-05,  7.8647e-05, -4.7893e-05, -3.5250e-04,\n",
       "             1.8310e-04,  2.6550e-05,  6.8868e-05, -7.8301e-05,  1.6403e-04,\n",
       "             6.3373e-05, -1.9303e-05, -1.0734e-04, -1.1000e-05, -6.2038e-05,\n",
       "            -4.8306e-05,  1.8627e-05, -8.3892e-04, -3.8781e-05,  2.0851e-04,\n",
       "             1.9151e-05,  1.5719e-04, -1.2491e-04, -1.2690e-04,  1.2275e-04,\n",
       "            -2.4370e-04, -2.3010e-04,  6.6461e-05,  3.2822e-04,  5.4198e-05,\n",
       "            -3.0355e-05,  3.5635e-04, -1.2594e-04, -1.2394e-04, -6.8097e-04,\n",
       "             5.8144e-05,  1.1654e-04,  3.1230e-04, -3.5185e-03, -1.1941e-04,\n",
       "             1.8332e-05, -2.6312e-04, -1.0929e-04,  5.0573e-04, -5.0290e-04,\n",
       "            -2.0248e-06, -1.0371e-04, -1.2987e-04, -1.8295e-04, -1.6490e-03,\n",
       "            -8.3458e-04,  5.8024e-04,  1.3470e-04,  4.3759e-04,  1.5243e-04,\n",
       "            -1.5187e-03,  1.3311e-03, -4.7586e-04,  6.3016e-05, -1.6506e-04,\n",
       "             4.0625e-04,  2.4423e-04, -5.7361e-05,  1.8452e-04,  4.6452e-04,\n",
       "             1.2697e-04,  6.4593e-04,  3.9108e-04,  4.9286e-04, -7.9064e-04,\n",
       "            -2.1004e-03,  3.1653e-04, -2.0013e-05,  1.2133e-04, -2.9613e-04,\n",
       "            -3.1953e-05,  2.4981e-04, -1.2436e-04, -4.8740e-05,  3.8196e-04,\n",
       "            -5.9502e-04, -1.1209e-04, -9.3135e-05, -5.2534e-05, -2.5238e-04,\n",
       "            -2.7823e-04,  3.7357e-04,  6.8817e-05, -1.1799e-03, -3.9640e-04,\n",
       "             4.3915e-04, -5.1458e-04,  2.4350e-04,  1.4893e-04, -1.0801e-03,\n",
       "            -9.4778e-04,  4.8863e-04, -6.5007e-04, -6.7279e-04,  1.0579e-03,\n",
       "            -9.6043e-05, -2.8394e-04,  5.0859e-04,  1.4055e-04, -1.1928e-05,\n",
       "             7.7137e-05, -8.7781e-04,  8.5434e-05, -4.2228e-04, -2.3380e-05,\n",
       "            -2.0156e-04, -4.6919e-04, -2.9345e-04, -1.0852e-04,  5.9288e-04,\n",
       "            -2.3562e-04, -1.6993e-04,  2.0903e-04, -4.6044e-04, -6.4682e-04,\n",
       "             1.1037e-03, -2.2219e-05, -1.1233e-03, -1.2837e-03, -1.2148e-04,\n",
       "            -1.0112e-04,  2.1231e-05,  6.6064e-04,  1.8889e-04,  5.4081e-04,\n",
       "             3.5901e-04,  1.8575e-05, -6.5365e-04, -1.1683e-03, -2.0759e-04,\n",
       "             5.4771e-04, -1.8053e-04, -1.5870e-04,  1.3123e-04,  3.0723e-04,\n",
       "            -3.4299e-04, -7.5176e-04, -1.5557e-04, -1.9363e-05,  6.3122e-04,\n",
       "             1.8163e-03, -8.9187e-04, -4.3696e-04,  5.0866e-05, -6.8057e-04,\n",
       "             3.1706e-04,  3.1837e-04,  4.7238e-04,  5.8785e-04,  8.1798e-06,\n",
       "            -2.1233e-04, -3.7790e-05, -8.3841e-04,  2.7904e-05,  2.4995e-04,\n",
       "             7.0887e-04,  2.8036e-05,  1.5090e-05,  2.1125e-05, -2.0928e-04,\n",
       "            -3.4744e-04, -3.2456e-04, -1.5519e-04, -6.2978e-06,  6.3259e-05,\n",
       "             2.6065e-04,  7.0498e-05,  1.0487e-04,  3.3947e-04,  3.3545e-04,\n",
       "             3.7759e-04, -1.0157e-03, -4.9680e-04,  1.5776e-04,  2.8175e-05,\n",
       "             1.6498e-04,  2.2623e-04,  7.1002e-05,  9.2708e-04, -1.3219e-04,\n",
       "            -2.7690e-04,  3.2301e-04, -7.7130e-05,  1.2452e-04,  1.4360e-05,\n",
       "            -9.6870e-05,  1.1520e-04,  5.1303e-05,  1.2323e-04,  4.6762e-04,\n",
       "             3.0387e-05, -9.4496e-05, -3.3356e-04, -7.0550e-05,  2.0489e-04,\n",
       "            -2.3720e-04, -2.0911e-04,  3.1848e-04, -2.1572e-04, -2.4573e-04,\n",
       "             4.1006e-04, -6.2096e-05,  2.3107e-04,  3.3475e-04,  3.8964e-05,\n",
       "             4.9533e-05,  1.0972e-04, -9.3680e-05, -6.9689e-04, -5.0182e-05,\n",
       "            -7.4321e-05, -2.4023e-05, -4.5434e-05,  5.8875e-06, -8.2068e-05,\n",
       "             4.1199e-05,  1.3350e-04, -5.3217e-04, -7.3550e-04, -1.4703e-05,\n",
       "             3.3599e-05, -3.6833e-05, -3.8482e-04,  3.3488e-04,  2.7911e-04,\n",
       "             1.2858e-04,  3.1285e-04,  3.0168e-04, -2.9388e-04, -1.4855e-03,\n",
       "             4.5667e-05,  6.9336e-06,  1.2970e-04, -2.4894e-04, -2.1649e-05,\n",
       "             1.6348e-04, -1.1926e-04,  1.7973e-04, -1.3875e-04, -4.4067e-04,\n",
       "             5.3384e-05, -1.4279e-04, -1.1296e-04, -3.3165e-05,  1.7105e-04,\n",
       "            -5.9309e-05, -1.5138e-05, -2.3325e-04, -1.0832e-04,  2.9848e-04,\n",
       "             1.1841e-04, -6.8337e-05,  2.1103e-04,  4.2657e-04, -2.4572e-05,\n",
       "             9.7118e-05, -3.2198e-04, -5.7500e-05, -7.2488e-05, -1.4163e-04,\n",
       "            -1.4520e-05,  3.6023e-04, -4.8036e-05, -2.8340e-05,  7.6812e-05,\n",
       "             1.5140e-04,  3.0148e-04,  1.5810e-04, -1.8888e-04, -4.2884e-04,\n",
       "             3.6893e-04,  8.4553e-05,  1.6363e-04, -6.6608e-05,  1.2473e-05,\n",
       "             2.9934e-05, -1.6769e-04, -8.6157e-05, -2.9410e-04, -2.5119e-04,\n",
       "             3.5815e-04,  2.8338e-04]),\n",
       "    'exp_avg_sq': tensor([1.5485e-07, 1.7599e-07, 6.6727e-08, 1.4534e-07, 1.3003e-07, 1.8578e-07,\n",
       "            6.0297e-07, 1.6511e-06, 7.4349e-08, 7.1406e-08, 1.4422e-07, 1.1784e-07,\n",
       "            8.5199e-08, 1.7293e-07, 1.3355e-07, 6.4512e-08, 2.2719e-07, 7.0470e-09,\n",
       "            2.4901e-07, 1.0063e-07, 1.9903e-07, 8.8271e-08, 9.7525e-08, 1.7929e-07,\n",
       "            9.8868e-07, 1.5734e-07, 8.6572e-08, 1.1037e-07, 1.5038e-07, 1.6639e-07,\n",
       "            1.3801e-07, 1.2720e-07, 1.6606e-07, 1.5359e-07, 1.4763e-07, 1.6527e-07,\n",
       "            1.1527e-07, 2.0246e-07, 9.6798e-08, 9.7416e-08, 4.1557e-07, 2.7207e-07,\n",
       "            1.3853e-07, 1.5467e-07, 9.8498e-08, 6.9421e-08, 2.1923e-07, 8.6947e-08,\n",
       "            1.6712e-07, 7.8913e-08, 2.2483e-07, 1.1389e-07, 1.3272e-07, 1.6759e-07,\n",
       "            6.1090e-08, 1.7759e-07, 1.1824e-07, 4.7275e-08, 1.2510e-07, 8.9943e-08,\n",
       "            1.7008e-07, 2.8296e-07, 1.0998e-07, 1.9478e-07, 6.0140e-08, 9.4830e-08,\n",
       "            1.1725e-07, 8.6832e-08, 7.6689e-08, 1.3118e-07, 1.0516e-07, 1.7074e-07,\n",
       "            1.1190e-07, 1.1298e-07, 6.6700e-07, 2.0557e-08, 1.0461e-07, 1.3307e-07,\n",
       "            1.6759e-07, 2.1678e-07, 7.9872e-08, 1.0009e-07, 1.0731e-07, 5.7011e-08,\n",
       "            5.2396e-08, 2.2306e-07, 3.7654e-07, 9.3674e-08, 1.4896e-07, 8.5707e-08,\n",
       "            1.1786e-07, 1.7818e-07, 3.5090e-07, 7.3074e-07, 1.6826e-07, 1.2429e-07,\n",
       "            1.3127e-07, 4.9254e-08, 4.0090e-08, 1.8900e-07, 2.4714e-07, 1.0573e-07,\n",
       "            9.7981e-08, 1.7429e-07, 2.5397e-07, 3.1395e-07, 1.5892e-07, 2.6886e-07,\n",
       "            7.9264e-08, 1.4727e-07, 4.6634e-08, 1.9090e-07, 7.9499e-08, 1.9230e-07,\n",
       "            7.9380e-08, 1.2959e-07, 1.2756e-07, 1.1901e-07, 5.9234e-08, 1.2823e-07,\n",
       "            3.4182e-07, 1.4945e-07, 1.2928e-07, 2.1168e-07, 2.3013e-07, 1.4403e-07,\n",
       "            1.1566e-07, 1.2441e-07, 6.5640e-08, 1.6139e-07, 9.3397e-08, 1.1125e-07,\n",
       "            1.0013e-07, 1.0057e-07, 7.4836e-07, 1.8591e-06, 6.4632e-08, 1.1605e-07,\n",
       "            1.2235e-07, 5.9610e-08, 7.9268e-08, 5.5339e-07, 1.9024e-07, 6.6675e-08,\n",
       "            2.0779e-07, 2.4522e-08, 7.4587e-08, 8.5620e-08, 1.2489e-07, 8.2307e-08,\n",
       "            1.5920e-07, 9.6337e-08, 7.4588e-08, 1.1972e-07, 5.0541e-08, 1.3759e-07,\n",
       "            3.9045e-07, 7.0350e-08, 4.9187e-08, 1.8313e-07, 1.2887e-07, 4.3047e-08,\n",
       "            6.0768e-08, 1.1326e-07, 7.9548e-08, 1.4542e-07, 1.0189e-07, 1.1248e-07,\n",
       "            2.6992e-07, 1.0628e-07, 1.7818e-07, 1.1992e-07, 1.1227e-07, 7.1499e-08,\n",
       "            5.8814e-08, 1.1761e-07, 1.2181e-07, 1.0361e-07, 7.1911e-08, 3.4759e-07,\n",
       "            4.0268e-08, 7.2781e-08, 7.2746e-08, 7.9683e-08, 1.3872e-07, 1.1198e-07,\n",
       "            9.4134e-08, 5.2537e-08, 1.9153e-07, 2.1448e-07, 7.3178e-08, 4.3197e-07,\n",
       "            2.4462e-08, 1.4619e-07, 7.2433e-08, 1.1160e-07, 1.4707e-07, 1.3409e-07,\n",
       "            1.0755e-07, 6.2962e-08, 4.9453e-08, 3.4335e-07, 9.4629e-07, 1.8175e-08,\n",
       "            9.5382e-08, 1.4206e-07, 1.7837e-07, 3.8020e-07, 1.0999e-07, 1.3961e-07,\n",
       "            2.7904e-08, 6.3190e-08, 1.2558e-07, 2.0417e-07, 1.5352e-07, 5.8302e-08,\n",
       "            1.6737e-07, 3.8689e-08, 1.2952e-07, 1.6931e-07, 1.7729e-07, 5.2504e-07,\n",
       "            9.0189e-08, 1.6071e-07, 7.2499e-08, 8.8759e-08, 6.0923e-08, 9.6056e-08,\n",
       "            1.1435e-07, 1.1014e-07, 9.5635e-08, 1.7438e-07, 1.0183e-07, 1.7160e-07,\n",
       "            9.3611e-08, 1.5938e-07, 6.3240e-08, 2.5303e-07, 9.0844e-08, 1.5399e-07,\n",
       "            1.1626e-07, 1.0994e-07, 1.7964e-07, 1.3879e-07, 1.3283e-07, 1.3599e-07,\n",
       "            1.2692e-07, 6.3836e-08, 1.5093e-07, 1.5436e-07, 2.4271e-07, 2.2488e-07,\n",
       "            1.1014e-07, 2.5227e-07, 4.9776e-08, 2.5802e-07, 2.0136e-07, 1.3188e-07,\n",
       "            1.7045e-06, 2.0715e-06, 9.6330e-07, 2.1568e-07, 7.2444e-06, 3.3436e-05,\n",
       "            9.8913e-08, 1.7107e-06, 1.3364e-06, 1.3512e-06, 1.2986e-06, 3.8082e-06,\n",
       "            8.7531e-07, 3.9445e-07, 2.3494e-07, 1.1401e-07, 1.6015e-06, 1.6669e-06,\n",
       "            1.1330e-06, 1.8773e-06, 4.8443e-07, 4.6057e-07, 1.9512e-06, 2.6248e-06,\n",
       "            1.7505e-06, 1.1563e-06, 3.3510e-07, 7.6142e-07, 1.2344e-07, 3.5749e-08,\n",
       "            9.8111e-07, 3.7507e-07, 1.5985e-06, 4.0157e-07, 1.4408e-06, 2.6782e-07,\n",
       "            3.0580e-07, 1.8035e-06, 2.9907e-06, 7.1056e-08, 2.1505e-07, 8.7371e-07,\n",
       "            4.5492e-07, 1.1125e-06, 1.0436e-07, 1.0033e-06, 2.8951e-07, 6.7577e-07,\n",
       "            1.0255e-06, 2.1095e-06, 2.6522e-08, 6.7536e-07, 1.8730e-07, 1.0583e-06,\n",
       "            7.0392e-08, 9.2761e-07, 2.7428e-06, 6.4445e-07, 1.1646e-06, 1.0245e-07,\n",
       "            1.2591e-06, 2.8420e-06, 1.6664e-06, 2.5782e-06, 5.8708e-07, 2.1904e-06,\n",
       "            7.7068e-07, 3.8740e-06, 2.0783e-07, 4.2810e-07, 2.0215e-06, 3.1861e-08,\n",
       "            7.0907e-06, 2.0784e-06, 1.3167e-06, 2.1089e-06, 1.3238e-07, 8.5102e-07,\n",
       "            1.1247e-06, 1.3988e-06, 6.8075e-07, 1.7438e-06, 1.0543e-06, 1.6307e-07,\n",
       "            6.1869e-08, 1.1085e-06, 2.9065e-06, 1.5646e-06, 8.0334e-07, 1.2449e-06,\n",
       "            4.2324e-06, 8.1610e-06, 1.6521e-06, 6.2169e-07, 1.7582e-06, 1.1746e-07,\n",
       "            1.9919e-06, 1.2684e-06, 3.4289e-06, 7.1380e-07, 4.4350e-06, 4.9612e-07,\n",
       "            5.7400e-07, 1.5913e-07, 9.9535e-07, 1.7375e-07, 9.6290e-07, 9.5353e-07,\n",
       "            1.4021e-06, 1.2853e-07, 8.3628e-07, 6.4713e-07, 9.0961e-07, 2.6636e-06,\n",
       "            8.7560e-07, 2.3755e-06, 2.6347e-06, 5.7920e-07, 3.7733e-07, 2.0538e-06,\n",
       "            1.0132e-06, 2.1091e-07, 5.5230e-07, 6.3335e-07, 1.2674e-06, 2.2498e-08,\n",
       "            1.9007e-07, 2.1240e-07, 7.3928e-08, 7.5226e-08, 1.9833e-07, 1.4407e-07,\n",
       "            2.1503e-07, 4.4501e-07, 1.7714e-07, 1.0136e-07, 3.7872e-08, 6.3752e-08,\n",
       "            1.0060e-07, 1.4625e-07, 2.3308e-07, 1.4078e-07, 2.0829e-07, 5.1695e-07,\n",
       "            1.3702e-07, 8.4921e-08, 8.1862e-08, 8.6318e-08, 7.5393e-08, 7.5555e-08,\n",
       "            6.6902e-07, 1.0763e-07, 2.9402e-07, 2.0986e-07, 8.0484e-08, 8.1342e-08,\n",
       "            1.1300e-07, 1.3200e-07, 1.1475e-07, 1.7213e-07, 4.9839e-08, 1.9908e-07,\n",
       "            1.3640e-07, 7.1176e-08, 9.8807e-08, 1.3135e-07, 5.8300e-07, 1.3438e-07,\n",
       "            1.3431e-07, 3.6359e-07, 1.9265e-07, 1.5323e-07, 1.3183e-07, 1.9271e-07,\n",
       "            9.8261e-08, 1.3412e-07, 9.3907e-08, 1.4431e-07, 2.6551e-07, 1.5358e-07,\n",
       "            1.8528e-07, 7.3042e-08, 2.0478e-07, 2.7233e-07, 9.4876e-08, 1.3841e-07,\n",
       "            2.3252e-07, 1.1053e-07, 7.9338e-08, 3.0835e-07, 1.1238e-06, 1.7669e-07,\n",
       "            9.5751e-08, 1.2901e-07, 1.9655e-07, 1.6431e-07, 2.2223e-07, 2.8637e-07,\n",
       "            7.8207e-08, 2.2557e-07, 2.3314e-07, 6.7899e-07, 1.8754e-07, 1.1295e-07,\n",
       "            1.6221e-07, 2.2141e-07, 1.1213e-07, 1.2460e-07, 1.4292e-07, 4.3538e-07,\n",
       "            2.7948e-07, 1.3226e-07, 1.9195e-07, 9.5356e-08, 9.5973e-08, 8.8734e-08,\n",
       "            1.1503e-07, 7.6212e-08, 1.6354e-07, 3.4122e-07, 4.1419e-08, 2.7709e-07,\n",
       "            5.9589e-08, 3.6545e-07, 2.3687e-07, 1.8780e-07, 2.0655e-07, 2.0032e-07,\n",
       "            1.8105e-07, 1.6496e-07, 1.8028e-07, 7.4736e-08, 8.1395e-08, 1.2073e-07,\n",
       "            9.2119e-08, 1.9617e-07, 1.9416e-07, 2.1103e-07, 1.8757e-07, 2.2033e-07,\n",
       "            2.0920e-07, 1.2834e-07, 2.5613e-07, 1.1309e-07, 1.5036e-07, 1.2303e-07,\n",
       "            6.2266e-08, 7.3379e-08, 1.4211e-07, 1.1143e-07, 1.3649e-07, 1.4924e-07,\n",
       "            9.8087e-08, 2.1021e-07])},\n",
       "   9: {'step': 23400,\n",
       "    'exp_avg': tensor([[-5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
       "             -5.6052e-45, -5.6052e-45],\n",
       "            [ 4.3528e-04,  5.2870e-04,  1.4266e-03,  ...,  4.6601e-04,\n",
       "             -6.5056e-04,  3.1235e-04],\n",
       "            [-5.8050e-04,  3.5280e-04, -1.6295e-04,  ...,  2.1055e-04,\n",
       "             -7.4421e-04,  6.6266e-04],\n",
       "            ...,\n",
       "            [-4.2413e-15,  1.7152e-12,  1.8371e-07,  ...,  2.6811e-07,\n",
       "             -5.2247e-09, -9.9842e-10],\n",
       "            [ 1.5111e-08,  1.8157e-13,  2.8537e-08,  ..., -2.0651e-14,\n",
       "              7.0103e-09,  5.6052e-45],\n",
       "            [-1.4316e-06, -1.7593e-06,  1.7902e-06,  ..., -5.7240e-07,\n",
       "             -1.4205e-07, -9.4262e-07]]),\n",
       "    'exp_avg_sq': tensor([[2.1500e-26, 7.7032e-26, 4.7811e-27,  ..., 2.0136e-26, 1.2594e-27,\n",
       "             3.1955e-26],\n",
       "            [5.8846e-07, 3.7940e-07, 4.2121e-07,  ..., 3.3060e-07, 4.0856e-07,\n",
       "             3.8134e-07],\n",
       "            [1.0263e-06, 1.5369e-06, 6.5358e-07,  ..., 5.6745e-07, 8.2790e-07,\n",
       "             1.3279e-06],\n",
       "            ...,\n",
       "            [2.4426e-10, 5.1300e-10, 1.7740e-10,  ..., 6.6198e-10, 2.1583e-10,\n",
       "             2.5372e-10],\n",
       "            [1.3978e-10, 2.8611e-12, 1.3878e-11,  ..., 1.0595e-11, 1.5263e-11,\n",
       "             1.0142e-11],\n",
       "            [8.9034e-09, 7.2641e-09, 5.2606e-09,  ..., 6.3755e-09, 7.4833e-09,\n",
       "             6.2051e-09]])},\n",
       "   10: {'step': 23400,\n",
       "    'exp_avg': tensor([[-2.3987e-05,  2.4200e-05,  1.1666e-05,  ...,  2.0293e-05,\n",
       "             -5.8919e-05, -4.0354e-05],\n",
       "            [ 1.6629e-05, -3.0395e-07, -7.1384e-05,  ..., -1.7490e-05,\n",
       "              2.0695e-05, -9.6563e-06],\n",
       "            [-4.1890e-06,  1.0880e-05,  4.0548e-06,  ...,  4.3496e-05,\n",
       "              5.9695e-05,  1.5095e-05],\n",
       "            ...,\n",
       "            [ 2.2243e-05, -1.2925e-05, -1.4857e-05,  ...,  8.6986e-05,\n",
       "             -1.0508e-05, -6.2362e-05],\n",
       "            [ 2.2291e-05, -1.5688e-05, -3.6762e-05,  ..., -9.0303e-06,\n",
       "             -2.6236e-05,  1.2404e-05],\n",
       "            [ 3.6508e-06, -1.4069e-05, -5.9831e-06,  ..., -6.9277e-05,\n",
       "              3.4904e-05,  9.4105e-05]]),\n",
       "    'exp_avg_sq': tensor([[3.6129e-09, 3.0874e-09, 3.1499e-09,  ..., 1.4720e-08, 7.7437e-09,\n",
       "             1.1765e-08],\n",
       "            [3.5473e-09, 1.7826e-09, 3.4999e-09,  ..., 1.8646e-08, 7.5704e-09,\n",
       "             1.0583e-08],\n",
       "            [1.8237e-09, 2.9664e-09, 3.6292e-09,  ..., 2.3401e-08, 1.2360e-08,\n",
       "             1.2068e-08],\n",
       "            ...,\n",
       "            [3.7595e-09, 5.1546e-09, 6.2158e-09,  ..., 1.7258e-08, 6.2657e-09,\n",
       "             1.2257e-08],\n",
       "            [1.1900e-08, 1.0068e-08, 1.2319e-08,  ..., 3.0445e-08, 1.4950e-08,\n",
       "             3.0471e-08],\n",
       "            [8.9869e-09, 6.7879e-09, 1.1687e-08,  ..., 4.1032e-08, 2.1236e-08,\n",
       "             3.4212e-08]])},\n",
       "   11: {'step': 23400,\n",
       "    'exp_avg': tensor([[ 3.7012e-05, -2.2401e-05, -2.4541e-05,  ..., -4.5300e-05,\n",
       "             -8.7119e-05,  7.1893e-06],\n",
       "            [-4.0890e-06,  4.2920e-05,  5.9749e-05,  ...,  2.4085e-05,\n",
       "              2.2537e-05, -8.2191e-05],\n",
       "            [ 2.3546e-05, -3.2661e-07,  1.8158e-05,  ..., -1.7502e-05,\n",
       "             -7.2209e-06,  6.8896e-06],\n",
       "            ...,\n",
       "            [ 3.4542e-05,  1.5108e-05,  8.2909e-06,  ..., -1.7295e-05,\n",
       "              2.7741e-05, -2.1524e-05],\n",
       "            [-4.6239e-07, -3.6302e-05, -3.2957e-05,  ..., -7.7067e-05,\n",
       "             -6.0951e-05,  3.8390e-05],\n",
       "            [-6.3962e-06, -4.6892e-05, -8.8828e-05,  ...,  4.0291e-05,\n",
       "              4.7036e-05, -3.0839e-05]]),\n",
       "    'exp_avg_sq': tensor([[2.6492e-09, 2.5370e-09, 4.1135e-09,  ..., 4.1091e-09, 4.0533e-09,\n",
       "             3.0065e-09],\n",
       "            [7.6733e-10, 2.1119e-09, 1.0253e-08,  ..., 5.7683e-09, 3.8936e-09,\n",
       "             4.7411e-09],\n",
       "            [9.7080e-10, 1.3370e-09, 1.3306e-08,  ..., 5.3817e-09, 5.9326e-09,\n",
       "             5.3656e-09],\n",
       "            ...,\n",
       "            [8.6014e-09, 8.5007e-09, 9.3439e-09,  ..., 3.4314e-08, 1.9248e-08,\n",
       "             1.0204e-08],\n",
       "            [1.3501e-08, 2.1881e-08, 1.4632e-08,  ..., 2.1609e-08, 3.8776e-08,\n",
       "             1.6255e-08],\n",
       "            [1.2672e-08, 1.6743e-08, 1.7688e-08,  ..., 1.2985e-08, 2.2121e-08,\n",
       "             2.4044e-08]])},\n",
       "   12: {'step': 23400,\n",
       "    'exp_avg': tensor([ 1.2446e-04, -1.7286e-04,  3.6197e-05,  ...,  3.2904e-05,\n",
       "             1.7053e-04,  8.2994e-05]),\n",
       "    'exp_avg_sq': tensor([7.7171e-08, 7.6464e-08, 4.5568e-08,  ..., 3.1651e-07, 3.2111e-07,\n",
       "            2.7440e-07])},\n",
       "   13: {'step': 23400,\n",
       "    'exp_avg': tensor([ 1.2446e-04, -1.7286e-04,  3.6197e-05,  ...,  3.2904e-05,\n",
       "             1.7053e-04,  8.2994e-05]),\n",
       "    'exp_avg_sq': tensor([7.7171e-08, 7.6464e-08, 4.5568e-08,  ..., 3.1651e-07, 3.2111e-07,\n",
       "            2.7440e-07])},\n",
       "   14: {'step': 23400,\n",
       "    'exp_avg': tensor([[ 1.0203e-04, -3.1469e-05, -5.5222e-06,  ..., -2.2859e-04,\n",
       "              2.2356e-04, -2.4492e-04],\n",
       "            [-1.4313e-04,  1.6475e-05,  5.6327e-05,  ..., -3.9800e-04,\n",
       "              9.9701e-05, -7.0854e-05],\n",
       "            [-1.3601e-04, -3.8672e-05, -3.2010e-05,  ..., -2.9000e-04,\n",
       "             -9.7196e-05,  7.4802e-05],\n",
       "            ...,\n",
       "            [ 3.6010e-04, -6.5782e-05, -9.1884e-06,  ...,  4.0553e-04,\n",
       "              2.3284e-04, -2.8899e-04],\n",
       "            [ 3.3208e-04, -7.7911e-05, -1.2898e-04,  ..., -1.4640e-04,\n",
       "              3.5549e-04, -4.5909e-05],\n",
       "            [-1.0777e-04, -1.5375e-06, -5.9407e-05,  ..., -1.3633e-04,\n",
       "              9.7392e-05,  3.8770e-05]]),\n",
       "    'exp_avg_sq': tensor([[4.4385e-08, 2.2052e-08, 1.1069e-08,  ..., 2.5770e-07, 1.7635e-07,\n",
       "             2.4788e-07],\n",
       "            [1.1323e-07, 3.6409e-08, 2.8289e-08,  ..., 8.0138e-07, 4.7475e-07,\n",
       "             8.9410e-07],\n",
       "            [5.5183e-08, 1.8089e-08, 2.4854e-08,  ..., 3.8479e-07, 2.7279e-07,\n",
       "             4.6106e-07],\n",
       "            ...,\n",
       "            [8.4039e-08, 8.6991e-08, 3.2440e-08,  ..., 4.6263e-07, 2.6923e-07,\n",
       "             6.8253e-07],\n",
       "            [1.0778e-07, 9.9844e-08, 3.4484e-08,  ..., 7.2918e-07, 4.9360e-07,\n",
       "             7.7937e-07],\n",
       "            [6.6568e-08, 3.9167e-08, 2.4878e-08,  ..., 5.1936e-07, 3.3141e-07,\n",
       "             6.9712e-07]])},\n",
       "   15: {'step': 23400,\n",
       "    'exp_avg': tensor([-1.4446e-10,  5.6116e-11,  2.2825e-10, -2.7760e-10,  1.5131e-11,\n",
       "             1.4700e-10, -3.4816e-11, -8.4432e-11,  8.4602e-12, -6.4749e-11,\n",
       "             2.9144e-11, -7.5656e-11,  8.6719e-13, -4.4517e-11, -2.6068e-10,\n",
       "             1.1024e-10,  3.8051e-10,  7.5628e-11, -2.1100e-10,  1.6519e-11,\n",
       "             3.2068e-10, -1.1307e-10, -4.1026e-11,  7.3182e-11,  8.9698e-11,\n",
       "            -1.3031e-10,  5.4755e-11, -5.1209e-12, -2.3705e-10, -1.2765e-11,\n",
       "            -1.0700e-10,  1.6990e-10,  1.2154e-10,  2.5475e-10,  1.3449e-10,\n",
       "            -5.9628e-10, -1.8984e-10,  3.7085e-11,  1.9896e-11,  8.8375e-11,\n",
       "             7.8560e-11,  1.9694e-10, -4.5941e-10,  1.3536e-10,  4.0724e-11,\n",
       "             7.8802e-11,  9.5306e-11, -3.0583e-11, -3.7088e-10, -7.3804e-11,\n",
       "            -2.9781e-10, -1.6268e-10, -2.1824e-11,  6.0616e-11, -4.6215e-11,\n",
       "            -1.0068e-10, -1.3337e-11,  3.0851e-10,  1.1532e-10, -6.3097e-11,\n",
       "             1.0449e-10, -3.7232e-11, -1.5504e-10, -1.5226e-11, -1.8347e-10,\n",
       "             1.5035e-11, -1.3826e-11,  1.1031e-10,  1.9883e-10, -9.9429e-11,\n",
       "             3.3218e-12,  2.6058e-10,  9.0542e-12,  1.2010e-10, -6.8371e-11,\n",
       "            -1.2330e-10,  9.7605e-11,  3.3939e-10,  1.1486e-10,  1.7564e-10,\n",
       "            -2.4458e-10,  4.6853e-11, -8.9735e-11, -1.5980e-10,  6.4968e-11,\n",
       "             2.3106e-11, -5.3851e-12,  7.1808e-12,  2.6535e-10,  6.4441e-11,\n",
       "            -9.8643e-11,  1.0117e-10,  3.3223e-11,  4.0662e-11, -2.1094e-10,\n",
       "             3.5588e-11, -1.3184e-10,  5.9244e-11,  2.0056e-10,  1.7226e-11,\n",
       "             1.1195e-10, -6.2225e-11, -4.2963e-10,  4.0554e-10, -6.3941e-12,\n",
       "            -4.6914e-11,  1.0992e-10, -6.3683e-11,  8.8107e-11,  2.6881e-11,\n",
       "            -3.8243e-11,  2.9742e-11,  7.1052e-11,  2.8694e-10,  1.8775e-10,\n",
       "            -4.2031e-11,  8.6573e-11,  1.3642e-10, -1.1906e-10, -3.1065e-11,\n",
       "             2.0931e-11, -2.9368e-11, -2.0806e-10,  1.9481e-10,  5.4461e-12,\n",
       "             1.7404e-10,  1.9337e-10,  4.7427e-10,  2.7506e-11, -3.3889e-10,\n",
       "            -1.0846e-10,  8.1004e-10,  3.4850e-12,  1.4591e-10,  5.3151e-12,\n",
       "            -1.8278e-11, -1.8574e-10,  2.9366e-12,  2.5029e-10,  6.5821e-12,\n",
       "             3.8904e-10,  1.9873e-10, -1.1379e-11,  1.2309e-10, -3.1405e-10,\n",
       "            -2.6947e-10, -7.6812e-11,  5.1916e-11, -3.0483e-10, -2.5512e-11,\n",
       "             1.6545e-10,  1.4382e-10, -2.8759e-10, -3.5811e-10,  5.5236e-11,\n",
       "            -1.4088e-10,  1.2698e-10,  1.2690e-10,  5.0247e-11,  2.4257e-11,\n",
       "            -1.1155e-10, -4.6806e-11,  7.7577e-11, -3.1056e-10,  1.8363e-10,\n",
       "             8.5287e-11, -4.1205e-12, -4.1673e-11, -1.8048e-10, -2.9216e-10,\n",
       "             3.1131e-11,  2.0229e-10, -8.8962e-11,  2.9102e-10,  2.2451e-10,\n",
       "            -3.4889e-11,  3.4170e-10, -4.3506e-11,  4.7840e-11,  2.4815e-11,\n",
       "             1.3980e-10, -3.0819e-12,  1.5898e-10,  1.4321e-10, -1.3125e-12,\n",
       "             1.3522e-10, -2.7429e-11,  2.1189e-10, -9.2418e-11,  1.2288e-10,\n",
       "            -5.3310e-11, -3.2832e-10, -5.9872e-11,  1.7290e-10,  2.2966e-10,\n",
       "             3.2097e-10, -4.3791e-11, -4.2047e-10, -1.5246e-10, -3.4811e-11,\n",
       "             2.9053e-11, -8.1924e-11, -2.0132e-11, -3.1375e-10, -2.8926e-11,\n",
       "             1.2812e-10,  1.1953e-10, -5.3004e-11, -1.1232e-10,  1.6546e-10,\n",
       "            -1.3055e-10, -1.2160e-10,  3.0143e-10,  5.2932e-11, -2.1384e-10,\n",
       "             2.4423e-10,  2.8488e-10, -2.9571e-10, -1.4459e-10,  2.2858e-10,\n",
       "            -4.1999e-10,  7.4001e-11,  2.1995e-10, -1.3716e-10,  3.4334e-11,\n",
       "             2.9698e-11, -5.5699e-11,  3.1830e-10, -3.7972e-11,  1.8149e-10,\n",
       "            -1.1262e-10,  2.8383e-10,  2.7591e-10,  2.2503e-10,  2.9661e-11,\n",
       "            -3.4126e-10, -2.6636e-10, -1.3742e-10,  6.5167e-12, -1.9392e-10,\n",
       "             1.8245e-10,  4.2119e-11, -9.7658e-11,  1.0445e-11, -4.2161e-11,\n",
       "             9.2735e-11,  4.5763e-10,  5.8843e-11,  3.8878e-10,  7.9570e-11,\n",
       "            -1.1409e-10,  1.3504e-10,  1.9978e-10, -3.1386e-10, -1.5699e-10,\n",
       "             6.8856e-11]),\n",
       "    'exp_avg_sq': tensor([2.0810e-19, 3.7792e-19, 3.2541e-19, 2.8897e-19, 2.0746e-19, 3.4857e-19,\n",
       "            2.1119e-19, 3.6518e-19, 5.3464e-19, 3.3000e-19, 1.1991e-19, 3.3113e-19,\n",
       "            5.0243e-19, 1.5814e-19, 2.4336e-19, 4.8358e-19, 5.5443e-19, 2.9984e-19,\n",
       "            3.5525e-19, 3.5110e-19, 1.1548e-18, 3.7622e-19, 2.9959e-19, 2.2329e-19,\n",
       "            3.8499e-19, 3.0995e-19, 3.8199e-19, 5.5886e-19, 2.0972e-19, 1.7022e-19,\n",
       "            2.1522e-19, 2.9377e-19, 3.7743e-19, 3.4079e-19, 2.3157e-19, 1.6464e-18,\n",
       "            4.6907e-19, 1.8052e-19, 2.7210e-19, 3.0694e-19, 3.3867e-19, 1.2036e-19,\n",
       "            7.3494e-19, 2.9125e-19, 2.3294e-19, 1.6584e-19, 1.5404e-19, 1.9334e-19,\n",
       "            4.7946e-19, 1.4318e-19, 1.2438e-19, 4.0036e-19, 1.3124e-19, 1.4823e-19,\n",
       "            3.0009e-19, 2.3129e-19, 2.9745e-19, 2.7175e-19, 4.1914e-19, 1.3400e-19,\n",
       "            1.6351e-19, 1.4494e-19, 3.4351e-19, 1.6751e-19, 2.0323e-19, 4.7904e-19,\n",
       "            1.1795e-19, 2.4226e-19, 2.4846e-19, 7.2959e-19, 1.4336e-19, 5.0161e-19,\n",
       "            5.4498e-19, 4.0808e-19, 4.7962e-19, 4.9650e-19, 2.2327e-19, 1.5231e-19,\n",
       "            2.8026e-19, 3.9110e-19, 3.4318e-19, 4.0825e-19, 1.9120e-19, 3.5934e-19,\n",
       "            3.1206e-19, 2.2211e-19, 1.0967e-19, 2.0514e-19, 4.5540e-19, 1.3926e-19,\n",
       "            1.0088e-19, 1.4365e-18, 2.7812e-19, 2.5570e-19, 3.5998e-19, 1.4110e-19,\n",
       "            3.0165e-19, 2.2495e-19, 5.1897e-19, 3.3872e-19, 1.7831e-19, 2.0103e-19,\n",
       "            2.7974e-19, 4.5772e-19, 1.5232e-19, 1.0467e-19, 1.9340e-19, 3.8437e-19,\n",
       "            4.9239e-19, 9.1051e-20, 1.6308e-19, 1.9389e-19, 4.0476e-19, 4.4283e-19,\n",
       "            2.0111e-19, 3.4919e-19, 2.1145e-19, 5.0947e-19, 1.8329e-19, 3.7016e-19,\n",
       "            3.9606e-19, 1.9278e-19, 2.2026e-19, 5.0788e-19, 6.0294e-19, 5.9256e-19,\n",
       "            3.2100e-19, 9.6682e-19, 1.4685e-19, 1.1684e-18, 6.3519e-19, 5.9281e-19,\n",
       "            3.2293e-19, 2.8162e-19, 1.7490e-19, 1.9648e-19, 2.1961e-19, 2.6612e-19,\n",
       "            1.9161e-19, 6.9358e-19, 4.1528e-19, 1.9266e-19, 2.0373e-19, 4.1740e-19,\n",
       "            4.4333e-19, 3.2286e-19, 4.7380e-19, 2.9088e-19, 4.2692e-19, 3.6254e-19,\n",
       "            3.2619e-19, 4.2503e-19, 1.8881e-19, 3.9033e-19, 2.2828e-19, 1.7269e-19,\n",
       "            3.4412e-19, 1.4775e-19, 4.3855e-19, 2.0851e-19, 3.1674e-19, 2.1411e-19,\n",
       "            1.8638e-19, 5.5920e-19, 3.7732e-19, 2.2788e-19, 2.3479e-19, 1.5786e-19,\n",
       "            2.6218e-19, 2.8752e-19, 3.1835e-19, 7.9814e-19, 2.4806e-19, 1.9276e-19,\n",
       "            6.1904e-19, 3.4048e-19, 3.0637e-19, 4.2986e-19, 8.6427e-20, 1.4355e-19,\n",
       "            2.2048e-19, 4.9771e-19, 3.3984e-19, 3.8747e-19, 3.3722e-19, 3.3202e-19,\n",
       "            1.3526e-19, 3.3552e-19, 2.0248e-19, 1.8198e-19, 2.6057e-19, 3.6726e-19,\n",
       "            2.8014e-19, 3.1350e-19, 3.3740e-19, 2.0269e-19, 1.7950e-19, 2.8349e-19,\n",
       "            4.3521e-19, 1.2352e-19, 1.1336e-19, 1.3703e-19, 3.3697e-19, 5.7613e-19,\n",
       "            3.0802e-19, 4.0985e-19, 2.0982e-19, 1.0514e-19, 1.4043e-19, 6.7578e-19,\n",
       "            2.9530e-19, 3.7528e-19, 7.3792e-19, 2.3904e-19, 2.7206e-19, 2.3354e-19,\n",
       "            2.5316e-19, 2.0835e-19, 6.7265e-19, 1.1209e-19, 4.7272e-19, 2.2787e-19,\n",
       "            6.1728e-19, 3.1348e-19, 4.2149e-19, 2.2226e-19, 4.4111e-19, 4.4343e-19,\n",
       "            2.2113e-19, 4.6311e-19, 4.3009e-19, 4.6821e-19, 5.9716e-19, 1.9097e-19,\n",
       "            2.8261e-19, 3.2032e-19, 3.6440e-19, 3.4982e-19, 4.3950e-19, 3.9880e-19,\n",
       "            2.3742e-19, 1.9341e-19, 1.6343e-19, 2.4020e-19, 1.4464e-19, 1.5106e-19,\n",
       "            4.8716e-19, 3.8023e-19, 5.5965e-19, 4.5887e-19, 9.5308e-20, 3.9199e-19,\n",
       "            6.6296e-19, 2.0508e-19, 4.0783e-19, 2.5553e-19])},\n",
       "   16: {'step': 23400,\n",
       "    'exp_avg': tensor([[ 2.0376e-10, -1.5051e-10,  2.3230e-11,  ..., -1.0628e-09,\n",
       "             -6.9272e-10,  3.9650e-10],\n",
       "            [ 2.0109e-10, -1.4904e-10,  2.3168e-11,  ..., -1.0361e-09,\n",
       "             -6.8001e-10,  3.9122e-10],\n",
       "            [ 9.0671e-04, -7.3159e-04,  9.2108e-04,  ..., -8.3945e-04,\n",
       "             -1.5467e-03,  6.6480e-04],\n",
       "            ...,\n",
       "            [ 1.3890e-07, -5.2207e-07, -1.0342e-08,  ..., -1.0003e-06,\n",
       "             -5.8079e-07,  2.1332e-07],\n",
       "            [ 6.0457e-08, -1.9262e-08,  1.2878e-09,  ..., -1.2533e-07,\n",
       "             -1.3604e-07,  3.2791e-08],\n",
       "            [ 2.9060e-09, -4.0064e-09,  2.2913e-12,  ..., -1.1851e-08,\n",
       "             -8.9953e-09,  1.1918e-08]]),\n",
       "    'exp_avg_sq': tensor([[1.7024e-20, 2.0551e-20, 1.5709e-20,  ..., 5.5543e-19, 4.5348e-19,\n",
       "             4.7620e-19],\n",
       "            [1.5743e-20, 1.9108e-20, 1.4421e-20,  ..., 4.9122e-19, 3.9796e-19,\n",
       "             4.1864e-19],\n",
       "            [2.9210e-07, 1.5415e-07, 3.4674e-07,  ..., 5.4120e-06, 4.7040e-06,\n",
       "             1.2172e-06],\n",
       "            ...,\n",
       "            [4.0402e-12, 1.1819e-11, 4.0301e-13,  ..., 1.1177e-10, 3.9964e-11,\n",
       "             4.9715e-12],\n",
       "            [6.2250e-12, 1.1035e-13, 1.2169e-13,  ..., 5.8139e-12, 2.8666e-11,\n",
       "             7.7386e-14],\n",
       "            [9.0036e-17, 6.4137e-16, 3.6208e-14,  ..., 1.5973e-13, 4.9992e-14,\n",
       "             4.2705e-13]])},\n",
       "   17: {'step': 23400,\n",
       "    'exp_avg': tensor([ 5.4879e-09,  5.3979e-09,  6.7798e-03, -1.0056e-03, -6.1210e-04,\n",
       "             3.2062e-04, -8.1780e-04, -5.0913e-04,  9.1986e-04, -7.2641e-04,\n",
       "             1.1569e-04,  1.5254e-04,  1.0592e-02,  1.2177e-04,  1.0526e-03,\n",
       "            -3.8985e-04, -1.6327e-03, -7.7647e-04, -3.3540e-04,  6.6756e-04,\n",
       "            -5.9339e-04, -1.1102e-03, -2.9071e-04,  7.7002e-04, -5.5456e-04,\n",
       "            -4.7373e-03,  5.8631e-04, -2.8496e-04, -1.0271e-03,  3.0326e-04,\n",
       "             1.0555e-03,  5.1372e-04, -9.9331e-06, -5.0587e-04,  3.7289e-04,\n",
       "            -5.2515e-04,  5.1014e-04, -1.8512e-03, -1.0679e-03,  8.1858e-05,\n",
       "             3.5737e-04, -4.9503e-06,  2.9042e-04,  5.0127e-04, -1.6551e-04,\n",
       "             9.0723e-05,  2.9417e-04,  2.3545e-04,  6.0579e-05,  1.3780e-04,\n",
       "             5.1949e-04, -1.0629e-04,  1.4702e-04,  7.8928e-05,  4.6228e-04,\n",
       "             8.1388e-05, -6.8386e-04, -1.0307e-02,  1.3895e-04,  3.9851e-04,\n",
       "             1.5053e-04,  3.5871e-04,  1.8849e-04,  3.4597e-04,  4.0258e-05,\n",
       "             2.3687e-04,  8.4393e-05,  9.7891e-05,  1.8755e-04,  6.1449e-06,\n",
       "             1.6645e-07,  1.2639e-06,  4.2347e-05,  1.2500e-05,  6.8785e-05,\n",
       "            -6.2021e-07,  2.1496e-05,  2.3806e-06, -2.0505e-05,  1.2434e-06,\n",
       "             1.1628e-07,  4.7826e-06, -9.4886e-07,  7.0556e-06,  1.7198e-05,\n",
       "             5.9180e-06,  4.0022e-05,  5.1060e-06,  4.6574e-07,  8.8648e-06,\n",
       "             6.4084e-06,  6.6708e-07,  6.3335e-08]),\n",
       "    'exp_avg_sq': tensor([8.8451e-17, 8.6712e-17, 5.7186e-05, 3.4010e-06, 3.1229e-06, 1.3365e-06,\n",
       "            4.1035e-06, 1.4482e-06, 1.7407e-06, 1.9845e-06, 1.1723e-06, 1.1539e-05,\n",
       "            1.7608e-05, 5.6159e-06, 7.8640e-06, 4.8221e-06, 1.8898e-06, 1.2331e-05,\n",
       "            1.8398e-06, 3.1468e-06, 4.1761e-06, 7.6596e-06, 9.2641e-06, 6.7605e-06,\n",
       "            5.0605e-06, 9.5985e-06, 3.0252e-06, 3.7465e-06, 6.4493e-06, 8.7584e-07,\n",
       "            3.3661e-06, 9.2608e-07, 2.3879e-06, 5.3084e-06, 2.5671e-06, 3.8208e-06,\n",
       "            9.4122e-07, 3.3116e-06, 2.7288e-06, 2.5221e-06, 1.6894e-06, 1.8671e-07,\n",
       "            2.6129e-07, 8.4767e-06, 1.3548e-06, 1.4313e-07, 2.9781e-07, 1.0939e-06,\n",
       "            2.3212e-07, 1.4058e-07, 1.1564e-06, 9.9373e-07, 4.4146e-06, 1.2185e-07,\n",
       "            1.0904e-06, 1.2686e-07, 3.3851e-06, 6.5560e-06, 5.3863e-07, 3.6265e-06,\n",
       "            2.1158e-07, 6.3892e-07, 4.3173e-07, 2.4597e-06, 3.0171e-08, 3.7601e-06,\n",
       "            9.8807e-07, 5.1050e-08, 2.7602e-07, 9.0613e-10, 4.1168e-11, 6.2824e-11,\n",
       "            8.2403e-09, 4.9521e-09, 1.1687e-08, 2.6864e-09, 8.7651e-09, 2.2800e-09,\n",
       "            3.5376e-09, 3.6260e-10, 5.5784e-11, 1.5905e-09, 3.1558e-09, 2.0436e-09,\n",
       "            3.4023e-09, 1.7873e-09, 1.4201e-08, 1.0454e-09, 2.2531e-10, 1.5800e-09,\n",
       "            1.9743e-09, 1.5299e-10, 2.3146e-12])}},\n",
       "  'param_groups': [{'lr': 0.001,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0,\n",
       "    'amsgrad': False,\n",
       "    'maximize': False,\n",
       "    'params': [0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4,\n",
       "     5,\n",
       "     6,\n",
       "     7,\n",
       "     8,\n",
       "     9,\n",
       "     10,\n",
       "     11,\n",
       "     12,\n",
       "     13,\n",
       "     14,\n",
       "     15,\n",
       "     16,\n",
       "     17]}]},\n",
       " 'char_to_ix': {'<PAD>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<start>': 2,\n",
       "  '<end>': 3,\n",
       "  'k': 4,\n",
       "  'a': 5,\n",
       "  's': 6,\n",
       "  'e': 7,\n",
       "  'm': 8,\n",
       "  ' ': 9,\n",
       "  'b': 10,\n",
       "  'u': 11,\n",
       "  'n': 12,\n",
       "  'h': 13,\n",
       "  'i': 14,\n",
       "  'r': 15,\n",
       "  'o': 16,\n",
       "  't': 17,\n",
       "  'p': 18,\n",
       "  'g': 19,\n",
       "  'w': 20,\n",
       "  'l': 21,\n",
       "  'd': 22,\n",
       "  'c': 23,\n",
       "  'y': 24,\n",
       "  'f': 25,\n",
       "  '1': 26,\n",
       "  '(': 27,\n",
       "  ')': 28,\n",
       "  '2': 29,\n",
       "  '5': 30,\n",
       "  '3': 31,\n",
       "  '6': 32,\n",
       "  '4': 33,\n",
       "  '7': 34,\n",
       "  '0': 35,\n",
       "  '!': 36,\n",
       "  '9': 37,\n",
       "  '8': 38,\n",
       "  '\"': 39,\n",
       "  'v': 4,\n",
       "  'x': 5,\n",
       "  'z': 6,\n",
       "  'é': 7,\n",
       "  'q': 8,\n",
       "  \"'\": 9,\n",
       "  'j': 10,\n",
       "  'ฺ': 11,\n",
       "  'è': 12,\n",
       "  '岸': 13,\n",
       "  '田': 14,\n",
       "  '文': 15,\n",
       "  '雄': 16,\n",
       "  '.': 17,\n",
       "  '–': 18},\n",
       " 'ix_to_char': {0: '<PAD>',\n",
       "  1: '<UNK>',\n",
       "  2: '<start>',\n",
       "  3: '<end>',\n",
       "  4: 'v',\n",
       "  5: 'x',\n",
       "  6: 'z',\n",
       "  7: 'é',\n",
       "  8: 'q',\n",
       "  9: \"'\",\n",
       "  10: 'j',\n",
       "  11: 'ฺ',\n",
       "  12: 'è',\n",
       "  13: '岸',\n",
       "  14: '田',\n",
       "  15: '文',\n",
       "  16: '雄',\n",
       "  17: '.',\n",
       "  18: '–',\n",
       "  19: 'g',\n",
       "  20: 'w',\n",
       "  21: 'l',\n",
       "  22: 'd',\n",
       "  23: 'c',\n",
       "  24: 'y',\n",
       "  25: 'f',\n",
       "  26: '1',\n",
       "  27: '(',\n",
       "  28: ')',\n",
       "  29: '2',\n",
       "  30: '5',\n",
       "  31: '3',\n",
       "  32: '6',\n",
       "  33: '4',\n",
       "  34: '7',\n",
       "  35: '0',\n",
       "  36: '!',\n",
       "  37: '9',\n",
       "  38: '8',\n",
       "  39: '\"'},\n",
       " 'target_char_to_ix': {'<PAD>': 0,\n",
       "  '<start>': 1,\n",
       "  '<end>': 2,\n",
       "  'เ': 3,\n",
       "  'ก': 4,\n",
       "  'ษ': 5,\n",
       "  'ม': 6,\n",
       "  ' ': 7,\n",
       "  'บ': 8,\n",
       "  'ุ': 9,\n",
       "  'ญ': 10,\n",
       "  'า': 11,\n",
       "  'ห': 12,\n",
       "  'ั': 13,\n",
       "  'น': 14,\n",
       "  'ิ': 15,\n",
       "  'โ': 16,\n",
       "  'ร': 17,\n",
       "  'ธ': 18,\n",
       "  'ศ': 19,\n",
       "  'ล': 20,\n",
       "  'พ': 21,\n",
       "  '่': 22,\n",
       "  'ท': 23,\n",
       "  'ย': 24,\n",
       "  '์': 25,\n",
       "  'ี': 26,\n",
       "  'ต': 27,\n",
       "  'อ': 28,\n",
       "  '็': 29,\n",
       "  'ง': 30,\n",
       "  'แ': 31,\n",
       "  'ซ': 32,\n",
       "  'ส': 33,\n",
       "  'ว': 34,\n",
       "  'ะ': 35,\n",
       "  'ำ': 36,\n",
       "  'ด': 37,\n",
       "  'จ': 38,\n",
       "  'ค': 39,\n",
       "  'ณ': 40,\n",
       "  'ฑ': 41,\n",
       "  'ึ': 42,\n",
       "  '้': 43,\n",
       "  'ู': 44,\n",
       "  'ฤ': 45,\n",
       "  'ฐ': 46,\n",
       "  'ข': 47,\n",
       "  'ฒ': 48,\n",
       "  'ฎ': 49,\n",
       "  'ป': 50,\n",
       "  'ไ': 51,\n",
       "  'ช': 52,\n",
       "  'ฏ': 53,\n",
       "  'ภ': 54,\n",
       "  'ฆ': 55,\n",
       "  'ฟ': 56,\n",
       "  'ฮ': 57,\n",
       "  'ื': 58,\n",
       "  'ผ': 59,\n",
       "  '๋': 60,\n",
       "  'ใ': 61,\n",
       "  '๊': 62,\n",
       "  'ถ': 63,\n",
       "  'ฌ': 64,\n",
       "  'ฉ': 65,\n",
       "  'ฝ': 66,\n",
       "  'ฬ': 67,\n",
       "  '.': 68,\n",
       "  'ฦ': 69,\n",
       "  '(': 70,\n",
       "  ')': 71,\n",
       "  'ฯ': 72,\n",
       "  '-': 73,\n",
       "  'ฃ': 74,\n",
       "  'ๆ': 75,\n",
       "  '2': 76,\n",
       "  'ๅ': 77,\n",
       "  'ฅ': 78,\n",
       "  'ฺ': 79,\n",
       "  'ํ': 80,\n",
       "  '5': 81,\n",
       "  '3': 82,\n",
       "  '6': 83,\n",
       "  '4': 84,\n",
       "  '7': 85,\n",
       "  '1': 86,\n",
       "  '0': 87,\n",
       "  '!': 88,\n",
       "  '9': 89,\n",
       "  '8': 90,\n",
       "  '\"': 91,\n",
       "  '๙': 92},\n",
       " 'ix_to_target_char': {0: '<PAD>',\n",
       "  1: '<start>',\n",
       "  2: '<end>',\n",
       "  3: 'เ',\n",
       "  4: 'ก',\n",
       "  5: 'ษ',\n",
       "  6: 'ม',\n",
       "  7: ' ',\n",
       "  8: 'บ',\n",
       "  9: 'ุ',\n",
       "  10: 'ญ',\n",
       "  11: 'า',\n",
       "  12: 'ห',\n",
       "  13: 'ั',\n",
       "  14: 'น',\n",
       "  15: 'ิ',\n",
       "  16: 'โ',\n",
       "  17: 'ร',\n",
       "  18: 'ธ',\n",
       "  19: 'ศ',\n",
       "  20: 'ล',\n",
       "  21: 'พ',\n",
       "  22: '่',\n",
       "  23: 'ท',\n",
       "  24: 'ย',\n",
       "  25: '์',\n",
       "  26: 'ี',\n",
       "  27: 'ต',\n",
       "  28: 'อ',\n",
       "  29: '็',\n",
       "  30: 'ง',\n",
       "  31: 'แ',\n",
       "  32: 'ซ',\n",
       "  33: 'ส',\n",
       "  34: 'ว',\n",
       "  35: 'ะ',\n",
       "  36: 'ำ',\n",
       "  37: 'ด',\n",
       "  38: 'จ',\n",
       "  39: 'ค',\n",
       "  40: 'ณ',\n",
       "  41: 'ฑ',\n",
       "  42: 'ึ',\n",
       "  43: '้',\n",
       "  44: 'ู',\n",
       "  45: 'ฤ',\n",
       "  46: 'ฐ',\n",
       "  47: 'ข',\n",
       "  48: 'ฒ',\n",
       "  49: 'ฎ',\n",
       "  50: 'ป',\n",
       "  51: 'ไ',\n",
       "  52: 'ช',\n",
       "  53: 'ฏ',\n",
       "  54: 'ภ',\n",
       "  55: 'ฆ',\n",
       "  56: 'ฟ',\n",
       "  57: 'ฮ',\n",
       "  58: 'ื',\n",
       "  59: 'ผ',\n",
       "  60: '๋',\n",
       "  61: 'ใ',\n",
       "  62: '๊',\n",
       "  63: 'ถ',\n",
       "  64: 'ฌ',\n",
       "  65: 'ฉ',\n",
       "  66: 'ฝ',\n",
       "  67: 'ฬ',\n",
       "  68: '.',\n",
       "  69: 'ฦ',\n",
       "  70: '(',\n",
       "  71: ')',\n",
       "  72: 'ฯ',\n",
       "  73: '-',\n",
       "  74: 'ฃ',\n",
       "  75: 'ๆ',\n",
       "  76: '2',\n",
       "  77: 'ๅ',\n",
       "  78: 'ฅ',\n",
       "  79: 'ฺ',\n",
       "  80: 'ํ',\n",
       "  81: '5',\n",
       "  82: '3',\n",
       "  83: '6',\n",
       "  84: '4',\n",
       "  85: '7',\n",
       "  86: '1',\n",
       "  87: '0',\n",
       "  88: '!',\n",
       "  89: '9',\n",
       "  90: '8',\n",
       "  91: '\"',\n",
       "  92: '๙'},\n",
       " 'encoder_params': (40, 128, 256, 0.5),\n",
       " 'decoder_params': (93, 128, 256, 0.5)}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8d1e31c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'loss', 'model_state_dict', 'optimizer_state_dict', 'char_to_ix', 'ix_to_char', 'target_char_to_ix', 'ix_to_target_char', 'encoder_params', 'decoder_params'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3c8e12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6c20d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ccb8f20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = data['char_to_ix']\n",
    "ix_to_char = data['ix_to_char'] \n",
    "target_char_to_ix = data['target_char_to_ix']\n",
    "ix_to_target_char = data['ix_to_target_char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84f47f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "df48fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "78bcc917",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\"learning_rate\": learning_rate, \"epochs\": N_EPOCHS, \"batch_size\": BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e2302add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b1c13fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator), total = len(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        source_seq, source_seq_len = batch['input_tensor'], batch['input_length']\n",
    "        batch_size = source_seq.size(0)\n",
    "        \n",
    "        # target_seq: (batch_size , MAX_LENGTH)\n",
    "        # output: (MAX_LENGTH , batch_size , target_vocab_size)\n",
    "        target_seq = batch['target_tensor']\n",
    "\n",
    "        output = model(source_seq, source_seq_len, target_seq, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        \n",
    "        # target_seq -> (MAX_LENGTH , batch_size)\n",
    "        target_seq = target_seq.transpose(0, 1)\n",
    "\n",
    "        # target_seq -> ((MAX_LENGTH - 1) * batch_size)\n",
    "        target_seq = target_seq[1:].contiguous().view(-1)\n",
    "\n",
    "        # output -> ((MAX_LENGTH -1) * batch_size, target_vocab_size)        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "\n",
    "        loss = criterion(output, target_seq)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "47c3f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char = thai_romanization_dataset.lang_th.char2index ,thai_romanization_dataset.lang_th.index2char , thai_romanization_dataset.lang_th_romanized.char2index ,  thai_romanization_dataset.lang_th_romanized.index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b42170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            source_seq, source_seq_len = batch['input_tensor'], batch['input_length']\n",
    "            batch_size = source_seq.size(0)\n",
    "\n",
    "            # target_seq: (batch_size , MAX_LENGTH)\n",
    "            # output: (MAX_LENGTH , batch_size , target_vocab_size)\n",
    "            target_seq = batch['target_tensor']\n",
    "            output = model(source_seq, source_seq_len, target_seq)\n",
    "        \n",
    "            # target_seq -> (MAX_LENGTH , batch_size)\n",
    "            target_seq = target_seq.transpose(0, 1)\n",
    "\n",
    "            # target_seq -> ((MAX_LENGTH - 1) * batch_size)\n",
    "            target_seq = target_seq[1:].contiguous().view(-1)\n",
    "\n",
    "            # output -> ((MAX_LENGTH -1) * batch_size, target_vocab_size)        \n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "\n",
    "            loss = criterion(output, target_seq)\n",
    "            wandb.log({\"evaluate_loss\":loss.item()})\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "617d8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text, char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char):\n",
    "    model.eval()\n",
    "\n",
    "    input_seq =  [ch for ch in text] +  ['<end>']\n",
    "    numericalized = [char_2_ix[ch] for ch in input_seq] \n",
    "    \n",
    "#     print('input ',numericalized)\n",
    "    sentence_length = [len(numericalized)]\n",
    "\n",
    "    tensor = torch.LongTensor(numericalized).view(1, -1)\n",
    "    \n",
    "#     print(tensor)\n",
    "    translation_tensor_logits = model(tensor, sentence_length, None, 0) \n",
    "#     print(translation_tensor_logits)\n",
    "    try:\n",
    "        translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1).cpu().numpy()\n",
    "        translation_indices = [t for t in translation_tensor]\n",
    "        \n",
    "#         print('translation_tensor', translation_tensor)\n",
    "        translation = [ix_to_target_char[t] for t in translation_tensor]\n",
    "    except:\n",
    "        translation_indices = [0]\n",
    "        translation = ['<pad>']\n",
    "    return ''.join(translation), translation_indices\n",
    "\n",
    "def show_inference_example(model, input_texts, target_texts, char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char):\n",
    "    for index, input_text in enumerate(input_texts):\n",
    "        prediction, indices = inference(model, input_text, char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char)\n",
    "        print('groundtruth: {}'.format(target_texts[index]))\n",
    "        print(' prediction: {} {}\\n'.format(prediction, indices))\n",
    "        \n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a309bb4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtruth: \n",
      " prediction: มมม [6, 6, 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_inference_example(model, ['m'], [''], char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fcc12afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = 5\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c299a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(1, N_EPOCHS+1)):\n",
    "#  model.train()\n",
    "#  start_time = time.time()\n",
    "#  train_loss = train(model, train_dataset_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=0.5)\n",
    "#  #valid_loss = evaluate(model, val_dataset_loader, criterion)\n",
    "#  end_time = time.time()\n",
    "#  save_model('new_model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)\n",
    "#  wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "375eff3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b820590a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model('new_model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "697e524b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3ae19588",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "055b6877482046e6b8670f0dbf8b06c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3b145192d7486ba1c32a9eaa769677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "263ea7a7b54349ea941c0d1db1707d59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c009b8e4364461390d85812c2a4375d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bcdf74f01c449ca490f1afc3d60ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26bc23d151e5467a954d437c179a82ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, N_EPOCHS+1)):\n",
    " model.train()\n",
    " start_time = time.time()\n",
    " train_loss = train(model, train_dataset_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=0.5)\n",
    " #valid_loss = evaluate(model, val_dataset_loader, criterion)\n",
    " end_time = time.time()\n",
    " save_model('model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)\n",
    " wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4bbcf595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5322823d44a4cb08dbe1fc160c76ebc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "265aa3b6539247fbb7328ed85f8456c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  6\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b953193eb6654f809d56eba95db495b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bfc854c66a42aabb4634a15527d2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7e4cedca8b41509efd53328fc84c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c52c4c67cfc54dffaeab0afb0cf01f29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  10\n"
     ]
    }
   ],
   "source": [
    "# for epoch in tqdm(range(6,10+1)):\n",
    "#  model.train()\n",
    "#  start_time = time.time()\n",
    "#  train_loss = train(model, train_dataset_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=0.5)\n",
    "#  #valid_loss = evaluate(model, val_dataset_loader, criterion)\n",
    "#  end_time = time.time()\n",
    "#  save_model('new_model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)\n",
    "#  wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40737612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
