{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea859061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d655f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import sys\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms, utils\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import OrderedDict\n",
    "\n",
    "SEED = 0\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f737ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dbd2186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwannaphong\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2022-05-23 04:50:56.214628: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.16"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/project/thai_transliterate/cs-th-en-seq2seq-pythai/wandb/run-20220523_045052-16wbcs7a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/wannaphong/cs-th-en-seq2seq_attention/runs/16wbcs7a\" target=\"_blank\">flowing-yogurt-14</a></strong> to <a href=\"https://wandb.ai/wannaphong/cs-th-en-seq2seq_attention\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/wannaphong/cs-th-en-seq2seq_attention/runs/16wbcs7a?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7fcfc7b4b1c0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "# start a new experiment\n",
    "wandb.init(project=\"cs-th-en-seq2seq_attention\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "505dd806",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = os.path.join('..','dataset','cs')\n",
    "\n",
    "train_filepaths = os.path.join(path_dataset,'train.tsv')\n",
    "dev_filepaths = os.path.join(path_dataset,'dev.tsv')\n",
    "test_filepaths = os.path.join(path_dataset,'test.tsv')\n",
    "\n",
    "train_df = pd.read_csv(train_filepaths,sep=\"\\t\")\n",
    "dev_df = pd.read_csv(dev_filepaths,sep=\"\\t\")\n",
    "test_df = pd.read_csv(test_filepaths,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59d68e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    input_texts = [str(i) for i in list(data_path['word'])]\n",
    "    target_texts =[str(i) for i in  list(data_path['roman'])]\n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b7b80e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special characters\n",
    "UNK_token = '<UNK>'\n",
    "PAD_token = '<PAD>'\n",
    "START_token = '<start>'\n",
    "END_token = '<end>'\n",
    "MAX_LENGTH = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395421f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Language:\n",
    "    def __init__(self, name,char2index={},index2char={}, is_input=False):\n",
    "        self.name = name\n",
    "        self.characters = set()\n",
    "        self.n_chars = 0\n",
    "        self.char2index = {}\n",
    "        self.index2char = {}\n",
    "\n",
    "        if is_input == True:\n",
    "            self.index2char = { 0: PAD_token, 1: UNK_token, 2: START_token, 3: END_token }\n",
    "            self.char2index = { ch:i for i, ch in self.index2char.items() } #reverse dictionary\n",
    "            self.n_chars = 4\n",
    "        else:\n",
    "            self.index2char = { 0: PAD_token, 1: START_token, 2: END_token }\n",
    "            self.char2index = { ch:i for i, ch in self.index2char.items() } #reverse dictionary\n",
    "            self.n_chars = 3\n",
    "        if char2index != {} and index2char != {}:\n",
    "            print(\"cat!!!\")\n",
    "            self.char2index = char2index\n",
    "            self.index2char = index2char\n",
    "            self.characters = set(list(self.char2index.keys()))\n",
    "\n",
    "    def addText(self, text):\n",
    "        for character in text:\n",
    "            self.addCharacter(character)\n",
    "    \n",
    "    def addCharacter(self, character):\n",
    "        if character not in self.char2index.keys():\n",
    "            self.char2index[character] = self.n_chars\n",
    "            self.index2char[self.n_chars] = character\n",
    "            self.n_chars += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1415ee6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def indexesFromText(lang, text):\n",
    "    \"\"\"returns indexes for all character given the text in the specified language\"\"\"\n",
    "    return [lang.char2index[char] for char in text]\n",
    "\n",
    "def tensorFromText(lang, text):\n",
    "    \"\"\"construct a tensor given the text in the specified language\"\"\"\n",
    "    indexes = indexesFromText(lang, text)\n",
    "    indexes.append(lang.char2index[END_token])\n",
    "    \n",
    "    no_padded_seq_length = len(indexes) # Number of characters in the text (including <END> token)\n",
    "    # Add padding token to make all tensors in the same length\n",
    "    for i in range(len(indexes), MAX_LENGTH): # padding\n",
    "        indexes.append(lang.char2index[PAD_token])\n",
    "        \n",
    "    return torch.tensor(indexes, dtype=torch.long), no_padded_seq_length\n",
    "\n",
    "def filterPair(p1, p2):\n",
    "    \"\"\"filter for the pair the both texts has length less than `MAX_LENGTH`\"\"\"\n",
    "    return len(p1) < MAX_LENGTH and len(p2) < MAX_LENGTH\n",
    "\n",
    "def tensorsFromPair(pair, lang1, lang2):\n",
    "    \"\"\"construct two tensors from a pair of source and target text specified by source and target language\"\"\"\n",
    "    input_tensor, input_length = tensorFromText(lang1, pair[0])\n",
    "    target_tensor, target_length = tensorFromText(lang2, pair[1])\n",
    "    return input_tensor, target_tensor, input_length, target_length\n",
    "\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        input_text, target_text, lang_th, lang_th_romanized = sample['input_text'], sample['target_text'],                                                               sample['lang_th'], sample['lang_th_romanized']\n",
    "\n",
    "        input_tensor, target_tensor, input_length, target_length = tensorsFromPair([input_text, target_text], \n",
    "                                                                                   lang_th, \n",
    "                                                                                   lang_th_romanized)\n",
    "        \n",
    "        return {\n",
    "                'input_text': input_text,\n",
    "                'target_text': target_text,\n",
    "                'input_length': input_length,\n",
    "                'target_length': target_length,\n",
    "                'input_tensor': input_tensor,\n",
    "                'target_tensor': target_tensor\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571a9fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThaiRomanizationDataset(Dataset):\n",
    "    \"\"\"Thai Romanization Dataset class\"\"\"\n",
    "    def __init__(self, \n",
    "                 data_path,\n",
    "                 config = {},\n",
    "                 transform=transforms.Compose([ ToTensor() ])):\n",
    "\n",
    "        input_texts, target_texts = load_data(data_path)\n",
    "        \n",
    "        self.input_texts = input_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.transform = transform\n",
    "        self.config=config\n",
    "        self.lang_th = Language('th', self.config['char_to_ix'],self.config['ix_to_char'],is_input=True)\n",
    "        self.lang_th_romanized = Language('th_romanized', self.config['target_char_to_ix'],self.config['ix_to_target_char'], is_input=False)\n",
    "        self.counter = Counter()\n",
    "        self.pairs = []\n",
    "        self.prepareData()\n",
    "\n",
    "    def prepareData(self):\n",
    "        for i in range(len(self.input_texts)):\n",
    "            \n",
    "            input_text = str(self.input_texts[i])\n",
    "            target_text = str(self.target_texts[i])\n",
    "            \n",
    "            # Count the number of input and target sequences with length `x`\n",
    "            self.counter.update({ \n",
    "                                  'len_input_{}'.format(len(input_text)): 1, \n",
    "                                  'len_target_{}'.format(len(target_text)): 1 \n",
    "                                })\n",
    "            \n",
    "            if filterPair(input_text, target_text):\n",
    "                self.pairs.append((input_text, target_text))\n",
    "                self.lang_th.addText(input_text)\n",
    "                self.lang_th_romanized.addText(target_text)    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sample = dict()\n",
    "        sample['input_text'] = self.pairs[idx][0]\n",
    "        sample['target_text'] = self.pairs[idx][1]\n",
    "        \n",
    "        sample['lang_th'] = self.lang_th\n",
    "        sample['lang_th_romanized'] = self.lang_th_romanized\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd21095d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    return torch.load(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75ab2bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=load_model(\"thai2rom-pytorch-72-Copy1.attn.v6.best_epoch-72.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a832f72b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '<PAD>',\n",
       " 1: '<start>',\n",
       " 2: '<end>',\n",
       " 3: 'k',\n",
       " 4: 'a',\n",
       " 5: 's',\n",
       " 6: 'e',\n",
       " 7: 'm',\n",
       " 8: ' ',\n",
       " 9: 'b',\n",
       " 10: 'u',\n",
       " 11: 'n',\n",
       " 12: 'h',\n",
       " 13: 'i',\n",
       " 14: 'r',\n",
       " 15: 'o',\n",
       " 16: 't',\n",
       " 17: 'p',\n",
       " 18: 'g',\n",
       " 19: 'w',\n",
       " 20: 'l',\n",
       " 21: 'd',\n",
       " 22: 'c',\n",
       " 23: 'y',\n",
       " 24: 'f',\n",
       " 25: '1',\n",
       " 26: '(',\n",
       " 27: ')',\n",
       " 28: '2',\n",
       " 29: '5',\n",
       " 30: '3',\n",
       " 31: '6',\n",
       " 32: '4',\n",
       " 33: '7',\n",
       " 34: '0',\n",
       " 35: '!',\n",
       " 36: '9',\n",
       " 37: '8',\n",
       " 38: '\"'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['ix_to_target_char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf1f4f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat!!!\n",
      "cat!!!\n"
     ]
    }
   ],
   "source": [
    "thai_romanization_dataset = ThaiRomanizationDataset(train_df,config=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00a9f043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '<PAD>', 1: '<start>', 2: '<end>', 3: 'v', 4: 'x', 5: 'z', 6: 'é', 7: 'q', 8: \"'\", 9: 'j', 10: 'ฺ', 11: 'è', 12: '岸', 13: '田', 14: '文', 15: '雄', 16: '.', 17: '–', 18: 'g', 19: 'w', 20: 'l', 21: 'd', 22: 'c', 23: 'y', 24: 'f', 25: '1', 26: '(', 27: ')', 28: '2', 29: '5', 30: '3', 31: '6', 32: '4', 33: '7', 34: '0', 35: '!', 36: '9', 37: '8', 38: '\"'}\n"
     ]
    }
   ],
   "source": [
    "print(thai_romanization_dataset.lang_th_romanized.index2char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a6a4981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples:  3041\n"
     ]
    }
   ],
   "source": [
    "SEED = 0\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "TRAIN_RATIO = 0.8\n",
    "\n",
    "N = len(thai_romanization_dataset)\n",
    "\n",
    "print('Number of samples: ', N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f954d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_loader = torch.utils.data.DataLoader(\n",
    "                                             thai_romanization_dataset,\n",
    "                                             batch_size=BATCH_SIZE, \n",
    "                                             num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "099e2010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train mini-batches 12\n"
     ]
    }
   ],
   "source": [
    "print('Number of train mini-batches', len(train_dataset_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df0d1484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Seq2Seq Model architecture\n",
    "\n",
    "# ## 1. Encoder\n",
    "\n",
    "# Encoder \n",
    "#     - Embedding layer :(vocaburay_size, embedding_size) \n",
    "#         Input: (batch_size, sequence_length)\n",
    "#         Output: (batch_size, sequence_length, embebeding_size)\n",
    "#       \n",
    "#     - Bi-LSTM layer : (input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         Input: (input=(batch_size, seq_len, embebeding_size),  hidden)\n",
    "#         Output: (output=(batch_size, seq_len, hidden_size),\n",
    "#                  (h_n, c_n))\n",
    "#      \n",
    "#      \n",
    "# __Steps:__\n",
    "# \n",
    "# 1. Receives a batch of source sequences (batch_size, MAX_LENGTH) and a 1-D array of the length for each sequence (batch_size).\n",
    "#      \n",
    "# 2. Sort sequences in the batch by sequence length (number of tokens in the sequence where <PAD> token is excluded).\n",
    "# \n",
    "# 3. Feed the batch of sorted sequences into the Embedding Layer to maps source character indices into vectors. (batch_size,  sequence_length, embebeding_size)\n",
    "# \n",
    "# 4. Use `pack_padded_sequence` to let LSTM packed input with same length at time step $t$ together. This will reduce time required for training by avoid feeding `<PAD>` token to the LSTMs.\n",
    "# \n",
    "# \n",
    "# 5. Returns LSTM outputs in the unsorted order, and the LSTM hidden state vectors.\n",
    "#      \n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size, dropout=0.5):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.character_embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, \n",
    "                            hidden_size=hidden_size // 2, \n",
    "                            bidirectional=True,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self, sequences, sequences_lengths):\n",
    "        batch_size = sequences.size(0)\n",
    "        self.hidden = self.init_hidden(batch_size) # batch_size\n",
    "\n",
    "        # sequences :(batch_size, sequence_length=MAX_LENGTH)\n",
    "        # sequences_lengths: (batch_size)  # an 1-D indicating length of each sequence (excluded <PAD> token) in `seq`\n",
    "        \n",
    "        # 1. Firstly we sort `sequences_lengths` according to theirs values and keep list of indexes to perform sorting\n",
    "        sequences_lengths = np.sort(sequences_lengths)[::-1] # sort in ascending order and reverse it\n",
    "        index_sorted = np.argsort(-sequences_lengths) # use negation in sort in descending order\n",
    "        index_unsort = np.argsort(index_sorted) # to unsorted sequence\n",
    "        \n",
    "        \n",
    "        # 2. Then, we change position of sequence in `sequences` according to `index_sorted`\n",
    "        index_sorted = torch.from_numpy(index_sorted)\n",
    "        sequences = sequences.index_select(0, index_sorted)\n",
    "        \n",
    "        # 3. Feed to Embedding Layer\n",
    "        \n",
    "        sequences = self.character_embedding(sequences)\n",
    "        sequences = self.dropout(sequences)\n",
    "        \n",
    "#         print('sequences',sequences.size(), sequences)\n",
    "            \n",
    "        # 3. Use function: pack_padded_sequence to let LSTM packed input with same length at time step T together\n",
    "        \n",
    "        # Quick fix: Use seq_len.copy(), instead of seq_len to fix `Torch.from_numpy not support negative strides`\n",
    "        # ndarray.copy() will alocate new memory for numpy array which make it normal, I mean the stride is not negative any more.\n",
    "\n",
    "        sequences_packed = nn.utils.rnn.pack_padded_sequence(sequences,\n",
    "                                                             sequences_lengths.copy(),\n",
    "                                                             batch_first=True)\n",
    "#         print('sequences_packed', sequences_packed)\n",
    "\n",
    "        # 4. Feed to LSTM\n",
    "        sequences_output, self.hidden = self.lstm(sequences_packed, self.hidden)\n",
    "        \n",
    "        # 5. Unpack\n",
    "        sequences_output, _ = nn.utils.rnn.pad_packed_sequence(sequences_output, batch_first=True)\n",
    "\n",
    "        # 6. Un-sort by length\n",
    "        index_unsort = torch.from_numpy(index_unsort)\n",
    "        sequences_output = sequences_output.index_select(0, Variable(index_unsort))\n",
    "\n",
    "#         print('hidden shape', self.hidden[0].shape, self.hidden[0], self.hidden[1].shape, self.hidden[1])\n",
    "        return sequences_output, self.hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        h_0 = torch.zeros([2, batch_size, self.hidden_size // 2], requires_grad=True)\n",
    "        c_0 = torch.zeros([2, batch_size, self.hidden_size // 2], requires_grad=True)\n",
    "        \n",
    "        return (h_0, c_0)\n",
    "    \n",
    "def save_model(name, epoch, loss, model):\n",
    "    print('Save model at epoch ', epoch)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'loss': loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'char_to_ix': thai_romanization_dataset.lang_th.char2index,\n",
    "        'ix_to_char': thai_romanization_dataset.lang_th.index2char,\n",
    "        'target_char_to_ix': thai_romanization_dataset.lang_th_romanized.char2index,\n",
    "        'ix_to_target_char':thai_romanization_dataset.lang_th_romanized.index2char,\n",
    "        'encoder_params': (INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, ENC_DROPOUT), \n",
    "        'decoder_params': (OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, DEC_DROPOUT)\n",
    "        \n",
    "    }, \"{}.best_epoch-{}.tar\".format(name, epoch))\n",
    "    \n",
    " \n",
    "    \n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "# ## Decoder\n",
    "\n",
    "#    \n",
    "# Decoder architecture\n",
    "# \n",
    "#     - Embedding layer :(vocabulary_size, embebeding_size)\n",
    "#         Input: (batch_size, sequence_length=1)\n",
    "#         Output: (batch_size, sequence_length=1, embebeding_size)\n",
    "#     - RNN layer :input_size=embebeding_size, hidden_size, num_layers, batch_first=True)\n",
    "#         Input: (input=(batch_size, input_size=embedding_dimension), hidden:tuple=encoder_hidden\n",
    "#         Output: (batch_size, seq_len, hidden_size), (h_n, c_n)\n",
    "#     - Attention Layer: (in_features=hidden_size, out_features=hidden_size, bias=True)\n",
    "#     - Linear Layer: (in_features, out_features=vocabulary_size)\n",
    "#         Input: (batch_size, hidden_size)\n",
    "#         Output: (batch_size, vocabulary_size)\n",
    "#     \n",
    "#     - Softmax layer\n",
    "#         Input: (batch_size, vocabulary_size)\n",
    "#         Output: (batch_size, vocabulary_size)\n",
    "# \n",
    "# \n",
    "# \n",
    "# For the Attention mechanishm in the Decoder, Luong-style attention [[Luong et. al (2015)](https://arxiv.org/abs/1508.04025)] is used. \n",
    "# \n",
    "# \n",
    "# \n",
    "# __Steps:__\n",
    "# \n",
    "# 1. Receives a batch of <start> token (batch_size, 1) and a batch of Encoder's hidden state.\n",
    "#      \n",
    "# 2. Embed input into vectors.\n",
    "# \n",
    "# 3. Feed vectors from (2) to the LSTM.\n",
    "# \n",
    "# 4. Feed the output of LSTM at time step $t_1$ and Encoder output to the Attention Layer.\n",
    "# \n",
    "# 5. Attention layer, returns weights for Encoder's hidden states in every time step (masked out the time step with <PAD> token), then multiply with Encoder's hidden states to obtain a context vector\n",
    "#     \n",
    "# 6. Concatenate both decoder hidden state and the context vector, feed to a linear layer, and return its output.\n",
    "# \n",
    "# 7. Decoder then returns, final output, decoder's hidden state, attention weights, and context vector at time step $t$\n",
    "\n",
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "class Attn(nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        if self.method == 'general':\n",
    "            self.attn = nn.Linear(self.hidden_size, hidden_size)\n",
    "\n",
    "        elif self.method == 'concat':\n",
    "            self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.other = nn.Parameter(torch.FloatTensor(1, hidden_size))\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs, mask):\n",
    "        # hidden: B x 1 x h ; \n",
    "        # encoder_outputs: B x S x h\n",
    "\n",
    "        # Calculate energies for each encoder output\n",
    "        if self.method == 'dot':\n",
    "            attn_energies = torch.bmm(encoder_outputs, hidden.transpose(1, 2)).squeeze(2)  # B x S\n",
    "        elif self.method == 'general':\n",
    "            attn_energies = self.attn(encoder_outputs.view(-1, encoder_outputs.size(-1)))  # (B * S) x h\n",
    "            attn_energies = torch.bmm(attn_energies.view(*encoder_outputs.size()),\n",
    "                                      hidden.transpose(1, 2)).squeeze(2)  # B x S\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.attn(\n",
    "                torch.cat((hidden.expand(*encoder_outputs.size()), encoder_outputs), 2))  # B x S x h\n",
    "            attn_energies = torch.bmm(attn_energies,\n",
    "                                      self.other.unsqueeze(0).expand(*hidden.size()).transpose(1, 2)).squeeze(2)\n",
    "\n",
    "        attn_energies = attn_energies.masked_fill(mask == 0, -1e10)\n",
    "\n",
    "        # Normalize energies to weights in range 0 to 1\n",
    "        return F.softmax(attn_energies, 1)\n",
    "\n",
    "class AttentionDecoder(nn.Module): \n",
    "    \n",
    "    def __init__(self, vocabulary_size, embedding_size, hidden_size, dropout=0.5):\n",
    "        \"\"\"Constructor\"\"\"\n",
    "        super(AttentionDecoder, self).__init__()\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.character_embedding = nn.Embedding(vocabulary_size, embedding_size)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_size + self.hidden_size,\n",
    "                            hidden_size=hidden_size,\n",
    "                            bidirectional=False,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        self.attn = Attn(method=\"general\", hidden_size=self.hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size * 2, vocabulary_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        \n",
    "    def forward(self, input, last_hidden, last_context, encoder_outputs, mask):\n",
    "        \"\"\"\"Defines the forward computation of the decoder\"\"\"\n",
    "        # input: (B, 1) ,\n",
    "        # last_hidden: (num_layers * num_directions, B, hidden_dim)\n",
    "        # last_context: (B, 1, hidden_dim)\n",
    "        # encoder_outputs: (B, S, hidden_dim)\n",
    "        \n",
    "        embedded = self.character_embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # embedded: (batch_size, emb_dim)\n",
    "        rnn_input = torch.cat((embedded, last_context), 2)\n",
    "\n",
    "        output, hidden = self.lstm(rnn_input, last_hidden)        \n",
    "        attn_weights = self.attn(output, encoder_outputs, mask)  # B x S\n",
    "    \n",
    "        #  context = (B, 1, S) x (B, S, hidden_dim)\n",
    "        #  context = (B, 1, hidden_dim)\n",
    "        context = attn_weights.unsqueeze(1).bmm(encoder_outputs)  \n",
    "        \n",
    "        output = torch.cat((context.squeeze(1), output.squeeze(1)), 1)\n",
    "        output = self.linear(output)\n",
    "        \n",
    "        return output, hidden, context, attn_weights\n",
    "\n",
    "\n",
    "# ## Seq2Seq model\n",
    "# \n",
    "# This class encapsulate _Decoder_ and _Encoder_ class.\n",
    "# \n",
    "# __Steps:__\n",
    "# \n",
    "# 1. The input sequcence $X$ is fed into the encoder to receive one hidden state vector.\n",
    "# \n",
    "# 2. The initial decoder hidden state is set to be the hidden state vector of the encoder\n",
    "# \n",
    "# 3. Add a batch of `<start>` tokens (batch_size, 1) as the first input $y_1$\n",
    "#     \n",
    "# 4. Then, decode within a loop:\n",
    "#     - Inserting the input token $y_t$, previous hidden state, $s_{t-1}$, and the context vector $z$ into the decoder\n",
    "#     - Receiveing a prediction $\\hat{y}$ and a new hidden state $s_t$\n",
    "#     - Then, either use teacher forcing to let groundtruth target character as the input for the decoder at time step $t+1$, or let the result from decoder as the input for the next time step.\n",
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module): \n",
    "\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.pad_idx = 0\n",
    "\n",
    "        assert encoder.hidden_size == decoder.hidden_size\n",
    "    \n",
    "    def create_mask(self, source_seq):\n",
    "        mask = (source_seq != self.pad_idx)\n",
    "        return mask\n",
    "        \n",
    "  \n",
    "    def forward(self, source_seq, source_seq_len, target_seq, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "            Parameters:\n",
    "                - source_seq: (batch_size x MAX_LENGTH) \n",
    "                - source_seq_len: (batch_size x 1)\n",
    "                - target_seq: (batch_size x MAX_LENGTH)\n",
    "\n",
    "            Returns\n",
    "        \"\"\"\n",
    "        batch_size = source_seq.size(0)\n",
    "        start_token = thai_romanization_dataset.lang_th_romanized.char2index[\"<start>\"]\n",
    "        end_token = thai_romanization_dataset.lang_th_romanized.char2index[\"<end>\"]\n",
    "        max_len = MAX_LENGTH\n",
    "        target_vocab_size = self.decoder.vocabulary_size\n",
    "\n",
    "        # init a tensor to store decoder outputs\n",
    "        outputs = torch.zeros(max_len, batch_size, target_vocab_size)\n",
    "        \n",
    "        if target_seq is None:\n",
    "            assert teacher_forcing_ratio == 0, \"Must be zero during inference\"\n",
    "            inference = True\n",
    "        else:\n",
    "            inference = False\n",
    "\n",
    "    \n",
    "        # feed mini-batch source sequences into the `Encoder`\n",
    "        encoder_outputs, encoder_hidden = self.encoder(source_seq, source_seq_len)\n",
    "\n",
    "        # create a Tensor of first input for the decoder\n",
    "        decoder_input = torch.tensor([[start_token] * batch_size]).view(batch_size, 1)\n",
    "        \n",
    "        # Initiate decoder output as the last state encoder's hidden state\n",
    "        decoder_hidden_0 = torch.cat([encoder_hidden[0][0], encoder_hidden[0][1]], dim=1).unsqueeze(dim=0)\n",
    "        decoder_hidden_1 = torch.cat([encoder_hidden[1][0], encoder_hidden[1][1]], dim=1).unsqueeze(dim=0)\n",
    "        decoder_hidden = (decoder_hidden_0, decoder_hidden_1) # (hidden state, cell state)\n",
    "\n",
    "        # define a context vector\n",
    "        decoder_context = Variable(torch.zeros(encoder_outputs.size(0), encoder_outputs.size(2))).unsqueeze(1)\n",
    "        \n",
    "        max_source_len = encoder_outputs.size(1)\n",
    "        mask = self.create_mask(source_seq[:, 0:max_source_len])\n",
    "            \n",
    "       \n",
    "        for di in range(max_len):\n",
    "            decoder_output, decoder_hidden, decoder_context, attn_weights = self.decoder(decoder_input,\n",
    "                                                                                    decoder_hidden,\n",
    "                                                                                    decoder_context,\n",
    "                                                                                    encoder_outputs,\n",
    "                                                                                    mask)\n",
    "            # decoder_output: (batch_size, target_vocab_size)\n",
    "\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            outputs[di] = decoder_output\n",
    "    \n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "\n",
    "            decoder_input = target_seq[:, di].reshape(batch_size, 1) if teacher_force else topi.detach() \n",
    "\n",
    "            if inference and decoder_input == end_token:\n",
    "                return outputs[:di]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8507dd6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf07818b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, ENC_DROPOUT = data['encoder_params']\n",
    "OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, DEC_DROPOUT = data['decoder_params']\n",
    "\n",
    "_encoder = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM)\n",
    "_decoder = AttentionDecoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM)\n",
    "\n",
    "#model = Seq2Seq(_encoder, _decoder)\n",
    "model = Seq2Seq(_encoder, _decoder)\n",
    "model.load_state_dict(data['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "700e310f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'epoch': 72,\n",
       " 'loss': 0.260413283491746,\n",
       " 'model_state_dict': OrderedDict([('encoder.character_embedding.weight',\n",
       "               tensor([[-0.0726,  0.1101, -0.0081,  ..., -0.3005, -0.1339,  0.0756],\n",
       "                       [ 0.0037, -0.0155, -0.0141,  ...,  0.0126,  0.0087,  0.0058],\n",
       "                       [ 0.0094,  0.0075,  0.0122,  ...,  0.0100, -0.0125, -0.0116],\n",
       "                       ...,\n",
       "                       [ 0.0368, -0.0111,  0.3473,  ..., -0.0964, -0.0316,  0.0479],\n",
       "                       [ 0.1193, -0.3317,  0.0783,  ..., -0.1472,  0.1345, -0.0841],\n",
       "                       [ 0.1311,  0.0516, -0.1785,  ..., -0.2887, -0.0801, -0.0385]])),\n",
       "              ('encoder.lstm.weight_ih_l0',\n",
       "               tensor([[-0.2214,  0.1812, -0.4905,  ...,  0.7163, -0.1723, -0.3168],\n",
       "                       [-0.5070, -0.2123,  0.0631,  ...,  0.3151, -0.0470, -0.0560],\n",
       "                       [-0.3759,  0.0631, -0.0918,  ...,  0.1992, -0.2342,  0.1362],\n",
       "                       ...,\n",
       "                       [ 0.2773,  0.4461, -0.0771,  ..., -0.4582, -0.0980, -0.1226],\n",
       "                       [-0.0561,  0.0291,  0.1259,  ...,  0.1727,  0.1661, -0.0661],\n",
       "                       [ 0.1224, -0.0640,  0.0766,  ...,  0.2415,  0.2506,  0.2536]])),\n",
       "              ('encoder.lstm.weight_hh_l0',\n",
       "               tensor([[ 0.1377,  0.2804,  0.3021,  ...,  0.6075,  0.0761,  0.3425],\n",
       "                       [ 0.3306,  0.1294, -0.2026,  ...,  0.0619,  0.3223,  0.3889],\n",
       "                       [-0.5328,  0.3220, -0.3202,  ...,  0.6701,  0.4231,  1.2151],\n",
       "                       ...,\n",
       "                       [-0.5501,  0.7969, -0.4243,  ...,  0.1601, -0.0460, -0.2475],\n",
       "                       [-0.3817,  0.3220,  0.2707,  ...,  0.5354, -0.1867,  0.0344],\n",
       "                       [-0.1919,  0.0025,  0.4173,  ...,  0.2131,  0.3676,  0.3596]])),\n",
       "              ('encoder.lstm.bias_ih_l0',\n",
       "               tensor([ 1.6192e-01, -1.0173e-01,  1.7607e-01, -6.3845e-02,  4.1094e-02,\n",
       "                        5.3954e-01, -3.5156e-02, -3.6812e-01, -1.0299e-01, -2.2685e-01,\n",
       "                        3.3280e-01, -2.4141e-01, -2.9932e-01, -6.1739e-01,  4.1298e-01,\n",
       "                       -1.6266e-01,  2.2480e-01,  9.6454e-01, -4.0633e-01, -4.0907e-01,\n",
       "                       -1.6913e-01,  1.3619e-02, -2.8365e-01,  3.5557e-01, -3.2284e-01,\n",
       "                       -4.0822e-01, -4.0520e-01,  4.8922e-01, -3.3449e-01, -4.1355e-01,\n",
       "                       -3.8460e-02, -6.4998e-01, -3.6469e-01, -4.3353e-01, -3.6186e-01,\n",
       "                       -1.9652e-01, -3.2682e-01,  9.2133e-01, -6.3788e-02, -3.4107e-01,\n",
       "                       -4.8700e-01,  2.8923e-02, -2.7358e-01, -2.7718e-01,  2.7085e-01,\n",
       "                       -4.7173e-01,  3.1395e-01, -8.8947e-04, -9.8564e-02, -3.3546e-02,\n",
       "                       -2.5580e-01, -1.4391e-01, -7.3957e-01, -3.4844e-01, -4.5552e-01,\n",
       "                       -6.0748e-01,  3.9800e-01, -2.3090e-01, -5.8514e-02,  1.4433e-01,\n",
       "                        7.4377e-02, -2.2137e-01, -4.5679e-01, -2.7068e-02,  5.9042e-01,\n",
       "                       -5.5538e-01, -3.0177e-01,  2.4567e-01,  4.1420e-01, -3.1932e-01,\n",
       "                        2.5045e-01, -3.6963e-01,  5.9586e-01, -2.9785e-01, -1.2049e-02,\n",
       "                       -7.8603e-02,  3.2105e-01, -4.9629e-01,  1.3940e-01, -2.3267e-01,\n",
       "                        5.1518e-02,  3.0919e+00,  3.4042e-01,  9.4269e-01,  4.2100e-01,\n",
       "                        5.9147e-01, -1.0465e-01,  4.2288e-01, -1.5666e-01, -1.4031e-01,\n",
       "                        1.2965e-01, -6.4088e-02, -2.9331e-02,  6.9017e-03, -7.1539e-01,\n",
       "                       -1.1288e-01,  2.0363e-01, -3.8948e-01, -1.2289e-01,  4.5566e-01,\n",
       "                       -4.3360e-01, -2.5151e-01, -2.5230e-01, -5.8409e-01, -3.1681e-01,\n",
       "                       -1.2046e-01,  4.8569e-01,  2.2228e-01, -2.1791e-01,  1.3214e-01,\n",
       "                        2.7254e-02,  2.5574e-01,  9.1120e-02,  6.8322e-01, -3.1461e-01,\n",
       "                       -9.7995e-02, -2.6109e-01, -5.2161e-01, -3.6704e-01, -5.5174e-01,\n",
       "                       -4.5672e-01,  1.2510e-01, -5.9375e-01, -6.1152e-02, -2.0948e-01,\n",
       "                       -2.1193e-01, -4.7064e-01,  9.2170e-02, -2.1753e-01, -3.4028e-01,\n",
       "                       -4.4344e-01, -2.5821e-01, -5.0870e-01, -7.8463e-02,  3.9635e-02,\n",
       "                       -1.1955e-01, -4.0093e-01,  3.1874e-01, -1.2243e-01,  1.9954e-02,\n",
       "                       -4.5267e-01, -2.0782e-01, -3.8298e-02, -7.3098e-02, -4.4956e-01,\n",
       "                       -4.2683e-01,  2.4358e-01, -1.6741e-01,  9.8016e-03, -4.3879e-02,\n",
       "                        1.0248e-01,  1.1209e-01,  1.1863e-01, -1.3190e-02, -2.3536e-02,\n",
       "                       -6.1193e-02, -2.2476e-01,  1.1317e-02, -2.4364e-01,  2.7718e-01,\n",
       "                       -1.7545e-01,  1.2773e-01, -4.0386e-01, -8.3687e-02, -4.2071e-02,\n",
       "                       -2.9057e-01, -3.6115e-01, -3.5993e-01, -2.0043e-01, -3.5487e-01,\n",
       "                        1.9751e-01,  6.5678e-02,  2.1896e-01,  5.7868e-02, -7.1502e-02,\n",
       "                       -2.1348e-01, -1.6819e-01, -4.1492e-01,  2.6627e-01,  7.2581e-02,\n",
       "                        1.8295e-01, -4.9528e-02, -9.1336e-03, -1.7923e-01, -1.6010e-01,\n",
       "                       -1.7829e-01, -4.9539e-02, -3.5848e-01, -5.0567e-01,  1.1222e-01,\n",
       "                       -7.5657e-02, -2.5165e-01, -4.9129e-01, -1.5151e-01,  2.0590e-01,\n",
       "                       -3.7323e-01, -3.0281e-01,  3.9872e-02, -4.3947e-02, -6.7074e-02,\n",
       "                       -2.8976e-01, -1.3274e-01, -1.2843e-01,  2.5768e-02, -4.7135e-02,\n",
       "                       -3.5619e-02, -3.6184e-01, -1.0629e-01, -3.2356e-01,  4.5887e-01,\n",
       "                       -2.8752e-01,  2.6141e-02, -3.3833e-01, -5.5884e-02, -7.0029e-02,\n",
       "                       -3.7779e-01, -1.8437e-01,  3.2053e-02, -1.6569e-01,  3.7152e-01,\n",
       "                       -4.6902e-01, -6.0996e-01,  8.9776e-02, -2.7489e-01, -2.1252e-01,\n",
       "                       -2.4399e-01,  4.2533e-02, -3.9542e-01,  2.8217e-01, -6.0072e-03,\n",
       "                       -2.2500e-01, -8.6238e-02, -1.8400e-01,  2.2065e-01, -2.8611e-01,\n",
       "                       -3.0947e-01,  1.0189e-01,  4.4694e-01, -2.6594e-01, -4.3706e-01,\n",
       "                       -3.2899e-01, -1.8253e-01,  1.6168e-02, -2.1094e-01,  3.6948e-01,\n",
       "                        1.2790e-01, -1.5847e-01, -2.8002e-02, -4.7892e-02,  1.0686e-01,\n",
       "                       -4.7457e-02,  1.7473e-03, -4.6770e-01, -2.5855e-01, -9.2103e-02,\n",
       "                       -6.9150e-01,  4.4589e-01,  3.6832e-01, -9.8456e-02, -4.6901e-02,\n",
       "                        2.6941e-01, -2.8248e-01,  4.7212e-02,  5.3599e-02,  1.0812e+00,\n",
       "                        9.9823e-01,  8.3534e-01,  3.2476e-01, -2.0664e-01, -1.4553e-01,\n",
       "                       -7.8433e-02,  3.9429e-01,  1.1447e+00,  9.2456e-01,  2.2661e-01,\n",
       "                       -2.5426e-01,  4.7830e-02, -8.9389e-01, -1.9270e-01, -4.1571e-01,\n",
       "                        4.1881e-01,  3.1038e-01,  3.9700e-01, -4.6026e-01,  1.4156e-01,\n",
       "                        2.2431e-01,  2.0544e-05, -2.7809e-02, -3.0935e-02, -1.3373e-01,\n",
       "                       -6.1152e-02,  8.0023e-02,  7.2233e-02,  3.7315e-01, -1.9428e-01,\n",
       "                        3.7336e-01,  8.8724e-02,  9.6853e-05, -2.4059e-01,  2.1569e-01,\n",
       "                        5.4830e-01,  1.6730e-01,  5.0354e-01,  6.2966e-01,  3.9875e-01,\n",
       "                        3.8916e-01,  3.5855e-01,  1.1135e-01,  2.9338e-01,  2.3035e-01,\n",
       "                        2.7952e-01, -2.1276e-01, -6.7923e-01, -5.1107e-01,  4.1151e-01,\n",
       "                       -6.3586e-01,  1.7626e-01,  3.1709e-01, -1.1398e-01,  4.1029e-01,\n",
       "                       -7.7600e-02, -3.2487e-01,  1.2085e-01,  9.0592e-01,  4.3095e-01,\n",
       "                       -2.1663e-01, -2.5042e-01, -2.0914e-02, -7.9002e-01,  1.3209e-01,\n",
       "                       -7.3649e-02, -5.8195e-02,  5.6597e-01,  1.9923e-03, -2.5577e-01,\n",
       "                        1.2270e+00,  2.6794e-01,  2.1286e+00,  9.3342e-02, -1.1874e+00,\n",
       "                        2.8608e-01,  1.0100e+00,  1.0210e-01,  1.8925e-01,  2.9192e-01,\n",
       "                        7.3082e-02, -1.6647e-01,  2.8845e-02, -3.0665e-01, -6.8108e-02,\n",
       "                       -1.3288e-02, -3.7740e-01,  3.4568e-01, -1.3934e-01, -2.4897e-01,\n",
       "                       -3.6948e-01, -2.3751e-01,  2.3389e-01,  3.7864e-01, -4.1875e-02,\n",
       "                        1.3722e-02,  3.1507e-02, -2.5186e-01, -1.1371e-01, -7.4897e-02,\n",
       "                       -1.3185e-01, -2.9312e-02, -7.6365e-01,  2.0530e-01, -1.2005e+00,\n",
       "                       -9.1774e-02,  4.1104e-01,  1.7228e-01,  5.0007e-02,  1.3516e-01,\n",
       "                        6.1174e-01, -2.7600e-04,  2.7334e-01,  2.6654e-02,  2.3822e-01,\n",
       "                       -3.0685e-01, -2.0060e-01,  2.6520e-01,  2.6385e-01,  3.4034e-01,\n",
       "                       -1.3540e-01,  9.4795e-02,  1.9603e-02,  4.2915e-01, -4.4190e-01,\n",
       "                       -4.7333e-01, -2.2292e-01,  9.5201e-01, -2.0447e-01, -2.8904e-01,\n",
       "                       -4.3898e-01, -3.7652e-01,  1.3530e-01, -3.2048e-01, -7.1973e-02,\n",
       "                        1.7669e-01,  1.6691e-01,  1.6279e-01,  3.3773e-01, -4.7382e-01,\n",
       "                       -1.1084e-02,  9.1937e-02, -1.9565e-01,  1.6057e-01, -3.5854e-01,\n",
       "                       -2.8933e-01,  8.4271e-01, -7.8193e-03, -1.4041e-01, -1.9462e-01,\n",
       "                        8.4507e-02, -2.1179e-02, -2.2189e-01, -3.7546e-01, -2.5706e-01,\n",
       "                       -4.2038e-01,  6.5374e-01, -2.1708e-01, -9.6932e-02, -3.5626e-01,\n",
       "                       -3.9712e-02, -3.7209e-01, -4.0402e-01, -1.0423e-01, -2.7726e-01,\n",
       "                       -6.3313e-02,  2.2142e-01, -2.3581e-02,  1.8650e-01, -5.7490e-02,\n",
       "                        3.2370e-01,  4.3272e-03, -1.9728e-01, -1.1004e-01, -2.1548e-01,\n",
       "                       -4.4619e-01, -3.3406e-01, -3.5962e-01,  4.9850e-01,  1.8405e-01,\n",
       "                       -3.5378e-01, -1.6234e-01,  3.8869e-01, -2.1133e-01,  1.8646e-01,\n",
       "                        2.0074e-01,  9.5634e-01, -5.0864e-03, -2.3863e-01, -1.2162e-01,\n",
       "                       -3.6123e-01,  3.1987e-01, -2.0192e-01, -1.6387e-01, -2.6114e-01,\n",
       "                       -1.9646e-01, -1.5101e-01,  1.4148e-01,  3.2650e-01, -1.4144e-01,\n",
       "                       -5.8146e-01,  3.6602e-01,  4.1376e-01,  8.4762e-02, -2.8121e-01,\n",
       "                       -1.5171e-02,  3.8879e-01, -2.3416e-02, -2.0862e-01,  1.4078e-01,\n",
       "                       -7.1135e-02, -2.9197e-02, -1.1384e-01, -2.7071e-01, -1.7764e-01,\n",
       "                       -1.7078e-01, -2.8358e-01, -2.5001e-01,  5.9167e-01,  2.5658e-01,\n",
       "                       -2.7908e-01, -2.9939e-01, -3.4284e-01,  1.2837e-01,  3.4674e-01,\n",
       "                        9.6569e-02,  7.6122e-01, -1.9069e-01,  1.4057e-01, -1.2956e-01,\n",
       "                        5.1467e-01,  2.7227e-01,  4.1077e-01, -4.7116e-01, -1.9173e-01,\n",
       "                       -4.1154e-01, -5.7577e-02, -2.7436e-01,  3.9076e-01, -2.3178e-01,\n",
       "                       -3.7806e-01, -4.2554e-01, -3.6011e-01,  5.9871e-02, -4.0778e-01,\n",
       "                       -1.1880e-01,  1.0033e-01])),\n",
       "              ('encoder.lstm.bias_hh_l0',\n",
       "               tensor([ 1.6192e-01, -1.0173e-01,  1.7607e-01, -6.3845e-02,  4.1094e-02,\n",
       "                        5.3954e-01, -3.5156e-02, -3.6812e-01, -1.0299e-01, -2.2685e-01,\n",
       "                        3.3280e-01, -2.4141e-01, -2.9932e-01, -6.1739e-01,  4.1298e-01,\n",
       "                       -1.6266e-01,  2.2480e-01,  9.6454e-01, -4.0633e-01, -4.0907e-01,\n",
       "                       -1.6913e-01,  1.3619e-02, -2.8365e-01,  3.5557e-01, -3.2284e-01,\n",
       "                       -4.0822e-01, -4.0520e-01,  4.8922e-01, -3.3449e-01, -4.1355e-01,\n",
       "                       -3.8460e-02, -6.4998e-01, -3.6469e-01, -4.3353e-01, -3.6186e-01,\n",
       "                       -1.9652e-01, -3.2682e-01,  9.2133e-01, -6.3788e-02, -3.4107e-01,\n",
       "                       -4.8700e-01,  2.8923e-02, -2.7358e-01, -2.7718e-01,  2.7085e-01,\n",
       "                       -4.7173e-01,  3.1395e-01, -8.8956e-04, -9.8564e-02, -3.3546e-02,\n",
       "                       -2.5580e-01, -1.4391e-01, -7.3957e-01, -3.4844e-01, -4.5551e-01,\n",
       "                       -6.0748e-01,  3.9800e-01, -2.3090e-01, -5.8514e-02,  1.4433e-01,\n",
       "                        7.4377e-02, -2.2137e-01, -4.5679e-01, -2.7068e-02,  5.9042e-01,\n",
       "                       -5.5538e-01, -3.0177e-01,  2.4567e-01,  4.1420e-01, -3.1932e-01,\n",
       "                        2.5045e-01, -3.6963e-01,  5.9586e-01, -2.9785e-01, -1.2049e-02,\n",
       "                       -7.8603e-02,  3.2105e-01, -4.9629e-01,  1.3940e-01, -2.3267e-01,\n",
       "                        5.1518e-02,  3.0919e+00,  3.4042e-01,  9.4269e-01,  4.2100e-01,\n",
       "                        5.9147e-01, -1.0465e-01,  4.2288e-01, -1.5666e-01, -1.4031e-01,\n",
       "                        1.2965e-01, -6.4088e-02, -2.9331e-02,  6.9016e-03, -7.1539e-01,\n",
       "                       -1.1288e-01,  2.0363e-01, -3.8948e-01, -1.2289e-01,  4.5566e-01,\n",
       "                       -4.3360e-01, -2.5151e-01, -2.5230e-01, -5.8409e-01, -3.1681e-01,\n",
       "                       -1.2046e-01,  4.8569e-01,  2.2228e-01, -2.1791e-01,  1.3214e-01,\n",
       "                        2.7254e-02,  2.5574e-01,  9.1119e-02,  6.8322e-01, -3.1461e-01,\n",
       "                       -9.7995e-02, -2.6109e-01, -5.2161e-01, -3.6704e-01, -5.5174e-01,\n",
       "                       -4.5672e-01,  1.2510e-01, -5.9376e-01, -6.1152e-02, -2.0948e-01,\n",
       "                       -2.1193e-01, -4.7064e-01,  9.2170e-02, -2.1753e-01, -3.4028e-01,\n",
       "                       -4.4344e-01, -2.5821e-01, -5.0870e-01, -7.8462e-02,  3.9635e-02,\n",
       "                       -1.1955e-01, -4.0093e-01,  3.1874e-01, -1.2243e-01,  1.9954e-02,\n",
       "                       -4.5267e-01, -2.0782e-01, -3.8298e-02, -7.3098e-02, -4.4956e-01,\n",
       "                       -4.2683e-01,  2.4358e-01, -1.6741e-01,  9.8017e-03, -4.3879e-02,\n",
       "                        1.0248e-01,  1.1209e-01,  1.1863e-01, -1.3190e-02, -2.3536e-02,\n",
       "                       -6.1193e-02, -2.2476e-01,  1.1317e-02, -2.4364e-01,  2.7718e-01,\n",
       "                       -1.7545e-01,  1.2773e-01, -4.0386e-01, -8.3687e-02, -4.2071e-02,\n",
       "                       -2.9057e-01, -3.6115e-01, -3.5993e-01, -2.0043e-01, -3.5487e-01,\n",
       "                        1.9751e-01,  6.5678e-02,  2.1896e-01,  5.7868e-02, -7.1502e-02,\n",
       "                       -2.1348e-01, -1.6819e-01, -4.1492e-01,  2.6627e-01,  7.2581e-02,\n",
       "                        1.8295e-01, -4.9528e-02, -9.1335e-03, -1.7923e-01, -1.6010e-01,\n",
       "                       -1.7829e-01, -4.9538e-02, -3.5848e-01, -5.0567e-01,  1.1222e-01,\n",
       "                       -7.5656e-02, -2.5165e-01, -4.9129e-01, -1.5151e-01,  2.0590e-01,\n",
       "                       -3.7323e-01, -3.0281e-01,  3.9872e-02, -4.3947e-02, -6.7074e-02,\n",
       "                       -2.8976e-01, -1.3274e-01, -1.2843e-01,  2.5768e-02, -4.7135e-02,\n",
       "                       -3.5619e-02, -3.6184e-01, -1.0629e-01, -3.2356e-01,  4.5887e-01,\n",
       "                       -2.8752e-01,  2.6141e-02, -3.3833e-01, -5.5884e-02, -7.0029e-02,\n",
       "                       -3.7779e-01, -1.8437e-01,  3.2053e-02, -1.6569e-01,  3.7152e-01,\n",
       "                       -4.6902e-01, -6.0996e-01,  8.9776e-02, -2.7489e-01, -2.1252e-01,\n",
       "                       -2.4399e-01,  4.2533e-02, -3.9542e-01,  2.8217e-01, -6.0072e-03,\n",
       "                       -2.2500e-01, -8.6238e-02, -1.8400e-01,  2.2065e-01, -2.8611e-01,\n",
       "                       -3.0947e-01,  1.0189e-01,  4.4694e-01, -2.6594e-01, -4.3706e-01,\n",
       "                       -3.2899e-01, -1.8253e-01,  1.6168e-02, -2.1094e-01,  3.6948e-01,\n",
       "                        1.2790e-01, -1.5847e-01, -2.8003e-02, -4.7892e-02,  1.0686e-01,\n",
       "                       -4.7457e-02,  1.7471e-03, -4.6770e-01, -2.5855e-01, -9.2103e-02,\n",
       "                       -6.9150e-01,  4.4589e-01,  3.6832e-01, -9.8456e-02, -4.6901e-02,\n",
       "                        2.6941e-01, -2.8248e-01,  4.7212e-02,  5.3599e-02,  1.0812e+00,\n",
       "                        9.9823e-01,  8.3534e-01,  3.2476e-01, -2.0664e-01, -1.4553e-01,\n",
       "                       -7.8433e-02,  3.9429e-01,  1.1447e+00,  9.2456e-01,  2.2661e-01,\n",
       "                       -2.5426e-01,  4.7830e-02, -8.9389e-01, -1.9270e-01, -4.1571e-01,\n",
       "                        4.1881e-01,  3.1038e-01,  3.9700e-01, -4.6026e-01,  1.4156e-01,\n",
       "                        2.2431e-01,  2.0610e-05, -2.7809e-02, -3.0935e-02, -1.3373e-01,\n",
       "                       -6.1153e-02,  8.0023e-02,  7.2233e-02,  3.7315e-01, -1.9428e-01,\n",
       "                        3.7336e-01,  8.8724e-02,  9.7070e-05, -2.4059e-01,  2.1569e-01,\n",
       "                        5.4830e-01,  1.6730e-01,  5.0354e-01,  6.2966e-01,  3.9875e-01,\n",
       "                        3.8916e-01,  3.5855e-01,  1.1135e-01,  2.9338e-01,  2.3035e-01,\n",
       "                        2.7952e-01, -2.1276e-01, -6.7923e-01, -5.1107e-01,  4.1151e-01,\n",
       "                       -6.3586e-01,  1.7626e-01,  3.1709e-01, -1.1398e-01,  4.1029e-01,\n",
       "                       -7.7600e-02, -3.2487e-01,  1.2085e-01,  9.0592e-01,  4.3095e-01,\n",
       "                       -2.1663e-01, -2.5042e-01, -2.0914e-02, -7.9002e-01,  1.3209e-01,\n",
       "                       -7.3649e-02, -5.8195e-02,  5.6597e-01,  1.9921e-03, -2.5578e-01,\n",
       "                        1.2270e+00,  2.6794e-01,  2.1286e+00,  9.3342e-02, -1.1874e+00,\n",
       "                        2.8608e-01,  1.0100e+00,  1.0210e-01,  1.8925e-01,  2.9192e-01,\n",
       "                        7.3082e-02, -1.6647e-01,  2.8845e-02, -3.0665e-01, -6.8108e-02,\n",
       "                       -1.3288e-02, -3.7740e-01,  3.4568e-01, -1.3934e-01, -2.4897e-01,\n",
       "                       -3.6948e-01, -2.3751e-01,  2.3389e-01,  3.7864e-01, -4.1875e-02,\n",
       "                        1.3722e-02,  3.1507e-02, -2.5186e-01, -1.1371e-01, -7.4897e-02,\n",
       "                       -1.3185e-01, -2.9312e-02, -7.6365e-01,  2.0530e-01, -1.2005e+00,\n",
       "                       -9.1774e-02,  4.1104e-01,  1.7228e-01,  5.0007e-02,  1.3516e-01,\n",
       "                        6.1174e-01, -2.7584e-04,  2.7334e-01,  2.6654e-02,  2.3822e-01,\n",
       "                       -3.0685e-01, -2.0060e-01,  2.6520e-01,  2.6385e-01,  3.4034e-01,\n",
       "                       -1.3540e-01,  9.4796e-02,  1.9603e-02,  4.2915e-01, -4.4190e-01,\n",
       "                       -4.7333e-01, -2.2292e-01,  9.5201e-01, -2.0447e-01, -2.8904e-01,\n",
       "                       -4.3898e-01, -3.7652e-01,  1.3530e-01, -3.2048e-01, -7.1973e-02,\n",
       "                        1.7669e-01,  1.6691e-01,  1.6279e-01,  3.3773e-01, -4.7382e-01,\n",
       "                       -1.1084e-02,  9.1937e-02, -1.9565e-01,  1.6057e-01, -3.5854e-01,\n",
       "                       -2.8933e-01,  8.4271e-01, -7.8195e-03, -1.4041e-01, -1.9462e-01,\n",
       "                        8.4507e-02, -2.1179e-02, -2.2189e-01, -3.7546e-01, -2.5706e-01,\n",
       "                       -4.2038e-01,  6.5374e-01, -2.1708e-01, -9.6932e-02, -3.5626e-01,\n",
       "                       -3.9712e-02, -3.7209e-01, -4.0402e-01, -1.0423e-01, -2.7726e-01,\n",
       "                       -6.3312e-02,  2.2142e-01, -2.3581e-02,  1.8650e-01, -5.7490e-02,\n",
       "                        3.2370e-01,  4.3272e-03, -1.9728e-01, -1.1004e-01, -2.1548e-01,\n",
       "                       -4.4619e-01, -3.3406e-01, -3.5962e-01,  4.9850e-01,  1.8405e-01,\n",
       "                       -3.5378e-01, -1.6234e-01,  3.8869e-01, -2.1133e-01,  1.8646e-01,\n",
       "                        2.0074e-01,  9.5634e-01, -5.0867e-03, -2.3863e-01, -1.2162e-01,\n",
       "                       -3.6123e-01,  3.1987e-01, -2.0192e-01, -1.6387e-01, -2.6114e-01,\n",
       "                       -1.9646e-01, -1.5101e-01,  1.4148e-01,  3.2650e-01, -1.4144e-01,\n",
       "                       -5.8146e-01,  3.6603e-01,  4.1376e-01,  8.4762e-02, -2.8122e-01,\n",
       "                       -1.5171e-02,  3.8879e-01, -2.3416e-02, -2.0862e-01,  1.4078e-01,\n",
       "                       -7.1135e-02, -2.9197e-02, -1.1384e-01, -2.7071e-01, -1.7764e-01,\n",
       "                       -1.7078e-01, -2.8358e-01, -2.5001e-01,  5.9167e-01,  2.5658e-01,\n",
       "                       -2.7908e-01, -2.9939e-01, -3.4284e-01,  1.2837e-01,  3.4674e-01,\n",
       "                        9.6569e-02,  7.6122e-01, -1.9069e-01,  1.4057e-01, -1.2956e-01,\n",
       "                        5.1467e-01,  2.7227e-01,  4.1077e-01, -4.7116e-01, -1.9173e-01,\n",
       "                       -4.1154e-01, -5.7577e-02, -2.7436e-01,  3.9076e-01, -2.3178e-01,\n",
       "                       -3.7806e-01, -4.2554e-01, -3.6011e-01,  5.9871e-02, -4.0778e-01,\n",
       "                       -1.1880e-01,  1.0033e-01])),\n",
       "              ('encoder.lstm.weight_ih_l0_reverse',\n",
       "               tensor([[ 0.2444,  0.1353, -0.4366,  ..., -0.1415, -0.0354,  0.0827],\n",
       "                       [-0.5795, -0.1175, -0.4819,  ..., -0.7699, -0.7644, -0.6355],\n",
       "                       [ 0.1436,  0.7373, -0.4134,  ..., -0.0619, -0.1395, -0.4540],\n",
       "                       ...,\n",
       "                       [-0.1250, -0.2688, -0.0263,  ...,  0.0965,  0.1792,  0.1064],\n",
       "                       [-0.0604, -0.1120,  0.1315,  ..., -0.2720, -0.2263,  0.0138],\n",
       "                       [ 0.4406, -0.1424,  0.1572,  ...,  0.0183,  0.2606,  0.0533]])),\n",
       "              ('encoder.lstm.weight_hh_l0_reverse',\n",
       "               tensor([[-0.1275, -0.1077,  0.0248,  ..., -0.1726, -0.0753,  0.2089],\n",
       "                       [ 0.0561,  0.1183,  0.3097,  ...,  0.2933,  0.3181,  0.0821],\n",
       "                       [-0.0829, -0.3369, -0.4304,  ...,  0.1505, -0.1839, -0.0338],\n",
       "                       ...,\n",
       "                       [-0.2475, -0.1546, -0.2142,  ...,  0.2294,  0.1595,  0.9609],\n",
       "                       [ 0.4759,  0.9816,  0.3867,  ...,  0.4658,  0.7783,  0.4631],\n",
       "                       [ 0.2939, -0.0140, -0.2389,  ...,  0.3702,  0.0504, -0.4989]])),\n",
       "              ('encoder.lstm.bias_ih_l0_reverse',\n",
       "               tensor([-5.4360e-01,  1.5883e-01, -2.5172e-01, -1.4205e-01, -7.8223e-01,\n",
       "                       -4.3967e-01, -4.8536e-01,  1.4581e-01, -2.4186e-01, -6.2160e-01,\n",
       "                       -4.8862e-01, -1.6827e-01, -4.5917e-01, -6.4417e-01,  3.3470e-02,\n",
       "                        3.8264e-01, -4.5224e-01, -8.0520e-01, -6.4309e-01,  1.9101e-01,\n",
       "                       -3.3183e-01,  2.8358e-01, -5.2711e-01, -5.7628e-01, -3.0313e-01,\n",
       "                       -8.7154e-01, -3.6952e-01, -6.5542e-01, -4.3224e-01, -5.5026e-01,\n",
       "                       -6.2745e-01,  5.5678e-01, -8.5321e-01, -4.3230e-01, -2.1979e-01,\n",
       "                       -7.0173e-01, -3.1960e-01, -6.6951e-01,  3.8908e-02, -5.6219e-01,\n",
       "                       -5.9783e-01,  4.3099e-01, -1.8809e-01, -9.5872e-01,  9.0939e-01,\n",
       "                       -6.5999e-01,  4.3797e-01, -3.3638e-01, -6.5099e-02,  1.2270e-01,\n",
       "                       -5.6718e-01, -3.4613e-01, -3.5484e-01, -4.5324e-01, -5.6223e-01,\n",
       "                        1.7060e-01, -2.9658e-01, -6.3229e-01, -7.4122e-01,  3.0509e-01,\n",
       "                       -5.2716e-01, -6.6433e-01, -3.4947e-01, -6.7009e-01, -4.7331e-01,\n",
       "                       -4.1859e-01, -4.1852e-01, -4.5415e-01, -4.6496e-01, -4.7128e-01,\n",
       "                       -5.1733e-01, -4.3585e-01,  4.2268e-01, -5.9215e-01, -1.1692e-01,\n",
       "                       -4.6716e-01, -1.0799e-01, -5.5094e-01,  1.2883e-01, -1.1669e+00,\n",
       "                        2.1870e-01, -3.9512e-01,  1.8832e-01,  3.9744e-01, -5.0116e-01,\n",
       "                       -5.7077e-01, -5.9531e-01, -3.2201e-01, -5.1203e-01, -8.2886e-01,\n",
       "                        4.5759e-01, -2.7586e-01, -3.0412e-01, -5.3146e-01, -3.4235e-01,\n",
       "                        8.2145e-01, -9.0365e-01, -5.9379e-01, -6.1537e-01, -2.7786e-01,\n",
       "                       -5.6000e-01,  2.1713e-01,  1.5806e-01, -6.8223e-01, -5.7362e-01,\n",
       "                        2.4543e-01, -7.3262e-01, -6.4127e-02, -2.8156e-01, -2.6278e-01,\n",
       "                       -4.7215e-01, -4.0375e-01,  7.6832e-01, -2.9233e-01, -5.2330e-01,\n",
       "                        7.0817e-02, -4.7775e-01, -3.1063e-01, -5.7277e-01,  4.7495e-02,\n",
       "                        7.3912e-01, -1.0458e-01,  9.7621e-01, -3.6526e-01, -5.2004e-01,\n",
       "                       -6.9401e-01, -3.0262e-01, -9.0328e-01,  1.0812e-01, -3.6749e-01,\n",
       "                       -3.1639e-01, -2.8959e-01,  1.6092e-01,  2.2628e-01, -2.0644e-01,\n",
       "                       -2.5052e-01,  3.1087e-02, -1.2088e-01,  8.3224e-02, -3.4647e-01,\n",
       "                        6.3488e-02, -1.5524e-02,  2.5754e-01, -6.1046e-01, -2.2889e-01,\n",
       "                        1.3367e-01,  7.8393e-03, -4.0944e-01,  3.4651e-02, -2.1379e-01,\n",
       "                       -1.1476e-01,  1.1313e-01, -3.2962e-01,  1.2362e-01, -1.2609e-01,\n",
       "                       -1.1903e-02,  3.1311e-01, -1.5994e-01, -2.1894e-01, -3.0614e-01,\n",
       "                        3.2893e-02,  2.2113e-01, -1.2037e-01, -1.2447e-01, -1.0490e-01,\n",
       "                        1.4839e-01, -5.4387e-01,  1.2893e-01, -2.3983e-01, -4.1642e-01,\n",
       "                        3.2267e-01,  1.7736e-02, -6.4082e-01,  1.4888e-01, -3.6427e-01,\n",
       "                        7.1730e-03, -3.3778e-01, -9.2891e-02,  1.5955e-01, -4.7565e-01,\n",
       "                       -2.1852e-01, -2.3353e-01,  3.9277e-01,  1.2473e-01, -1.3146e-02,\n",
       "                        4.2572e-04, -4.7051e-02, -5.0544e-01, -1.4966e-01, -2.8158e-01,\n",
       "                       -4.0500e-01, -3.0518e-01,  3.3833e-01,  5.0775e-01, -1.8821e-01,\n",
       "                       -1.1859e-01, -5.0798e-02, -4.8879e-02, -6.8801e-02,  1.1835e-01,\n",
       "                       -3.9981e-01, -7.4625e-02, -6.9442e-02, -9.8929e-02, -1.1495e-01,\n",
       "                       -1.1705e-01, -5.7393e-01, -1.9946e-01, -2.1654e-01, -1.8960e-01,\n",
       "                       -3.3035e-01, -7.6630e-01,  8.8457e-02, -5.7600e-01,  1.8755e-01,\n",
       "                       -1.1870e-01, -3.1286e-01,  1.9674e-01, -4.1695e-01, -2.9938e-01,\n",
       "                        4.7998e-02, -3.1165e-02, -1.4802e-01, -1.2831e-01,  1.2210e-01,\n",
       "                        2.9993e-01, -1.9638e-01, -1.8555e-01, -2.6323e-02, -3.8088e-01,\n",
       "                       -5.0503e-01,  1.1711e-01,  1.2457e-01, -2.7296e-01,  5.1575e-01,\n",
       "                       -1.9852e-01,  7.7211e-02, -3.3793e-01,  3.5427e-02, -1.0651e-01,\n",
       "                       -1.7796e-01, -4.1815e-01, -4.8508e-02,  3.8172e-01,  3.5139e-02,\n",
       "                       -1.0900e-01,  1.2299e-01, -2.4939e-01, -3.9761e-01, -1.9679e-01,\n",
       "                       -2.8363e-01, -1.9108e-01, -6.2466e-02, -5.8121e-02, -2.2955e-01,\n",
       "                       -2.6936e-01,  6.7488e-02, -2.6809e-01, -6.7528e-01, -8.5998e-02,\n",
       "                       -2.8528e-01, -3.9480e-01, -1.5557e-01,  2.5776e-01, -3.0313e-01,\n",
       "                        3.8695e-02, -4.0736e-01, -4.3454e-01,  3.2383e-02, -8.6008e-01,\n",
       "                       -6.6883e-01, -1.1023e+00,  2.8362e-02,  4.1993e-02,  1.4644e-02,\n",
       "                       -5.7776e-02, -3.2596e-01,  2.5010e-01, -1.7565e-01,  2.1137e-01,\n",
       "                        6.0242e-02, -2.3590e-01, -1.3644e-01, -3.1773e-01, -5.4749e-01,\n",
       "                       -1.0320e-01, -9.4536e-02, -7.7942e-02,  2.4858e-01, -4.3552e-01,\n",
       "                       -2.6152e-02,  2.0700e-02,  6.7793e-03,  3.0207e-01,  1.1622e-01,\n",
       "                        1.9239e-01,  6.9707e-02, -5.3417e-02, -1.6037e-01,  1.7557e-01,\n",
       "                        1.0844e+00,  1.1469e-01,  7.9982e-01,  3.6271e-02,  1.5876e-01,\n",
       "                        1.3117e-01, -1.2630e-01,  3.2526e-01,  3.0646e-01,  1.1131e-01,\n",
       "                        2.3086e-01,  2.8796e-01,  5.3679e-01,  3.2994e-01,  3.4953e-01,\n",
       "                        3.2540e-01,  2.3667e-01,  2.6663e-01,  5.8615e-02, -2.1434e-01,\n",
       "                       -9.7771e-01, -4.2499e-01,  1.3050e-02,  1.1696e-01, -1.1765e-01,\n",
       "                       -2.5115e-02, -4.1293e-02, -3.8229e-01, -1.4955e-01, -7.0558e-02,\n",
       "                       -4.4257e-01, -3.6815e-01,  2.1779e-01,  2.7012e-01,  7.9223e-01,\n",
       "                       -1.4951e-01, -2.5609e-02, -1.1590e-01, -3.4870e-01, -2.9711e-02,\n",
       "                        1.9344e-01,  8.7312e-01, -1.8744e-02, -1.6922e-01,  3.4509e-01,\n",
       "                       -1.8499e-01, -2.2456e-01, -5.5812e-02, -1.5374e-01,  2.4933e-01,\n",
       "                       -2.0638e-01,  7.1410e-01, -1.5504e-01,  3.2021e-01,  1.2034e-02,\n",
       "                        2.1736e-01,  2.3355e-02,  2.7430e-01, -1.6131e-01, -1.8857e-01,\n",
       "                        4.9897e-01,  4.8245e-01,  1.4971e-01, -6.6463e-03,  3.5272e-02,\n",
       "                       -5.8075e-01, -8.1958e-02, -1.6633e-01, -5.5801e-01,  3.0950e-01,\n",
       "                        5.8248e-02, -1.0253e+00,  5.2857e-02, -2.5236e-01,  1.9120e-01,\n",
       "                        2.8409e-01,  4.3406e-01, -2.3032e-01,  1.0112e+00,  1.4989e-01,\n",
       "                        3.5868e-01,  2.8970e-02, -1.5049e-01,  7.8559e-02, -3.0049e-01,\n",
       "                       -1.0169e+00, -6.1760e-01,  3.1780e-01, -5.4689e-01, -5.3476e-01,\n",
       "                       -2.2272e-01, -6.8125e-01, -6.5672e-01, -4.0185e-01, -5.7599e-01,\n",
       "                       -2.7813e-01, -9.4051e-01, -6.5057e-01,  1.6903e-01,  1.7088e-02,\n",
       "                       -3.4237e-01, -1.3899e-01, -4.3562e-01, -3.9324e-01, -2.6446e-01,\n",
       "                       -3.7741e-01, -7.5090e-01, -5.7119e-01, -2.5370e-01, -5.2029e-01,\n",
       "                       -5.2316e-01, -4.9681e-01, -3.2994e-01, -4.4032e-01, -4.5027e-01,\n",
       "                       -8.0633e-02, -6.1206e-01, -2.7426e-01, -1.3906e+00, -2.4362e-01,\n",
       "                       -4.9337e-01, -7.0374e-01, -1.4110e-01, -7.6222e-01, -3.2490e-01,\n",
       "                       -1.5073e-01, -2.6672e-01, -5.8160e-01,  4.9331e-01, -4.0374e-01,\n",
       "                       -4.8723e-02, -2.0588e-01, -3.8718e-01, -2.7280e-01, -8.6167e-01,\n",
       "                       -5.9498e-02, -2.3160e-01, -2.8290e-01, -4.9154e-01, -1.7182e-01,\n",
       "                       -2.7481e-01, -2.6830e-01, -5.7773e-01,  1.5651e-01, -2.3377e-01,\n",
       "                       -4.3172e-01, -4.8836e-01, -6.1872e-01,  1.2966e-01,  6.5079e-03,\n",
       "                       -8.1664e-01, -2.8784e-01, -5.1086e-01, -1.0077e+00, -4.8946e-01,\n",
       "                       -7.5889e-01, -4.7090e-02, -4.8472e-01, -6.3486e-02, -1.3394e-01,\n",
       "                       -2.1207e-01,  2.0276e-01, -4.7094e-03, -4.3732e-01, -4.0615e-02,\n",
       "                       -3.5336e-01, -7.0398e-02,  7.9345e-02, -6.3814e-01,  6.7501e-01,\n",
       "                       -5.3011e-01, -3.8072e-01, -5.7122e-01, -8.9294e-01,  1.7455e-01,\n",
       "                       -2.2197e-01, -4.2206e-01, -4.1712e-01, -5.0503e-01,  6.6605e-01,\n",
       "                       -4.8533e-01, -3.3802e-01, -8.9835e-01, -2.5651e-01, -4.7325e-01,\n",
       "                       -7.5361e-01, -5.1646e-01, -5.3396e-01, -1.7479e-01, -4.2775e-01,\n",
       "                       -4.9362e-01, -3.8437e-01,  2.0067e-01, -7.5352e-01, -5.7498e-02,\n",
       "                       -3.8417e-01, -4.0961e-02, -3.6520e-01, -5.2424e-01,  7.1921e-01,\n",
       "                       -3.0902e-01, -4.9064e-01, -6.0930e-01, -3.8710e-01, -4.3414e-01,\n",
       "                       -2.5418e-01, -4.5804e-01, -8.0867e-01, -4.3687e-01, -3.3390e-01,\n",
       "                       -6.3518e-01, -3.5664e-01])),\n",
       "              ('encoder.lstm.bias_hh_l0_reverse',\n",
       "               tensor([-5.4360e-01,  1.5883e-01, -2.5172e-01, -1.4205e-01, -7.8223e-01,\n",
       "                       -4.3967e-01, -4.8536e-01,  1.4581e-01, -2.4186e-01, -6.2160e-01,\n",
       "                       -4.8862e-01, -1.6827e-01, -4.5917e-01, -6.4417e-01,  3.3469e-02,\n",
       "                        3.8264e-01, -4.5224e-01, -8.0520e-01, -6.4309e-01,  1.9101e-01,\n",
       "                       -3.3183e-01,  2.8358e-01, -5.2711e-01, -5.7628e-01, -3.0313e-01,\n",
       "                       -8.7154e-01, -3.6952e-01, -6.5542e-01, -4.3224e-01, -5.5026e-01,\n",
       "                       -6.2745e-01,  5.5678e-01, -8.5321e-01, -4.3230e-01, -2.1979e-01,\n",
       "                       -7.0173e-01, -3.1960e-01, -6.6951e-01,  3.8908e-02, -5.6219e-01,\n",
       "                       -5.9783e-01,  4.3099e-01, -1.8809e-01, -9.5872e-01,  9.0939e-01,\n",
       "                       -6.5999e-01,  4.3796e-01, -3.3638e-01, -6.5099e-02,  1.2270e-01,\n",
       "                       -5.6718e-01, -3.4613e-01, -3.5484e-01, -4.5324e-01, -5.6223e-01,\n",
       "                        1.7060e-01, -2.9658e-01, -6.3229e-01, -7.4122e-01,  3.0509e-01,\n",
       "                       -5.2716e-01, -6.6433e-01, -3.4947e-01, -6.7009e-01, -4.7331e-01,\n",
       "                       -4.1859e-01, -4.1852e-01, -4.5415e-01, -4.6496e-01, -4.7128e-01,\n",
       "                       -5.1733e-01, -4.3585e-01,  4.2268e-01, -5.9215e-01, -1.1692e-01,\n",
       "                       -4.6716e-01, -1.0799e-01, -5.5094e-01,  1.2883e-01, -1.1669e+00,\n",
       "                        2.1870e-01, -3.9512e-01,  1.8832e-01,  3.9744e-01, -5.0116e-01,\n",
       "                       -5.7078e-01, -5.9531e-01, -3.2201e-01, -5.1203e-01, -8.2886e-01,\n",
       "                        4.5759e-01, -2.7586e-01, -3.0412e-01, -5.3146e-01, -3.4235e-01,\n",
       "                        8.2145e-01, -9.0365e-01, -5.9379e-01, -6.1537e-01, -2.7786e-01,\n",
       "                       -5.6000e-01,  2.1713e-01,  1.5806e-01, -6.8223e-01, -5.7362e-01,\n",
       "                        2.4543e-01, -7.3262e-01, -6.4127e-02, -2.8156e-01, -2.6278e-01,\n",
       "                       -4.7215e-01, -4.0375e-01,  7.6832e-01, -2.9233e-01, -5.2330e-01,\n",
       "                        7.0817e-02, -4.7775e-01, -3.1063e-01, -5.7277e-01,  4.7495e-02,\n",
       "                        7.3912e-01, -1.0458e-01,  9.7621e-01, -3.6526e-01, -5.2004e-01,\n",
       "                       -6.9401e-01, -3.0262e-01, -9.0328e-01,  1.0812e-01, -3.6749e-01,\n",
       "                       -3.1639e-01, -2.8959e-01,  1.6092e-01,  2.2627e-01, -2.0644e-01,\n",
       "                       -2.5052e-01,  3.1087e-02, -1.2088e-01,  8.3224e-02, -3.4647e-01,\n",
       "                        6.3488e-02, -1.5524e-02,  2.5754e-01, -6.1046e-01, -2.2889e-01,\n",
       "                        1.3367e-01,  7.8392e-03, -4.0944e-01,  3.4650e-02, -2.1379e-01,\n",
       "                       -1.1476e-01,  1.1313e-01, -3.2962e-01,  1.2362e-01, -1.2609e-01,\n",
       "                       -1.1903e-02,  3.1311e-01, -1.5994e-01, -2.1894e-01, -3.0614e-01,\n",
       "                        3.2893e-02,  2.2113e-01, -1.2037e-01, -1.2447e-01, -1.0490e-01,\n",
       "                        1.4839e-01, -5.4387e-01,  1.2893e-01, -2.3983e-01, -4.1642e-01,\n",
       "                        3.2267e-01,  1.7736e-02, -6.4082e-01,  1.4888e-01, -3.6427e-01,\n",
       "                        7.1728e-03, -3.3778e-01, -9.2891e-02,  1.5955e-01, -4.7565e-01,\n",
       "                       -2.1852e-01, -2.3353e-01,  3.9277e-01,  1.2473e-01, -1.3146e-02,\n",
       "                        4.2568e-04, -4.7051e-02, -5.0544e-01, -1.4966e-01, -2.8158e-01,\n",
       "                       -4.0500e-01, -3.0518e-01,  3.3833e-01,  5.0775e-01, -1.8821e-01,\n",
       "                       -1.1859e-01, -5.0797e-02, -4.8879e-02, -6.8801e-02,  1.1835e-01,\n",
       "                       -3.9981e-01, -7.4625e-02, -6.9442e-02, -9.8929e-02, -1.1495e-01,\n",
       "                       -1.1705e-01, -5.7393e-01, -1.9946e-01, -2.1654e-01, -1.8960e-01,\n",
       "                       -3.3035e-01, -7.6630e-01,  8.8457e-02, -5.7600e-01,  1.8755e-01,\n",
       "                       -1.1870e-01, -3.1286e-01,  1.9674e-01, -4.1695e-01, -2.9938e-01,\n",
       "                        4.7998e-02, -3.1165e-02, -1.4802e-01, -1.2831e-01,  1.2210e-01,\n",
       "                        2.9993e-01, -1.9638e-01, -1.8555e-01, -2.6323e-02, -3.8088e-01,\n",
       "                       -5.0503e-01,  1.1711e-01,  1.2457e-01, -2.7296e-01,  5.1575e-01,\n",
       "                       -1.9852e-01,  7.7211e-02, -3.3793e-01,  3.5426e-02, -1.0651e-01,\n",
       "                       -1.7796e-01, -4.1815e-01, -4.8508e-02,  3.8172e-01,  3.5139e-02,\n",
       "                       -1.0900e-01,  1.2299e-01, -2.4939e-01, -3.9761e-01, -1.9679e-01,\n",
       "                       -2.8363e-01, -1.9108e-01, -6.2466e-02, -5.8121e-02, -2.2955e-01,\n",
       "                       -2.6936e-01,  6.7488e-02, -2.6808e-01, -6.7528e-01, -8.5998e-02,\n",
       "                       -2.8528e-01, -3.9480e-01, -1.5557e-01,  2.5776e-01, -3.0313e-01,\n",
       "                        3.8695e-02, -4.0736e-01, -4.3454e-01,  3.2383e-02, -8.6008e-01,\n",
       "                       -6.6883e-01, -1.1023e+00,  2.8362e-02,  4.1993e-02,  1.4644e-02,\n",
       "                       -5.7776e-02, -3.2596e-01,  2.5010e-01, -1.7565e-01,  2.1137e-01,\n",
       "                        6.0242e-02, -2.3590e-01, -1.3644e-01, -3.1773e-01, -5.4749e-01,\n",
       "                       -1.0320e-01, -9.4536e-02, -7.7942e-02,  2.4858e-01, -4.3552e-01,\n",
       "                       -2.6152e-02,  2.0700e-02,  6.7794e-03,  3.0207e-01,  1.1622e-01,\n",
       "                        1.9239e-01,  6.9707e-02, -5.3416e-02, -1.6037e-01,  1.7557e-01,\n",
       "                        1.0844e+00,  1.1469e-01,  7.9982e-01,  3.6271e-02,  1.5876e-01,\n",
       "                        1.3117e-01, -1.2630e-01,  3.2526e-01,  3.0646e-01,  1.1131e-01,\n",
       "                        2.3086e-01,  2.8796e-01,  5.3679e-01,  3.2994e-01,  3.4954e-01,\n",
       "                        3.2540e-01,  2.3667e-01,  2.6663e-01,  5.8615e-02, -2.1434e-01,\n",
       "                       -9.7771e-01, -4.2499e-01,  1.3050e-02,  1.1696e-01, -1.1765e-01,\n",
       "                       -2.5115e-02, -4.1293e-02, -3.8229e-01, -1.4955e-01, -7.0558e-02,\n",
       "                       -4.4257e-01, -3.6815e-01,  2.1779e-01,  2.7012e-01,  7.9224e-01,\n",
       "                       -1.4951e-01, -2.5609e-02, -1.1590e-01, -3.4870e-01, -2.9711e-02,\n",
       "                        1.9344e-01,  8.7313e-01, -1.8744e-02, -1.6922e-01,  3.4509e-01,\n",
       "                       -1.8499e-01, -2.2456e-01, -5.5812e-02, -1.5374e-01,  2.4933e-01,\n",
       "                       -2.0638e-01,  7.1410e-01, -1.5504e-01,  3.2021e-01,  1.2034e-02,\n",
       "                        2.1736e-01,  2.3355e-02,  2.7430e-01, -1.6131e-01, -1.8857e-01,\n",
       "                        4.9897e-01,  4.8245e-01,  1.4971e-01, -6.6461e-03,  3.5272e-02,\n",
       "                       -5.8075e-01, -8.1958e-02, -1.6633e-01, -5.5801e-01,  3.0950e-01,\n",
       "                        5.8248e-02, -1.0253e+00,  5.2857e-02, -2.5236e-01,  1.9120e-01,\n",
       "                        2.8409e-01,  4.3406e-01, -2.3032e-01,  1.0112e+00,  1.4989e-01,\n",
       "                        3.5868e-01,  2.8970e-02, -1.5049e-01,  7.8559e-02, -3.0049e-01,\n",
       "                       -1.0169e+00, -6.1760e-01,  3.1780e-01, -5.4689e-01, -5.3476e-01,\n",
       "                       -2.2272e-01, -6.8125e-01, -6.5672e-01, -4.0185e-01, -5.7599e-01,\n",
       "                       -2.7813e-01, -9.4051e-01, -6.5057e-01,  1.6903e-01,  1.7089e-02,\n",
       "                       -3.4237e-01, -1.3899e-01, -4.3562e-01, -3.9324e-01, -2.6446e-01,\n",
       "                       -3.7741e-01, -7.5090e-01, -5.7119e-01, -2.5370e-01, -5.2029e-01,\n",
       "                       -5.2316e-01, -4.9681e-01, -3.2994e-01, -4.4032e-01, -4.5027e-01,\n",
       "                       -8.0634e-02, -6.1206e-01, -2.7426e-01, -1.3906e+00, -2.4362e-01,\n",
       "                       -4.9337e-01, -7.0374e-01, -1.4110e-01, -7.6222e-01, -3.2490e-01,\n",
       "                       -1.5073e-01, -2.6672e-01, -5.8160e-01,  4.9331e-01, -4.0374e-01,\n",
       "                       -4.8722e-02, -2.0588e-01, -3.8718e-01, -2.7280e-01, -8.6167e-01,\n",
       "                       -5.9498e-02, -2.3160e-01, -2.8290e-01, -4.9154e-01, -1.7182e-01,\n",
       "                       -2.7481e-01, -2.6830e-01, -5.7772e-01,  1.5651e-01, -2.3377e-01,\n",
       "                       -4.3172e-01, -4.8836e-01, -6.1872e-01,  1.2966e-01,  6.5077e-03,\n",
       "                       -8.1664e-01, -2.8784e-01, -5.1086e-01, -1.0077e+00, -4.8946e-01,\n",
       "                       -7.5889e-01, -4.7090e-02, -4.8472e-01, -6.3485e-02, -1.3394e-01,\n",
       "                       -2.1207e-01,  2.0276e-01, -4.7092e-03, -4.3732e-01, -4.0615e-02,\n",
       "                       -3.5336e-01, -7.0398e-02,  7.9345e-02, -6.3814e-01,  6.7501e-01,\n",
       "                       -5.3011e-01, -3.8072e-01, -5.7122e-01, -8.9294e-01,  1.7455e-01,\n",
       "                       -2.2197e-01, -4.2206e-01, -4.1712e-01, -5.0503e-01,  6.6605e-01,\n",
       "                       -4.8533e-01, -3.3802e-01, -8.9835e-01, -2.5651e-01, -4.7325e-01,\n",
       "                       -7.5361e-01, -5.1646e-01, -5.3396e-01, -1.7479e-01, -4.2775e-01,\n",
       "                       -4.9362e-01, -3.8437e-01,  2.0067e-01, -7.5352e-01, -5.7498e-02,\n",
       "                       -3.8417e-01, -4.0961e-02, -3.6520e-01, -5.2424e-01,  7.1921e-01,\n",
       "                       -3.0902e-01, -4.9064e-01, -6.0930e-01, -3.8710e-01, -4.3414e-01,\n",
       "                       -2.5418e-01, -4.5804e-01, -8.0867e-01, -4.3687e-01, -3.3390e-01,\n",
       "                       -6.3518e-01, -3.5664e-01])),\n",
       "              ('decoder.character_embedding.weight',\n",
       "               tensor([[-5.4694e-03, -1.3337e-03, -4.6120e-03,  ...,  1.1155e-02,\n",
       "                         3.4798e-03, -2.0573e-02],\n",
       "                       [ 1.6085e-01, -4.6857e-01, -8.5496e-02,  ...,  1.3754e-01,\n",
       "                        -5.1390e-01,  4.2390e-01],\n",
       "                       [ 3.3428e-04, -7.7502e-03, -1.6381e-02,  ..., -3.8834e-02,\n",
       "                         2.0294e-03,  3.2716e-02],\n",
       "                       ...,\n",
       "                       [-3.8310e-02,  1.1428e-01, -7.8152e-02,  ..., -9.2544e-03,\n",
       "                        -6.4878e-02, -9.1421e-02],\n",
       "                       [ 1.0301e-01, -6.1431e-02, -1.2974e-01,  ...,  1.7364e-01,\n",
       "                         8.6154e-02, -3.3156e-02],\n",
       "                       [ 1.7100e-01,  7.9977e-02,  4.4606e-02,  ...,  1.4031e-02,\n",
       "                         3.8652e-01,  1.5770e-01]])),\n",
       "              ('decoder.lstm.weight_ih_l0',\n",
       "               tensor([[-0.1989,  0.1888,  0.7055,  ...,  0.2709, -0.2599,  0.6561],\n",
       "                       [ 0.0092, -0.1742,  0.1550,  ...,  0.4105, -0.3846, -1.0680],\n",
       "                       [ 0.0320, -0.0683,  0.1082,  ..., -0.8236, -0.7017,  0.1880],\n",
       "                       ...,\n",
       "                       [-0.0082, -0.0370, -0.1774,  ..., -0.7178,  0.5427,  0.5309],\n",
       "                       [-0.8034,  0.2744,  0.1576,  ...,  0.5431, -0.3729,  0.0192],\n",
       "                       [ 0.9733, -0.1816,  0.3023,  ..., -0.1099, -0.2474,  0.7924]])),\n",
       "              ('decoder.lstm.weight_hh_l0',\n",
       "               tensor([[ 0.3651, -0.0484,  0.0500,  ...,  0.4381,  0.0710,  0.1669],\n",
       "                       [ 0.3736, -0.3140,  0.2038,  ..., -0.2132,  0.6709,  0.0597],\n",
       "                       [-0.0903, -0.5376,  0.3834,  ..., -0.3663, -0.4606, -0.3777],\n",
       "                       ...,\n",
       "                       [ 0.5486, -0.0464,  0.2658,  ...,  0.4472, -0.3652, -0.0736],\n",
       "                       [ 0.4544, -0.4225,  0.0834,  ..., -0.6808, -0.7878, -0.1273],\n",
       "                       [ 0.0720, -0.2547, -0.3122,  ..., -0.0626,  0.0326, -0.0250]])),\n",
       "              ('decoder.lstm.bias_ih_l0',\n",
       "               tensor([ 0.0035, -0.0246, -0.0542,  ..., -0.1944, -0.0588, -0.0575])),\n",
       "              ('decoder.lstm.bias_hh_l0',\n",
       "               tensor([ 0.0035, -0.0246, -0.0542,  ..., -0.1944, -0.0588, -0.0575])),\n",
       "              ('decoder.attn.attn.weight',\n",
       "               tensor([[-0.2569,  0.2635, -0.5006,  ...,  0.2578,  0.1452,  0.0291],\n",
       "                       [-0.3724,  0.1319, -0.3754,  ..., -0.2285, -0.0878,  0.2432],\n",
       "                       [-0.0859,  0.1746,  0.2900,  ...,  0.0482, -0.0008, -0.0710],\n",
       "                       ...,\n",
       "                       [ 0.1149,  0.0267, -0.3410,  ...,  0.2059,  0.0719,  0.1089],\n",
       "                       [-0.4598,  0.2450,  0.2927,  ...,  0.1649,  0.1153, -0.2093],\n",
       "                       [-0.0090, -0.0081, -0.1810,  ..., -0.2263,  0.1825,  0.1365]])),\n",
       "              ('decoder.attn.attn.bias',\n",
       "               tensor([-6.4039e-02,  1.4209e-02,  1.9641e-01,  1.9644e-02, -2.5171e-02,\n",
       "                        3.8065e-03,  1.2017e-02, -1.3029e-01, -9.0110e-03, -4.8285e-02,\n",
       "                       -1.1667e-01,  5.7934e-02,  2.1675e-02,  8.8690e-03, -2.9790e-02,\n",
       "                        7.6649e-02, -1.0166e-02, -3.7740e-02, -1.5274e-02, -4.0548e-03,\n",
       "                        6.5783e-02,  7.5610e-02, -7.4615e-02, -9.4169e-03, -2.7856e-02,\n",
       "                        8.8781e-02, -3.3526e-02,  1.9052e-02,  1.3114e-01, -6.6238e-02,\n",
       "                       -5.5654e-03,  6.2285e-02, -2.8484e-02,  2.1244e-02,  3.6332e-02,\n",
       "                       -5.9497e-03,  1.3439e-02, -5.4774e-02, -5.6291e-02, -1.2115e-02,\n",
       "                       -6.7415e-03, -2.0280e-02, -2.7855e-02,  8.4528e-02,  1.4460e-01,\n",
       "                       -4.1478e-03,  2.1322e-02, -8.6492e-03, -5.4214e-02,  7.6502e-03,\n",
       "                        1.9367e-02,  4.5687e-02,  2.7801e-02,  3.6551e-02,  3.5968e-02,\n",
       "                        1.6630e-02, -5.0415e-02, -6.1156e-02, -2.4244e-02, -4.4888e-02,\n",
       "                        2.0562e-02,  2.7185e-02,  2.4693e-02, -1.5433e-02, -2.0218e-02,\n",
       "                       -5.3315e-02,  2.9714e-02, -3.2277e-03, -1.2230e-03, -8.6138e-02,\n",
       "                       -3.0305e-03,  1.2574e-02,  8.2449e-02, -1.2593e-01,  2.5798e-02,\n",
       "                       -2.5411e-02, -5.7638e-02,  2.6877e-02,  1.0588e-01,  1.1359e-01,\n",
       "                        6.6995e-02, -4.2703e-02,  7.8722e-02,  6.7626e-03, -1.8069e-02,\n",
       "                        6.0028e-02,  2.0944e-02,  1.1326e-02,  1.2136e-01,  1.8268e-02,\n",
       "                        2.3107e-02,  5.2818e-02, -5.1023e-02, -4.0409e-02,  1.1280e-01,\n",
       "                        5.4297e-03, -1.0251e-01,  5.6536e-02, -1.0758e-02,  1.2192e-01,\n",
       "                       -2.2816e-02, -1.2896e-02, -6.5671e-02, -2.9260e-02,  6.8711e-02,\n",
       "                       -3.0962e-02,  1.0427e-01,  1.1016e-02,  2.4626e-02,  2.8772e-02,\n",
       "                        1.1807e-03,  6.4952e-02, -6.3679e-02, -3.7714e-02, -6.3999e-03,\n",
       "                       -1.9058e-01,  1.5753e-02,  1.9015e-02, -2.3688e-02, -3.6049e-02,\n",
       "                        1.1579e-02, -8.2888e-03,  2.3359e-02,  5.8866e-02,  1.5258e-01,\n",
       "                        7.5770e-02,  3.3950e-02, -2.2027e-02,  5.3463e-03,  1.3457e-02,\n",
       "                        6.7751e-02, -1.8964e-02,  3.1011e-02,  9.2301e-02,  3.7827e-03,\n",
       "                       -4.7071e-03, -6.1962e-02, -1.4040e-02,  1.1529e-02, -9.2279e-03,\n",
       "                       -3.0390e-02,  1.0178e-02, -1.9776e-01,  3.0243e-03, -7.3344e-03,\n",
       "                        8.4366e-02, -2.1561e-02,  3.9437e-03, -1.4121e-02,  2.2720e-02,\n",
       "                       -3.8366e-02,  2.7672e-02, -2.1913e-01, -1.3234e-01, -9.3325e-03,\n",
       "                        5.9515e-03, -1.1828e-01, -8.6194e-03, -9.4033e-02, -1.2549e-01,\n",
       "                        1.7048e-03, -6.3210e-02, -7.3010e-02, -2.7260e-02, -6.0765e-03,\n",
       "                       -7.2648e-02,  1.6946e-02, -1.3356e-03, -3.8790e-02,  3.5556e-03,\n",
       "                        2.6858e-02, -2.6974e-02, -5.1652e-03,  2.7446e-02, -1.1302e-02,\n",
       "                       -8.8132e-03, -2.1113e-02,  7.8881e-03, -5.4172e-02,  5.6816e-02,\n",
       "                        2.5414e-03,  8.3208e-05,  1.8852e-04, -2.0439e-03, -9.3062e-03,\n",
       "                       -6.4536e-02,  2.6675e-02,  2.7284e-02,  4.4568e-02,  2.9239e-02,\n",
       "                        8.2549e-02,  7.6031e-03,  5.8528e-02, -5.4581e-02,  6.6030e-03,\n",
       "                        5.1099e-02,  4.0529e-02, -2.3570e-02,  1.4820e-03, -1.2032e-02,\n",
       "                       -1.0844e-01, -7.6624e-03, -5.9801e-02, -1.0208e-01, -3.5841e-02,\n",
       "                       -9.8221e-02,  1.1183e-02, -4.8635e-02,  4.9190e-03,  9.2766e-03,\n",
       "                        4.1650e-02,  3.0459e-03,  1.5403e-02, -4.8596e-02, -6.3540e-02,\n",
       "                       -2.6922e-02,  8.6349e-03, -6.5248e-02,  1.9343e-01, -2.1374e-02,\n",
       "                       -3.5450e-02, -1.4262e-01, -2.7651e-02, -8.1638e-03,  9.8255e-02,\n",
       "                       -7.0465e-02, -2.3052e-01, -3.7345e-02,  1.3866e-02,  1.0782e-01,\n",
       "                       -1.2189e-02, -2.3303e-02, -5.6429e-02,  3.9041e-02, -1.3958e-02,\n",
       "                       -3.8975e-03,  7.4455e-02,  3.3854e-02, -3.3549e-03,  3.5675e-03,\n",
       "                        1.1646e-02, -9.1934e-02,  3.9768e-02,  2.6499e-02,  6.2346e-02,\n",
       "                       -6.4795e-02,  3.9328e-02, -2.1836e-02,  7.6645e-02, -2.6717e-02,\n",
       "                       -3.4851e-02, -8.0681e-02, -4.5602e-03, -4.2009e-02, -1.5177e-02,\n",
       "                        4.7646e-02])),\n",
       "              ('decoder.linear.weight',\n",
       "               tensor([[ 0.0402, -0.0479, -0.0179,  ...,  0.3256, -0.0292, -0.5076],\n",
       "                       [ 0.0180, -0.0736, -0.0193,  ...,  0.3536, -0.0265, -0.5038],\n",
       "                       [-0.0517,  0.0036,  0.0897,  ..., -0.0528,  0.0174,  0.1593],\n",
       "                       ...,\n",
       "                       [ 0.6535, -0.3836,  0.0452,  ..., -0.1194,  0.6511, -0.3253],\n",
       "                       [ 0.8480, -0.5193, -0.0250,  ...,  0.4792,  0.9259, -0.7316],\n",
       "                       [ 0.3990,  0.4289, -0.6489,  ...,  0.1238,  0.8445, -0.0843]])),\n",
       "              ('decoder.linear.bias',\n",
       "               tensor([-1.1547, -1.1595, -0.0155,  0.1017,  0.1565,  0.2744, -0.3843,  0.0170,\n",
       "                        0.0470,  0.0017, -0.1194,  0.0912,  0.0164, -0.2491,  0.2343, -0.0802,\n",
       "                        0.2138,  0.1706, -0.1898, -0.0048,  0.0216,  0.0472, -0.1796,  0.0535,\n",
       "                       -0.2476, -0.0339,  0.2341,  0.3603, -0.1774,  0.1238, -0.0049,  0.0986,\n",
       "                        0.0376,  0.1546,  0.2558,  0.2082,  0.3248,  0.1634,  0.1013]))]),\n",
       " 'optimizer_state_dict': {'state': {0: {'step': 112320,\n",
       "    'exp_avg': tensor([[ 3.2953e-05, -4.6512e-05,  1.9605e-05,  ...,  1.7377e-05,\n",
       "              3.2809e-05,  3.3470e-05],\n",
       "            [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "              0.0000e+00,  0.0000e+00],\n",
       "            ...,\n",
       "            [ 5.6613e-07,  1.3842e-06, -9.5746e-07,  ...,  9.1616e-07,\n",
       "              1.7168e-07, -1.1404e-22],\n",
       "            [ 2.3346e-42,  3.3631e-42, -1.0454e-42,  ..., -6.3815e-42,\n",
       "              5.6052e-45,  5.6052e-45],\n",
       "            [ 5.6052e-45,  5.6052e-45,  5.6052e-45,  ..., -5.6052e-45,\n",
       "              5.6052e-45,  5.6052e-45]]),\n",
       "    'exp_avg_sq': tensor([[2.4543e-08, 2.4171e-08, 5.8723e-08,  ..., 3.0257e-08, 2.0821e-08,\n",
       "             1.3919e-08],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            [0.0000e+00, 0.0000e+00, 0.0000e+00,  ..., 0.0000e+00, 0.0000e+00,\n",
       "             0.0000e+00],\n",
       "            ...,\n",
       "            [7.2603e-11, 6.0233e-11, 1.4363e-11,  ..., 5.6947e-11, 1.2414e-10,\n",
       "             5.7355e-12],\n",
       "            [1.1297e-11, 1.2584e-13, 1.7972e-11,  ..., 1.2928e-11, 9.0495e-15,\n",
       "             2.8826e-12],\n",
       "            [1.0058e-10, 1.0772e-13, 2.9762e-11,  ..., 2.0456e-10, 7.1512e-11,\n",
       "             6.4800e-10]])},\n",
       "   1: {'step': 112320,\n",
       "    'exp_avg': tensor([[-1.9540e-06, -5.7016e-06, -8.9037e-06,  ...,  8.9592e-06,\n",
       "              4.2125e-06, -3.6037e-06],\n",
       "            [-4.2923e-06,  6.4350e-06, -1.7708e-06,  ...,  2.5993e-06,\n",
       "              1.3638e-06, -4.5460e-06],\n",
       "            [ 5.7912e-05, -1.4109e-05, -3.2867e-06,  ..., -1.1123e-05,\n",
       "              1.9076e-05,  1.7402e-05],\n",
       "            ...,\n",
       "            [-6.5314e-06,  5.7026e-06, -1.2146e-05,  ..., -1.0172e-05,\n",
       "             -1.7370e-05, -5.4426e-06],\n",
       "            [ 2.6166e-06, -1.6440e-06, -2.8160e-06,  ...,  5.7260e-06,\n",
       "              2.3525e-05, -4.1908e-06],\n",
       "            [ 1.9401e-05, -1.0335e-06,  4.3900e-06,  ..., -1.4860e-05,\n",
       "              6.6044e-06,  8.1710e-06]]),\n",
       "    'exp_avg_sq': tensor([[2.1549e-09, 3.7441e-09, 2.5812e-09,  ..., 4.6845e-09, 7.2248e-09,\n",
       "             1.4751e-09],\n",
       "            [1.9526e-09, 1.3170e-09, 1.2493e-09,  ..., 2.5716e-09, 1.0986e-09,\n",
       "             2.1749e-09],\n",
       "            [1.3274e-08, 4.5595e-09, 4.5158e-09,  ..., 1.1883e-08, 4.1002e-09,\n",
       "             6.4589e-09],\n",
       "            ...,\n",
       "            [2.3988e-09, 1.7485e-09, 2.4591e-09,  ..., 2.4699e-09, 1.9687e-09,\n",
       "             1.7440e-09],\n",
       "            [3.8668e-09, 2.3795e-09, 5.9977e-09,  ..., 4.1997e-09, 3.1218e-09,\n",
       "             2.4802e-09],\n",
       "            [3.9957e-09, 2.9421e-09, 3.4733e-09,  ..., 3.2405e-09, 4.6465e-09,\n",
       "             2.8018e-09]])},\n",
       "   2: {'step': 112320,\n",
       "    'exp_avg': tensor([[-3.6305e-06, -5.5063e-06, -1.1828e-06,  ..., -8.5910e-07,\n",
       "              3.9022e-07,  4.9021e-06],\n",
       "            [-1.0817e-05,  9.2799e-08, -6.8688e-06,  ..., -1.6148e-06,\n",
       "             -4.1622e-06,  2.1620e-06],\n",
       "            [-1.8752e-05, -5.9521e-06,  1.2398e-05,  ..., -1.4475e-06,\n",
       "              1.1693e-05, -2.5443e-06],\n",
       "            ...,\n",
       "            [-1.2630e-05, -1.1710e-06, -1.2178e-06,  ...,  9.7995e-06,\n",
       "             -1.9043e-06,  8.6412e-06],\n",
       "            [-1.7615e-05, -9.5371e-07,  5.1476e-06,  ...,  1.7337e-06,\n",
       "              1.9794e-06, -8.7354e-06],\n",
       "            [-1.3479e-05, -4.1674e-07,  1.6125e-06,  ...,  3.4624e-06,\n",
       "              7.2596e-06, -7.1178e-06]]),\n",
       "    'exp_avg_sq': tensor([[2.3767e-08, 7.2299e-09, 6.1604e-09,  ..., 9.7352e-10, 6.1160e-09,\n",
       "             1.6774e-09],\n",
       "            [7.3028e-09, 2.5541e-09, 9.5108e-09,  ..., 7.9053e-10, 2.2016e-09,\n",
       "             1.0213e-09],\n",
       "            [8.5296e-08, 3.7795e-09, 4.6787e-08,  ..., 1.4572e-09, 5.2717e-09,\n",
       "             1.4273e-09],\n",
       "            ...,\n",
       "            [1.1905e-08, 3.8162e-09, 6.4231e-09,  ..., 2.1756e-09, 3.7100e-09,\n",
       "             1.7708e-09],\n",
       "            [2.0329e-08, 3.5087e-09, 2.4787e-09,  ..., 5.6134e-09, 1.5448e-08,\n",
       "             1.8002e-09],\n",
       "            [2.4869e-08, 6.9581e-09, 7.8891e-09,  ..., 1.9456e-09, 3.3592e-09,\n",
       "             2.2358e-09]])},\n",
       "   3: {'step': 112320,\n",
       "    'exp_avg': tensor([-2.3516e-05, -1.1116e-05,  1.0755e-04,  8.4242e-05,  1.1718e-04,\n",
       "             6.8176e-06, -7.2220e-06, -2.6737e-05, -6.3128e-06, -1.3210e-05,\n",
       "             2.7185e-05,  1.5404e-05, -2.9801e-06, -3.8539e-05,  1.9611e-05,\n",
       "            -4.8453e-06,  4.2298e-05,  9.2238e-06,  4.6570e-05,  2.0068e-05,\n",
       "             1.0382e-05, -1.8487e-05,  7.5546e-06, -5.3233e-06, -1.4426e-05,\n",
       "            -2.9918e-05, -1.6663e-05,  2.1851e-05,  7.1377e-05,  6.9825e-06,\n",
       "            -1.4432e-05, -4.9779e-05,  1.4583e-05,  3.4808e-05,  1.1362e-05,\n",
       "            -1.0209e-05,  4.8738e-06, -2.9576e-05, -2.6068e-05,  1.9277e-06,\n",
       "            -2.4647e-06, -1.1668e-05, -4.2184e-06, -8.7205e-06,  6.5784e-06,\n",
       "            -2.4592e-05,  2.6296e-06, -3.0782e-05,  5.2863e-05, -2.4090e-05,\n",
       "             1.1586e-04, -1.1973e-05, -1.4111e-04, -2.0093e-05,  8.2411e-06,\n",
       "            -1.8898e-05, -3.6768e-05, -2.1267e-06, -2.3150e-05, -2.3591e-05,\n",
       "            -6.0952e-05, -1.8081e-05,  2.1426e-05, -4.3578e-06, -3.2092e-06,\n",
       "            -1.0140e-05,  2.2556e-05,  1.2264e-04, -1.4429e-05, -1.5503e-05,\n",
       "             5.5995e-06, -3.2255e-05, -1.0686e-05, -1.1962e-05, -7.2886e-06,\n",
       "             3.4674e-06,  2.1250e-05,  1.7397e-05, -3.5734e-07,  1.7849e-05,\n",
       "            -1.3648e-05,  5.2036e-08, -1.8524e-05,  1.1675e-05,  3.9128e-05,\n",
       "             1.9175e-05, -1.0285e-05, -1.1592e-05,  1.7174e-05,  2.7888e-05,\n",
       "             6.6552e-05,  3.7430e-06, -1.2114e-05,  2.3219e-05,  5.7640e-05,\n",
       "             8.3456e-07,  8.8841e-06,  4.3384e-05,  6.7362e-06, -7.7181e-06,\n",
       "             9.6931e-05, -8.4082e-06,  1.4362e-05, -1.9834e-05, -1.8869e-05,\n",
       "             4.1699e-05, -1.3442e-05,  9.0431e-05,  2.9924e-06,  1.3896e-05,\n",
       "             1.3350e-06, -1.7866e-04, -7.4549e-05,  9.4607e-06,  2.9743e-05,\n",
       "             2.7412e-05,  7.2604e-06,  1.8155e-04,  2.1476e-06,  1.3841e-05,\n",
       "             3.8181e-06, -7.4171e-06, -3.0940e-05,  1.3235e-05, -7.2708e-07,\n",
       "             3.3635e-06, -3.1985e-05, -8.0391e-05, -2.0032e-05, -5.2268e-06,\n",
       "            -3.0441e-05, -2.3095e-05, -3.5957e-06,  5.2450e-06,  3.1992e-06,\n",
       "            -3.4643e-05, -3.2597e-05, -1.2792e-05,  2.0357e-06,  9.5523e-07,\n",
       "             1.1021e-05, -9.8818e-06, -1.1163e-06, -7.2246e-06,  2.2172e-05,\n",
       "             9.3412e-06, -1.2800e-06, -1.4201e-05,  7.0925e-07,  1.2020e-06,\n",
       "            -7.7535e-06, -1.7691e-05,  5.3028e-06,  7.0202e-06,  3.6384e-05,\n",
       "             3.4880e-06, -1.2805e-05, -3.7202e-05, -5.9494e-06, -2.6030e-05,\n",
       "            -1.3576e-05,  6.6220e-05, -4.9913e-06,  4.7233e-06,  4.8971e-06,\n",
       "             8.6906e-05, -2.7461e-05, -3.7355e-06, -1.1675e-05,  3.2942e-05,\n",
       "            -1.9738e-06, -8.5099e-06, -5.2191e-06, -1.4540e-06, -1.3816e-05,\n",
       "            -1.1288e-05,  1.4666e-06,  4.7953e-05,  3.9341e-05,  1.4147e-06,\n",
       "            -4.3302e-05,  2.1894e-05,  6.9398e-06,  1.0448e-05,  1.2970e-05,\n",
       "             2.7881e-05, -9.2849e-06,  4.6049e-05,  1.7379e-05, -8.8710e-06,\n",
       "            -3.4151e-06,  9.2222e-05, -2.2874e-05, -1.5231e-06, -2.9499e-06,\n",
       "            -5.0833e-06, -7.8870e-05, -4.0053e-06,  1.4548e-05,  3.4805e-06,\n",
       "             4.7228e-05, -1.7650e-05,  2.6204e-06,  1.3622e-06, -2.0490e-05,\n",
       "            -1.1119e-05, -1.8636e-05, -2.9618e-05,  9.5247e-06, -6.0037e-04,\n",
       "             1.4519e-05,  1.3006e-05, -3.4552e-05,  2.0175e-05,  4.2003e-06,\n",
       "            -1.0739e-05,  3.5974e-05, -4.9414e-06, -1.2615e-05,  7.4291e-07,\n",
       "            -8.1947e-06,  5.3340e-06,  9.3678e-06,  3.8690e-06,  1.1702e-05,\n",
       "             1.4180e-05,  3.2957e-06, -3.0469e-05, -1.4025e-05, -8.5719e-06,\n",
       "            -2.2981e-06, -1.9306e-05,  6.1321e-05, -9.7699e-06,  4.0260e-05,\n",
       "             1.0818e-05,  1.6527e-05, -8.9706e-06,  1.1791e-05,  6.2342e-05,\n",
       "            -3.1411e-06,  2.2281e-05,  2.0787e-06,  1.1154e-05, -3.0619e-07,\n",
       "            -8.2386e-06, -4.2538e-06, -6.8729e-05, -2.3381e-05,  7.8578e-06,\n",
       "             2.8847e-05, -7.6346e-06, -6.4341e-05, -2.6221e-06,  3.9692e-07,\n",
       "            -1.3263e-05,  1.5244e-04, -1.6485e-05, -5.5085e-06, -1.0000e-04,\n",
       "             8.8979e-06, -3.1866e-05,  2.4866e-05, -5.4089e-05,  4.8738e-06,\n",
       "            -5.4080e-06,  5.7086e-06, -9.0554e-06,  7.1659e-06,  5.7959e-07,\n",
       "             7.0558e-05, -3.5967e-05, -2.5985e-05, -4.9486e-05, -6.0062e-05,\n",
       "            -7.5913e-05,  1.6022e-05, -1.8905e-05,  4.9544e-05,  1.6233e-05,\n",
       "            -1.1591e-05, -3.1259e-05, -3.8556e-06,  7.9045e-05, -7.7651e-06,\n",
       "            -1.2328e-05,  1.1264e-05, -5.1186e-05,  1.9249e-04, -4.4208e-06,\n",
       "            -2.1224e-05, -2.4608e-05,  1.2826e-05,  3.3457e-05, -2.4617e-05,\n",
       "             1.7872e-05, -2.2072e-07, -2.6714e-04,  2.7293e-05, -1.8298e-05,\n",
       "            -2.7224e-05,  3.6930e-05,  1.3449e-04,  5.0519e-06,  6.4736e-06,\n",
       "            -2.5824e-05, -3.3121e-05,  4.0132e-06, -9.3535e-05, -1.8572e-05,\n",
       "             2.0935e-05,  8.7862e-05, -7.1470e-05,  1.0096e-05, -1.4521e-05,\n",
       "             1.1884e-05,  3.4289e-05,  6.4158e-06, -8.1234e-06, -6.1587e-05,\n",
       "            -6.3150e-05,  4.2382e-05, -3.0645e-05,  6.3681e-05,  1.3753e-05,\n",
       "            -1.5421e-05, -7.2805e-06,  2.4691e-05,  7.1043e-06, -1.6765e-05,\n",
       "             7.0557e-07,  8.1751e-06,  2.2092e-05,  4.3598e-05, -2.1853e-05,\n",
       "             8.6078e-07, -8.2120e-06, -6.1690e-08,  1.5531e-05, -4.1364e-06,\n",
       "            -6.5496e-05,  2.2128e-05, -3.4471e-05, -3.2508e-05, -2.4203e-05,\n",
       "             3.8246e-05, -1.6806e-05, -3.8131e-05,  3.7351e-05,  1.5732e-05,\n",
       "            -6.5300e-06,  8.1039e-06, -3.9172e-05,  3.4022e-05, -1.6651e-06,\n",
       "             2.1977e-05, -6.7592e-05,  1.4777e-06, -6.9389e-06,  2.0697e-05,\n",
       "             1.4443e-04, -9.4391e-06,  2.3916e-05,  6.1151e-05, -1.5568e-07,\n",
       "             7.9251e-05,  1.5214e-05, -1.5349e-05,  5.4849e-05,  3.7860e-06,\n",
       "            -4.6645e-06,  2.8548e-05,  3.3911e-08, -7.8396e-05, -1.0268e-05,\n",
       "             1.6269e-06,  1.2375e-04,  1.1684e-05, -3.1768e-05,  1.4324e-05,\n",
       "             4.6516e-05,  1.5659e-05,  2.9385e-05,  7.7246e-05, -5.5344e-05,\n",
       "             5.9385e-05, -3.9392e-05,  1.0721e-05,  1.3895e-05,  7.0307e-05,\n",
       "             2.3101e-05, -2.8827e-05, -6.1670e-05, -5.1838e-06,  2.8061e-05,\n",
       "             3.5473e-05,  1.3299e-05,  2.1816e-06,  8.5435e-05,  1.9641e-06,\n",
       "             2.7866e-05,  3.7503e-05,  3.6098e-06,  3.2817e-05,  3.2308e-05,\n",
       "            -3.4299e-05,  1.6456e-05, -7.5573e-05,  2.2872e-05, -1.6577e-05,\n",
       "             2.8115e-05,  2.2222e-05,  8.4404e-05,  8.3314e-06, -5.1927e-05,\n",
       "             3.4306e-06,  3.0990e-05,  3.3379e-05,  7.0334e-06,  1.8229e-05,\n",
       "             1.0363e-05, -2.9222e-05, -4.2022e-05,  1.9301e-05,  8.4440e-06,\n",
       "            -4.5957e-06, -9.2025e-06, -7.0753e-06,  4.5721e-05, -4.8498e-05,\n",
       "             9.9858e-06,  1.2880e-04,  1.9367e-05, -7.7512e-05,  3.3743e-05,\n",
       "            -2.2087e-05, -2.1486e-05, -1.7530e-05, -5.1824e-05,  3.9907e-05,\n",
       "             1.0811e-05, -2.4332e-06, -4.1634e-05, -5.2028e-05, -1.3335e-05,\n",
       "            -6.9298e-05,  1.6836e-05,  1.8206e-05, -1.7062e-05, -1.2341e-05,\n",
       "            -5.5497e-06,  1.2921e-05, -6.3602e-05,  2.1725e-06, -1.4828e-05,\n",
       "            -5.0688e-05, -3.8030e-05, -9.1679e-06,  3.1906e-05, -2.4082e-05,\n",
       "            -3.8511e-05, -8.0590e-07, -5.8948e-05, -4.7935e-05, -4.9526e-05,\n",
       "            -2.4462e-05, -5.5599e-05, -1.5646e-05, -1.4635e-05,  3.9636e-05,\n",
       "            -3.3729e-05, -2.1581e-05,  7.4329e-06,  1.0255e-04,  4.5880e-05,\n",
       "             2.9352e-05,  3.4946e-06,  1.4369e-05,  4.3044e-06, -1.0435e-05,\n",
       "            -2.0314e-05,  2.1251e-05,  4.2622e-05,  6.1976e-06, -9.6717e-06,\n",
       "            -2.0532e-05,  3.4334e-05, -2.6242e-05, -5.1230e-05,  5.9355e-05,\n",
       "            -8.5011e-05, -5.9165e-05,  2.6602e-06, -3.5033e-05, -8.8717e-05,\n",
       "            -1.4927e-04, -4.4388e-05, -2.2116e-06,  1.5064e-05, -2.7226e-05,\n",
       "             3.9007e-06,  1.2812e-05, -3.0119e-05, -2.2455e-05, -6.5431e-06,\n",
       "            -6.0483e-05, -1.7487e-05, -2.1973e-05, -1.2399e-05, -1.2046e-06,\n",
       "            -7.2456e-06,  6.3444e-07]),\n",
       "    'exp_avg_sq': tensor([8.9302e-08, 3.8119e-08, 1.8911e-07, 1.3782e-07, 1.5815e-07, 3.8138e-09,\n",
       "            1.5299e-08, 1.8253e-07, 2.7087e-07, 6.8388e-08, 1.1204e-08, 1.9197e-08,\n",
       "            4.1222e-08, 8.0256e-08, 2.0193e-08, 2.4456e-07, 2.3937e-07, 1.1649e-08,\n",
       "            3.3627e-07, 3.1562e-08, 1.0580e-08, 8.8397e-08, 7.6475e-07, 7.4028e-09,\n",
       "            8.6109e-08, 2.5414e-08, 6.8246e-08, 9.3432e-08, 2.3912e-07, 8.7019e-08,\n",
       "            2.3950e-08, 4.4459e-08, 5.0406e-07, 1.4961e-07, 6.7438e-08, 3.0404e-08,\n",
       "            2.0063e-08, 2.5418e-08, 4.4560e-08, 1.0168e-07, 2.2976e-08, 3.6631e-07,\n",
       "            1.3447e-08, 3.0758e-08, 2.3477e-08, 5.5785e-08, 5.3540e-08, 8.5220e-08,\n",
       "            1.4042e-07, 5.6055e-08, 6.5831e-07, 6.5555e-08, 1.1576e-07, 1.2266e-07,\n",
       "            2.7908e-07, 3.1989e-07, 2.8077e-08, 3.4750e-07, 3.3370e-08, 1.9738e-07,\n",
       "            1.0495e-07, 1.9288e-08, 1.2164e-07, 2.1827e-07, 4.0208e-09, 7.1690e-08,\n",
       "            9.1010e-08, 1.4855e-07, 3.3740e-08, 8.6006e-08, 7.2866e-09, 6.2629e-08,\n",
       "            6.5201e-08, 2.2295e-08, 9.6574e-07, 1.1986e-08, 2.8597e-08, 5.4855e-08,\n",
       "            1.0525e-07, 6.4461e-08, 2.0383e-07, 3.0367e-11, 1.9636e-07, 2.8484e-08,\n",
       "            6.6074e-08, 1.5289e-08, 1.1390e-07, 2.5855e-08, 9.4849e-07, 4.1817e-08,\n",
       "            8.0141e-08, 1.0083e-08, 2.3443e-08, 4.9066e-08, 3.5251e-07, 2.1818e-08,\n",
       "            5.0982e-08, 3.1193e-07, 3.3443e-08, 4.4119e-08, 3.3301e-07, 1.6047e-08,\n",
       "            9.7238e-07, 3.9195e-08, 2.6696e-07, 8.3229e-07, 1.8819e-08, 1.1871e-07,\n",
       "            1.3302e-08, 7.6557e-09, 4.5180e-08, 8.3918e-08, 1.3038e-07, 3.0081e-08,\n",
       "            5.5945e-08, 1.1624e-06, 2.1656e-09, 1.8176e-06, 3.2606e-08, 1.6578e-07,\n",
       "            3.4665e-08, 5.7234e-09, 2.4497e-08, 1.7733e-08, 5.1912e-07, 1.9076e-08,\n",
       "            5.9103e-08, 4.7122e-08, 6.9795e-08, 5.5954e-08, 2.1553e-07, 1.1952e-07,\n",
       "            1.7080e-08, 4.7894e-09, 4.9605e-08, 8.3063e-09, 2.6606e-08, 1.1027e-08,\n",
       "            9.2823e-09, 6.0717e-09, 8.3830e-09, 4.1793e-08, 1.4911e-08, 2.9986e-07,\n",
       "            2.7433e-08, 3.5965e-08, 4.0425e-08, 4.7425e-08, 2.2324e-09, 1.1525e-08,\n",
       "            1.8302e-07, 6.9560e-09, 8.5895e-09, 1.4510e-08, 1.7634e-08, 2.0478e-07,\n",
       "            6.3241e-08, 2.3160e-08, 4.6560e-08, 1.3414e-08, 7.7785e-08, 1.5295e-07,\n",
       "            2.7873e-08, 2.2282e-08, 4.4568e-09, 7.3886e-08, 1.4424e-08, 1.1384e-08,\n",
       "            3.0005e-08, 2.4092e-08, 2.8857e-09, 5.8985e-09, 4.8076e-08, 1.2332e-08,\n",
       "            6.9109e-09, 6.3832e-08, 4.1336e-08, 3.3482e-08, 4.5002e-07, 2.3320e-08,\n",
       "            6.0541e-08, 5.8728e-08, 8.6961e-08, 1.6163e-08, 4.4828e-08, 4.3698e-08,\n",
       "            1.0765e-08, 6.2815e-08, 9.4289e-08, 1.3734e-08, 6.7204e-08, 1.3699e-07,\n",
       "            2.0052e-09, 1.4666e-07, 1.0272e-08, 2.9630e-08, 3.9265e-08, 3.5603e-09,\n",
       "            2.8566e-09, 3.8947e-08, 1.0501e-07, 1.7095e-08, 5.3876e-08, 6.4230e-09,\n",
       "            3.0744e-08, 3.5310e-08, 4.1046e-08, 2.6100e-08, 4.2149e-08, 1.6117e-05,\n",
       "            1.4302e-07, 1.2593e-08, 1.8790e-08, 2.6533e-08, 7.1162e-08, 3.0984e-08,\n",
       "            5.4924e-08, 2.6683e-08, 2.5151e-08, 3.2549e-09, 8.3123e-09, 1.5148e-09,\n",
       "            6.1804e-08, 5.0584e-09, 2.8566e-08, 6.1562e-08, 3.3973e-09, 8.5231e-08,\n",
       "            4.9174e-07, 6.6801e-09, 3.8425e-07, 9.4334e-09, 1.7484e-07, 2.2319e-07,\n",
       "            1.5737e-07, 3.6855e-08, 2.0781e-08, 2.2609e-09, 1.0971e-08, 3.8511e-08,\n",
       "            6.9800e-08, 5.9182e-09, 2.9144e-07, 2.3740e-08, 3.4328e-10, 3.7183e-08,\n",
       "            3.0642e-08, 7.9140e-08, 1.4982e-07, 6.4179e-09, 1.4022e-08, 7.0392e-09,\n",
       "            4.1529e-08, 1.4263e-08, 2.7460e-08, 4.2401e-09, 2.6320e-07, 7.3052e-08,\n",
       "            5.2899e-08, 7.3717e-08, 1.1506e-06, 3.5300e-08, 5.6111e-08, 3.2585e-08,\n",
       "            2.6871e-08, 1.1013e-08, 1.3611e-08, 1.1866e-07, 1.0065e-07, 4.0834e-08,\n",
       "            1.4968e-07, 6.0048e-08, 1.0673e-07, 1.3492e-08, 1.3677e-07, 7.9993e-08,\n",
       "            2.4614e-08, 1.6105e-07, 1.2465e-07, 1.1386e-08, 1.3922e-07, 2.7982e-07,\n",
       "            1.0517e-07, 2.1311e-07, 1.2133e-07, 5.6862e-07, 7.6262e-08, 5.0318e-08,\n",
       "            5.0717e-07, 1.0437e-07, 7.3224e-08, 1.0680e-07, 2.2652e-08, 3.1376e-08,\n",
       "            9.5913e-08, 4.4862e-08, 4.1930e-07, 4.7643e-07, 5.3786e-08, 2.3298e-08,\n",
       "            2.0802e-08, 1.0058e-07, 2.3348e-07, 3.8106e-08, 1.5035e-07, 1.1097e-07,\n",
       "            1.5976e-07, 9.9839e-08, 2.4217e-07, 5.6158e-08, 9.5562e-08, 1.1977e-07,\n",
       "            8.8678e-08, 8.5196e-08, 2.6891e-08, 7.9782e-07, 1.6402e-07, 2.3161e-08,\n",
       "            9.2948e-08, 3.0402e-08, 1.2414e-08, 1.7810e-08, 1.0124e-07, 2.2593e-07,\n",
       "            1.9776e-07, 2.7540e-07, 1.6891e-08, 1.7409e-07, 6.3375e-08, 4.2145e-08,\n",
       "            5.7027e-07, 6.2410e-08, 1.0272e-07, 1.2522e-07, 3.0740e-08, 7.0033e-08,\n",
       "            6.8117e-08, 9.6515e-10, 5.3526e-08, 1.0285e-09, 1.0552e-07, 3.7564e-08,\n",
       "            5.2801e-07, 2.4630e-07, 7.9962e-08, 5.6681e-08, 2.9742e-08, 9.8008e-08,\n",
       "            1.7615e-07, 3.5981e-08, 1.1884e-07, 1.1046e-07, 2.2479e-07, 3.1808e-07,\n",
       "            3.8218e-08, 4.6634e-08, 1.4645e-07, 3.3892e-08, 3.2329e-08, 4.5068e-08,\n",
       "            1.5166e-07, 5.0584e-07, 5.9251e-08, 8.3796e-08, 2.9553e-08, 6.7688e-08,\n",
       "            2.4889e-06, 3.5084e-08, 6.1936e-08, 4.0573e-09, 1.2549e-07, 1.9544e-07,\n",
       "            6.6717e-09, 4.0424e-07, 5.7290e-08, 4.0465e-08, 1.1792e-07, 5.6353e-09,\n",
       "            1.0703e-07, 7.2529e-08, 9.6097e-08, 3.3848e-08, 9.6959e-08, 1.7230e-07,\n",
       "            2.4674e-07, 2.4727e-07, 8.0741e-07, 2.1108e-07, 4.0579e-08, 8.1934e-08,\n",
       "            6.9268e-08, 1.3188e-07, 7.3807e-08, 3.2196e-07, 1.0836e-07, 7.8340e-08,\n",
       "            2.2565e-08, 3.6258e-08, 9.1041e-08, 8.5752e-07, 1.3834e-07, 2.2209e-07,\n",
       "            8.3457e-08, 9.5772e-08, 2.7541e-07, 1.7182e-07, 7.9443e-07, 1.9995e-07,\n",
       "            1.9344e-07, 3.7097e-07, 4.1643e-08, 1.4261e-07, 5.4985e-08, 3.5389e-08,\n",
       "            1.0181e-07, 5.5025e-08, 1.5763e-07, 4.6782e-08, 1.3800e-07, 5.2856e-08,\n",
       "            8.4542e-08, 9.6560e-08, 1.4180e-07, 7.0506e-08, 3.2747e-08, 5.6530e-07,\n",
       "            3.7158e-08, 1.3076e-07, 1.4881e-07, 1.9687e-08, 8.0959e-08, 9.2677e-08,\n",
       "            8.5565e-08, 1.4369e-07, 1.2568e-06, 2.2168e-08, 9.9564e-09, 4.3413e-08,\n",
       "            4.0752e-07, 3.5582e-08, 2.2769e-07, 7.1820e-08, 9.7605e-08, 5.6390e-08,\n",
       "            7.1009e-08, 5.2242e-08, 4.2738e-08, 1.6962e-07, 9.7236e-08, 1.8869e-08,\n",
       "            1.8009e-08, 2.2587e-08, 4.8286e-07, 6.9126e-07, 9.5341e-08, 6.5816e-08,\n",
       "            3.1703e-07, 5.6863e-08, 1.7048e-07, 3.4826e-08, 1.2509e-07, 2.9602e-08,\n",
       "            5.8270e-07, 9.2657e-08, 1.5906e-07, 3.9443e-07, 1.1075e-07, 9.7281e-08,\n",
       "            7.0413e-07, 1.8360e-07, 1.5827e-07, 6.2106e-08, 2.1534e-07, 2.3552e-07,\n",
       "            1.1642e-07, 4.8602e-08, 5.9269e-08, 6.6854e-08, 7.1324e-08, 6.1375e-08,\n",
       "            2.3935e-07, 8.1145e-08, 2.4295e-07, 1.4681e-07, 2.7976e-07, 6.1028e-08,\n",
       "            6.0333e-08, 6.6523e-08, 5.7650e-08, 2.4317e-07, 2.3352e-07, 1.2513e-07,\n",
       "            1.8796e-07, 8.5840e-08, 1.0970e-07, 9.1822e-08, 1.7486e-07, 9.0815e-08,\n",
       "            5.5077e-07, 3.8250e-07, 8.6878e-08, 1.2832e-08, 1.0126e-07, 3.0481e-08,\n",
       "            4.8868e-08, 2.4943e-07, 8.1011e-08, 5.5305e-08, 9.6857e-08, 4.6612e-08,\n",
       "            4.7888e-08, 6.2003e-08])},\n",
       "   4: {'step': 112320,\n",
       "    'exp_avg': tensor([-2.3516e-05, -1.1116e-05,  1.0755e-04,  8.4242e-05,  1.1718e-04,\n",
       "             6.8176e-06, -7.2220e-06, -2.6737e-05, -6.3128e-06, -1.3210e-05,\n",
       "             2.7185e-05,  1.5404e-05, -2.9801e-06, -3.8539e-05,  1.9611e-05,\n",
       "            -4.8453e-06,  4.2298e-05,  9.2238e-06,  4.6570e-05,  2.0068e-05,\n",
       "             1.0382e-05, -1.8487e-05,  7.5546e-06, -5.3233e-06, -1.4426e-05,\n",
       "            -2.9918e-05, -1.6663e-05,  2.1851e-05,  7.1377e-05,  6.9825e-06,\n",
       "            -1.4432e-05, -4.9779e-05,  1.4583e-05,  3.4808e-05,  1.1362e-05,\n",
       "            -1.0209e-05,  4.8738e-06, -2.9576e-05, -2.6068e-05,  1.9277e-06,\n",
       "            -2.4647e-06, -1.1668e-05, -4.2184e-06, -8.7205e-06,  6.5784e-06,\n",
       "            -2.4592e-05,  2.6296e-06, -3.0782e-05,  5.2863e-05, -2.4090e-05,\n",
       "             1.1586e-04, -1.1973e-05, -1.4111e-04, -2.0093e-05,  8.2411e-06,\n",
       "            -1.8898e-05, -3.6768e-05, -2.1267e-06, -2.3150e-05, -2.3591e-05,\n",
       "            -6.0952e-05, -1.8081e-05,  2.1426e-05, -4.3578e-06, -3.2092e-06,\n",
       "            -1.0140e-05,  2.2556e-05,  1.2264e-04, -1.4429e-05, -1.5503e-05,\n",
       "             5.5995e-06, -3.2255e-05, -1.0686e-05, -1.1962e-05, -7.2886e-06,\n",
       "             3.4674e-06,  2.1250e-05,  1.7397e-05, -3.5734e-07,  1.7849e-05,\n",
       "            -1.3648e-05,  5.2036e-08, -1.8524e-05,  1.1675e-05,  3.9128e-05,\n",
       "             1.9175e-05, -1.0285e-05, -1.1592e-05,  1.7174e-05,  2.7888e-05,\n",
       "             6.6552e-05,  3.7430e-06, -1.2114e-05,  2.3219e-05,  5.7640e-05,\n",
       "             8.3456e-07,  8.8841e-06,  4.3384e-05,  6.7362e-06, -7.7181e-06,\n",
       "             9.6931e-05, -8.4082e-06,  1.4362e-05, -1.9834e-05, -1.8869e-05,\n",
       "             4.1699e-05, -1.3442e-05,  9.0431e-05,  2.9924e-06,  1.3896e-05,\n",
       "             1.3350e-06, -1.7866e-04, -7.4549e-05,  9.4607e-06,  2.9743e-05,\n",
       "             2.7412e-05,  7.2604e-06,  1.8155e-04,  2.1476e-06,  1.3841e-05,\n",
       "             3.8181e-06, -7.4171e-06, -3.0940e-05,  1.3235e-05, -7.2708e-07,\n",
       "             3.3635e-06, -3.1985e-05, -8.0391e-05, -2.0032e-05, -5.2268e-06,\n",
       "            -3.0441e-05, -2.3095e-05, -3.5957e-06,  5.2450e-06,  3.1992e-06,\n",
       "            -3.4643e-05, -3.2597e-05, -1.2792e-05,  2.0357e-06,  9.5523e-07,\n",
       "             1.1021e-05, -9.8819e-06, -1.1163e-06, -7.2246e-06,  2.2172e-05,\n",
       "             9.3412e-06, -1.2800e-06, -1.4201e-05,  7.0925e-07,  1.2020e-06,\n",
       "            -7.7535e-06, -1.7691e-05,  5.3028e-06,  7.0202e-06,  3.6384e-05,\n",
       "             3.4880e-06, -1.2805e-05, -3.7202e-05, -5.9494e-06, -2.6030e-05,\n",
       "            -1.3576e-05,  6.6220e-05, -4.9913e-06,  4.7233e-06,  4.8971e-06,\n",
       "             8.6906e-05, -2.7461e-05, -3.7355e-06, -1.1675e-05,  3.2942e-05,\n",
       "            -1.9738e-06, -8.5099e-06, -5.2191e-06, -1.4539e-06, -1.3816e-05,\n",
       "            -1.1288e-05,  1.4666e-06,  4.7953e-05,  3.9341e-05,  1.4147e-06,\n",
       "            -4.3302e-05,  2.1894e-05,  6.9398e-06,  1.0448e-05,  1.2970e-05,\n",
       "             2.7881e-05, -9.2849e-06,  4.6049e-05,  1.7379e-05, -8.8710e-06,\n",
       "            -3.4151e-06,  9.2222e-05, -2.2874e-05, -1.5231e-06, -2.9499e-06,\n",
       "            -5.0833e-06, -7.8870e-05, -4.0053e-06,  1.4548e-05,  3.4805e-06,\n",
       "             4.7228e-05, -1.7650e-05,  2.6204e-06,  1.3622e-06, -2.0490e-05,\n",
       "            -1.1119e-05, -1.8636e-05, -2.9618e-05,  9.5247e-06, -6.0037e-04,\n",
       "             1.4519e-05,  1.3006e-05, -3.4552e-05,  2.0175e-05,  4.2003e-06,\n",
       "            -1.0739e-05,  3.5974e-05, -4.9414e-06, -1.2615e-05,  7.4291e-07,\n",
       "            -8.1947e-06,  5.3339e-06,  9.3678e-06,  3.8690e-06,  1.1702e-05,\n",
       "             1.4180e-05,  3.2957e-06, -3.0469e-05, -1.4025e-05, -8.5719e-06,\n",
       "            -2.2981e-06, -1.9306e-05,  6.1321e-05, -9.7699e-06,  4.0260e-05,\n",
       "             1.0818e-05,  1.6527e-05, -8.9706e-06,  1.1791e-05,  6.2342e-05,\n",
       "            -3.1411e-06,  2.2281e-05,  2.0787e-06,  1.1154e-05, -3.0619e-07,\n",
       "            -8.2386e-06, -4.2538e-06, -6.8729e-05, -2.3381e-05,  7.8578e-06,\n",
       "             2.8847e-05, -7.6346e-06, -6.4341e-05, -2.6221e-06,  3.9692e-07,\n",
       "            -1.3263e-05,  1.5244e-04, -1.6485e-05, -5.5085e-06, -1.0000e-04,\n",
       "             8.8979e-06, -3.1866e-05,  2.4866e-05, -5.4089e-05,  4.8738e-06,\n",
       "            -5.4080e-06,  5.7086e-06, -9.0554e-06,  7.1659e-06,  5.7959e-07,\n",
       "             7.0558e-05, -3.5967e-05, -2.5985e-05, -4.9486e-05, -6.0062e-05,\n",
       "            -7.5913e-05,  1.6022e-05, -1.8905e-05,  4.9544e-05,  1.6233e-05,\n",
       "            -1.1591e-05, -3.1259e-05, -3.8556e-06,  7.9045e-05, -7.7651e-06,\n",
       "            -1.2328e-05,  1.1264e-05, -5.1186e-05,  1.9249e-04, -4.4208e-06,\n",
       "            -2.1224e-05, -2.4608e-05,  1.2826e-05,  3.3457e-05, -2.4617e-05,\n",
       "             1.7872e-05, -2.2072e-07, -2.6714e-04,  2.7293e-05, -1.8298e-05,\n",
       "            -2.7224e-05,  3.6930e-05,  1.3449e-04,  5.0519e-06,  6.4736e-06,\n",
       "            -2.5824e-05, -3.3121e-05,  4.0132e-06, -9.3535e-05, -1.8572e-05,\n",
       "             2.0935e-05,  8.7862e-05, -7.1470e-05,  1.0096e-05, -1.4521e-05,\n",
       "             1.1884e-05,  3.4289e-05,  6.4158e-06, -8.1234e-06, -6.1587e-05,\n",
       "            -6.3150e-05,  4.2382e-05, -3.0645e-05,  6.3681e-05,  1.3753e-05,\n",
       "            -1.5421e-05, -7.2805e-06,  2.4691e-05,  7.1043e-06, -1.6765e-05,\n",
       "             7.0557e-07,  8.1751e-06,  2.2092e-05,  4.3598e-05, -2.1853e-05,\n",
       "             8.6078e-07, -8.2120e-06, -6.1690e-08,  1.5531e-05, -4.1364e-06,\n",
       "            -6.5496e-05,  2.2128e-05, -3.4471e-05, -3.2508e-05, -2.4203e-05,\n",
       "             3.8246e-05, -1.6806e-05, -3.8131e-05,  3.7351e-05,  1.5732e-05,\n",
       "            -6.5300e-06,  8.1039e-06, -3.9172e-05,  3.4022e-05, -1.6651e-06,\n",
       "             2.1977e-05, -6.7592e-05,  1.4777e-06, -6.9389e-06,  2.0697e-05,\n",
       "             1.4443e-04, -9.4391e-06,  2.3916e-05,  6.1151e-05, -1.5568e-07,\n",
       "             7.9251e-05,  1.5214e-05, -1.5349e-05,  5.4849e-05,  3.7860e-06,\n",
       "            -4.6645e-06,  2.8548e-05,  3.3912e-08, -7.8396e-05, -1.0268e-05,\n",
       "             1.6269e-06,  1.2375e-04,  1.1684e-05, -3.1768e-05,  1.4324e-05,\n",
       "             4.6516e-05,  1.5659e-05,  2.9385e-05,  7.7246e-05, -5.5344e-05,\n",
       "             5.9385e-05, -3.9392e-05,  1.0721e-05,  1.3895e-05,  7.0307e-05,\n",
       "             2.3101e-05, -2.8827e-05, -6.1670e-05, -5.1839e-06,  2.8061e-05,\n",
       "             3.5473e-05,  1.3299e-05,  2.1816e-06,  8.5435e-05,  1.9641e-06,\n",
       "             2.7866e-05,  3.7503e-05,  3.6098e-06,  3.2817e-05,  3.2308e-05,\n",
       "            -3.4299e-05,  1.6456e-05, -7.5573e-05,  2.2872e-05, -1.6577e-05,\n",
       "             2.8115e-05,  2.2222e-05,  8.4404e-05,  8.3314e-06, -5.1927e-05,\n",
       "             3.4306e-06,  3.0990e-05,  3.3379e-05,  7.0334e-06,  1.8229e-05,\n",
       "             1.0363e-05, -2.9222e-05, -4.2022e-05,  1.9301e-05,  8.4440e-06,\n",
       "            -4.5957e-06, -9.2025e-06, -7.0753e-06,  4.5721e-05, -4.8498e-05,\n",
       "             9.9857e-06,  1.2880e-04,  1.9367e-05, -7.7512e-05,  3.3743e-05,\n",
       "            -2.2087e-05, -2.1486e-05, -1.7530e-05, -5.1824e-05,  3.9907e-05,\n",
       "             1.0811e-05, -2.4332e-06, -4.1634e-05, -5.2028e-05, -1.3335e-05,\n",
       "            -6.9298e-05,  1.6836e-05,  1.8206e-05, -1.7062e-05, -1.2341e-05,\n",
       "            -5.5497e-06,  1.2921e-05, -6.3602e-05,  2.1725e-06, -1.4828e-05,\n",
       "            -5.0688e-05, -3.8030e-05, -9.1679e-06,  3.1906e-05, -2.4082e-05,\n",
       "            -3.8511e-05, -8.0590e-07, -5.8948e-05, -4.7935e-05, -4.9526e-05,\n",
       "            -2.4462e-05, -5.5599e-05, -1.5646e-05, -1.4635e-05,  3.9636e-05,\n",
       "            -3.3729e-05, -2.1581e-05,  7.4329e-06,  1.0255e-04,  4.5880e-05,\n",
       "             2.9352e-05,  3.4946e-06,  1.4369e-05,  4.3044e-06, -1.0435e-05,\n",
       "            -2.0314e-05,  2.1251e-05,  4.2622e-05,  6.1977e-06, -9.6717e-06,\n",
       "            -2.0532e-05,  3.4334e-05, -2.6242e-05, -5.1230e-05,  5.9355e-05,\n",
       "            -8.5011e-05, -5.9165e-05,  2.6602e-06, -3.5033e-05, -8.8717e-05,\n",
       "            -1.4927e-04, -4.4388e-05, -2.2116e-06,  1.5064e-05, -2.7226e-05,\n",
       "             3.9007e-06,  1.2812e-05, -3.0119e-05, -2.2455e-05, -6.5431e-06,\n",
       "            -6.0483e-05, -1.7487e-05, -2.1973e-05, -1.2399e-05, -1.2046e-06,\n",
       "            -7.2456e-06,  6.3445e-07]),\n",
       "    'exp_avg_sq': tensor([8.9302e-08, 3.8119e-08, 1.8911e-07, 1.3782e-07, 1.5815e-07, 3.8138e-09,\n",
       "            1.5299e-08, 1.8253e-07, 2.7087e-07, 6.8388e-08, 1.1204e-08, 1.9197e-08,\n",
       "            4.1222e-08, 8.0256e-08, 2.0193e-08, 2.4456e-07, 2.3937e-07, 1.1649e-08,\n",
       "            3.3627e-07, 3.1562e-08, 1.0580e-08, 8.8397e-08, 7.6475e-07, 7.4028e-09,\n",
       "            8.6109e-08, 2.5414e-08, 6.8246e-08, 9.3432e-08, 2.3912e-07, 8.7019e-08,\n",
       "            2.3950e-08, 4.4459e-08, 5.0406e-07, 1.4961e-07, 6.7438e-08, 3.0404e-08,\n",
       "            2.0063e-08, 2.5418e-08, 4.4560e-08, 1.0168e-07, 2.2976e-08, 3.6631e-07,\n",
       "            1.3447e-08, 3.0758e-08, 2.3477e-08, 5.5785e-08, 5.3540e-08, 8.5220e-08,\n",
       "            1.4042e-07, 5.6055e-08, 6.5831e-07, 6.5555e-08, 1.1576e-07, 1.2266e-07,\n",
       "            2.7908e-07, 3.1989e-07, 2.8077e-08, 3.4750e-07, 3.3370e-08, 1.9738e-07,\n",
       "            1.0495e-07, 1.9288e-08, 1.2164e-07, 2.1827e-07, 4.0208e-09, 7.1690e-08,\n",
       "            9.1010e-08, 1.4855e-07, 3.3740e-08, 8.6006e-08, 7.2866e-09, 6.2629e-08,\n",
       "            6.5201e-08, 2.2295e-08, 9.6574e-07, 1.1986e-08, 2.8597e-08, 5.4855e-08,\n",
       "            1.0525e-07, 6.4461e-08, 2.0383e-07, 3.0367e-11, 1.9636e-07, 2.8484e-08,\n",
       "            6.6074e-08, 1.5289e-08, 1.1390e-07, 2.5855e-08, 9.4849e-07, 4.1817e-08,\n",
       "            8.0141e-08, 1.0083e-08, 2.3443e-08, 4.9066e-08, 3.5251e-07, 2.1818e-08,\n",
       "            5.0982e-08, 3.1193e-07, 3.3443e-08, 4.4119e-08, 3.3301e-07, 1.6047e-08,\n",
       "            9.7238e-07, 3.9195e-08, 2.6696e-07, 8.3229e-07, 1.8819e-08, 1.1871e-07,\n",
       "            1.3302e-08, 7.6557e-09, 4.5180e-08, 8.3918e-08, 1.3038e-07, 3.0081e-08,\n",
       "            5.5945e-08, 1.1624e-06, 2.1656e-09, 1.8176e-06, 3.2606e-08, 1.6578e-07,\n",
       "            3.4665e-08, 5.7234e-09, 2.4497e-08, 1.7733e-08, 5.1911e-07, 1.9076e-08,\n",
       "            5.9103e-08, 4.7122e-08, 6.9795e-08, 5.5954e-08, 2.1553e-07, 1.1952e-07,\n",
       "            1.7080e-08, 4.7894e-09, 4.9605e-08, 8.3063e-09, 2.6606e-08, 1.1027e-08,\n",
       "            9.2823e-09, 6.0717e-09, 8.3830e-09, 4.1793e-08, 1.4911e-08, 2.9986e-07,\n",
       "            2.7433e-08, 3.5965e-08, 4.0425e-08, 4.7425e-08, 2.2324e-09, 1.1525e-08,\n",
       "            1.8302e-07, 6.9560e-09, 8.5895e-09, 1.4510e-08, 1.7634e-08, 2.0478e-07,\n",
       "            6.3241e-08, 2.3160e-08, 4.6560e-08, 1.3414e-08, 7.7785e-08, 1.5295e-07,\n",
       "            2.7873e-08, 2.2282e-08, 4.4568e-09, 7.3886e-08, 1.4424e-08, 1.1384e-08,\n",
       "            3.0005e-08, 2.4092e-08, 2.8857e-09, 5.8985e-09, 4.8076e-08, 1.2332e-08,\n",
       "            6.9109e-09, 6.3832e-08, 4.1336e-08, 3.3482e-08, 4.5002e-07, 2.3320e-08,\n",
       "            6.0541e-08, 5.8728e-08, 8.6961e-08, 1.6163e-08, 4.4828e-08, 4.3698e-08,\n",
       "            1.0765e-08, 6.2815e-08, 9.4289e-08, 1.3734e-08, 6.7204e-08, 1.3699e-07,\n",
       "            2.0052e-09, 1.4666e-07, 1.0272e-08, 2.9630e-08, 3.9265e-08, 3.5603e-09,\n",
       "            2.8566e-09, 3.8947e-08, 1.0501e-07, 1.7095e-08, 5.3876e-08, 6.4230e-09,\n",
       "            3.0744e-08, 3.5310e-08, 4.1046e-08, 2.6100e-08, 4.2149e-08, 1.6117e-05,\n",
       "            1.4302e-07, 1.2593e-08, 1.8790e-08, 2.6533e-08, 7.1162e-08, 3.0984e-08,\n",
       "            5.4924e-08, 2.6683e-08, 2.5151e-08, 3.2549e-09, 8.3123e-09, 1.5148e-09,\n",
       "            6.1804e-08, 5.0584e-09, 2.8566e-08, 6.1562e-08, 3.3973e-09, 8.5231e-08,\n",
       "            4.9174e-07, 6.6800e-09, 3.8425e-07, 9.4334e-09, 1.7484e-07, 2.2319e-07,\n",
       "            1.5737e-07, 3.6855e-08, 2.0781e-08, 2.2609e-09, 1.0971e-08, 3.8511e-08,\n",
       "            6.9800e-08, 5.9182e-09, 2.9144e-07, 2.3740e-08, 3.4328e-10, 3.7183e-08,\n",
       "            3.0642e-08, 7.9140e-08, 1.4982e-07, 6.4179e-09, 1.4022e-08, 7.0392e-09,\n",
       "            4.1529e-08, 1.4263e-08, 2.7460e-08, 4.2401e-09, 2.6320e-07, 7.3052e-08,\n",
       "            5.2899e-08, 7.3717e-08, 1.1506e-06, 3.5300e-08, 5.6111e-08, 3.2585e-08,\n",
       "            2.6871e-08, 1.1013e-08, 1.3611e-08, 1.1866e-07, 1.0065e-07, 4.0834e-08,\n",
       "            1.4968e-07, 6.0048e-08, 1.0673e-07, 1.3492e-08, 1.3677e-07, 7.9993e-08,\n",
       "            2.4614e-08, 1.6105e-07, 1.2465e-07, 1.1386e-08, 1.3922e-07, 2.7982e-07,\n",
       "            1.0517e-07, 2.1311e-07, 1.2133e-07, 5.6862e-07, 7.6262e-08, 5.0318e-08,\n",
       "            5.0717e-07, 1.0437e-07, 7.3224e-08, 1.0680e-07, 2.2652e-08, 3.1376e-08,\n",
       "            9.5913e-08, 4.4862e-08, 4.1930e-07, 4.7643e-07, 5.3786e-08, 2.3298e-08,\n",
       "            2.0802e-08, 1.0058e-07, 2.3348e-07, 3.8106e-08, 1.5035e-07, 1.1097e-07,\n",
       "            1.5976e-07, 9.9839e-08, 2.4217e-07, 5.6158e-08, 9.5562e-08, 1.1977e-07,\n",
       "            8.8678e-08, 8.5196e-08, 2.6891e-08, 7.9782e-07, 1.6402e-07, 2.3161e-08,\n",
       "            9.2948e-08, 3.0402e-08, 1.2414e-08, 1.7810e-08, 1.0124e-07, 2.2593e-07,\n",
       "            1.9776e-07, 2.7540e-07, 1.6891e-08, 1.7409e-07, 6.3375e-08, 4.2145e-08,\n",
       "            5.7027e-07, 6.2410e-08, 1.0272e-07, 1.2522e-07, 3.0740e-08, 7.0033e-08,\n",
       "            6.8117e-08, 9.6515e-10, 5.3526e-08, 1.0285e-09, 1.0552e-07, 3.7564e-08,\n",
       "            5.2801e-07, 2.4630e-07, 7.9962e-08, 5.6681e-08, 2.9742e-08, 9.8008e-08,\n",
       "            1.7615e-07, 3.5981e-08, 1.1884e-07, 1.1046e-07, 2.2479e-07, 3.1808e-07,\n",
       "            3.8218e-08, 4.6634e-08, 1.4645e-07, 3.3892e-08, 3.2329e-08, 4.5068e-08,\n",
       "            1.5166e-07, 5.0584e-07, 5.9251e-08, 8.3796e-08, 2.9553e-08, 6.7688e-08,\n",
       "            2.4889e-06, 3.5084e-08, 6.1936e-08, 4.0573e-09, 1.2549e-07, 1.9544e-07,\n",
       "            6.6717e-09, 4.0424e-07, 5.7290e-08, 4.0465e-08, 1.1792e-07, 5.6353e-09,\n",
       "            1.0703e-07, 7.2529e-08, 9.6097e-08, 3.3848e-08, 9.6959e-08, 1.7230e-07,\n",
       "            2.4674e-07, 2.4727e-07, 8.0741e-07, 2.1108e-07, 4.0579e-08, 8.1934e-08,\n",
       "            6.9268e-08, 1.3188e-07, 7.3807e-08, 3.2196e-07, 1.0836e-07, 7.8340e-08,\n",
       "            2.2565e-08, 3.6258e-08, 9.1041e-08, 8.5752e-07, 1.3834e-07, 2.2209e-07,\n",
       "            8.3457e-08, 9.5772e-08, 2.7541e-07, 1.7182e-07, 7.9443e-07, 1.9995e-07,\n",
       "            1.9344e-07, 3.7097e-07, 4.1643e-08, 1.4261e-07, 5.4985e-08, 3.5389e-08,\n",
       "            1.0181e-07, 5.5025e-08, 1.5763e-07, 4.6782e-08, 1.3800e-07, 5.2856e-08,\n",
       "            8.4542e-08, 9.6560e-08, 1.4180e-07, 7.0506e-08, 3.2747e-08, 5.6530e-07,\n",
       "            3.7158e-08, 1.3076e-07, 1.4881e-07, 1.9687e-08, 8.0959e-08, 9.2677e-08,\n",
       "            8.5565e-08, 1.4369e-07, 1.2568e-06, 2.2168e-08, 9.9564e-09, 4.3413e-08,\n",
       "            4.0752e-07, 3.5582e-08, 2.2769e-07, 7.1820e-08, 9.7605e-08, 5.6390e-08,\n",
       "            7.1009e-08, 5.2242e-08, 4.2738e-08, 1.6962e-07, 9.7236e-08, 1.8869e-08,\n",
       "            1.8009e-08, 2.2587e-08, 4.8286e-07, 6.9126e-07, 9.5341e-08, 6.5816e-08,\n",
       "            3.1703e-07, 5.6863e-08, 1.7048e-07, 3.4826e-08, 1.2509e-07, 2.9602e-08,\n",
       "            5.8270e-07, 9.2657e-08, 1.5906e-07, 3.9443e-07, 1.1075e-07, 9.7281e-08,\n",
       "            7.0413e-07, 1.8360e-07, 1.5827e-07, 6.2106e-08, 2.1534e-07, 2.3552e-07,\n",
       "            1.1642e-07, 4.8602e-08, 5.9269e-08, 6.6854e-08, 7.1324e-08, 6.1375e-08,\n",
       "            2.3935e-07, 8.1145e-08, 2.4295e-07, 1.4681e-07, 2.7976e-07, 6.1028e-08,\n",
       "            6.0333e-08, 6.6523e-08, 5.7650e-08, 2.4317e-07, 2.3352e-07, 1.2513e-07,\n",
       "            1.8796e-07, 8.5840e-08, 1.0970e-07, 9.1822e-08, 1.7486e-07, 9.0815e-08,\n",
       "            5.5077e-07, 3.8250e-07, 8.6878e-08, 1.2832e-08, 1.0126e-07, 3.0481e-08,\n",
       "            4.8868e-08, 2.4943e-07, 8.1011e-08, 5.5305e-08, 9.6857e-08, 4.6612e-08,\n",
       "            4.7888e-08, 6.2003e-08])},\n",
       "   5: {'step': 112320,\n",
       "    'exp_avg': tensor([[ 1.6002e-05,  1.0605e-05, -7.2670e-06,  ...,  2.2533e-06,\n",
       "              1.3301e-05,  4.1916e-06],\n",
       "            [ 1.1969e-05,  5.2912e-06, -1.4031e-05,  ..., -1.9242e-06,\n",
       "             -1.0343e-05, -6.5163e-06],\n",
       "            [-1.0789e-05, -2.1337e-05, -1.9638e-05,  ...,  8.2986e-06,\n",
       "             -1.4323e-05, -5.5923e-06],\n",
       "            ...,\n",
       "            [-1.9831e-06,  1.1816e-05,  3.1178e-06,  ..., -7.9903e-07,\n",
       "             -8.0808e-06,  6.1046e-06],\n",
       "            [ 1.4480e-05,  1.4038e-05,  1.6936e-06,  ..., -1.3775e-06,\n",
       "              1.8841e-05,  9.4492e-06],\n",
       "            [ 1.1474e-05,  1.3738e-05, -8.5094e-06,  ...,  1.3754e-05,\n",
       "             -1.0927e-06,  5.4097e-06]]),\n",
       "    'exp_avg_sq': tensor([[2.8409e-08, 8.4786e-09, 1.3123e-08,  ..., 3.1387e-08, 2.1566e-08,\n",
       "             1.8998e-08],\n",
       "            [6.1923e-08, 3.2172e-08, 1.6595e-08,  ..., 1.1474e-08, 4.0102e-09,\n",
       "             1.0301e-08],\n",
       "            [5.1309e-08, 7.7702e-09, 1.1909e-08,  ..., 9.0164e-09, 2.9036e-08,\n",
       "             8.9246e-09],\n",
       "            ...,\n",
       "            [4.3229e-09, 7.6269e-09, 1.3956e-08,  ..., 2.9565e-09, 4.8398e-08,\n",
       "             2.7932e-09],\n",
       "            [3.4200e-09, 3.4589e-09, 2.8340e-09,  ..., 3.6228e-09, 1.0213e-08,\n",
       "             2.2049e-09],\n",
       "            [3.7907e-09, 3.2948e-09, 2.6936e-09,  ..., 4.8167e-09, 2.8740e-09,\n",
       "             1.8310e-09]])},\n",
       "   6: {'step': 112320,\n",
       "    'exp_avg': tensor([[ 4.8735e-06,  2.6070e-05, -2.3722e-05,  ..., -4.5874e-06,\n",
       "              2.7692e-07,  2.2375e-05],\n",
       "            [-3.4450e-06,  1.8089e-05,  1.9172e-05,  ...,  1.7617e-06,\n",
       "              8.0952e-06, -1.7397e-05],\n",
       "            [ 4.1384e-06,  4.3438e-05, -3.0789e-05,  ...,  3.2599e-06,\n",
       "              8.1562e-06, -3.8396e-06],\n",
       "            ...,\n",
       "            [-9.5073e-06, -4.9919e-06, -1.3417e-07,  ..., -7.5113e-06,\n",
       "             -1.9596e-06,  8.9597e-08],\n",
       "            [ 5.5150e-06,  2.8519e-05, -1.6690e-05,  ..., -2.7694e-06,\n",
       "              2.0475e-05,  3.8713e-06],\n",
       "            [ 1.1308e-05, -4.0481e-06, -4.0288e-05,  ..., -6.8494e-06,\n",
       "              2.7751e-06, -5.1546e-06]]),\n",
       "    'exp_avg_sq': tensor([[2.7706e-08, 3.0417e-07, 6.5478e-08,  ..., 3.0387e-08, 9.9849e-08,\n",
       "             1.1969e-08],\n",
       "            [2.5802e-08, 1.5282e-07, 2.7477e-08,  ..., 7.2098e-08, 1.1905e-07,\n",
       "             6.5002e-09],\n",
       "            [2.0512e-08, 7.9805e-08, 5.3578e-08,  ..., 3.3724e-08, 1.0580e-07,\n",
       "             5.6290e-09],\n",
       "            ...,\n",
       "            [2.8480e-08, 8.6930e-08, 6.4618e-08,  ..., 2.0403e-08, 3.3043e-08,\n",
       "             4.1061e-09],\n",
       "            [6.5789e-09, 3.8661e-08, 3.2107e-08,  ..., 2.2196e-09, 4.7232e-09,\n",
       "             4.1596e-09],\n",
       "            [3.8932e-09, 2.2754e-08, 1.7902e-08,  ..., 2.0347e-09, 1.0615e-08,\n",
       "             9.9860e-09]])},\n",
       "   7: {'step': 112320,\n",
       "    'exp_avg': tensor([ 6.0548e-05, -9.7010e-05, -8.3944e-05, -4.1320e-05, -3.7831e-05,\n",
       "             2.7451e-05, -4.8566e-05, -1.8057e-05, -2.6701e-05, -9.9660e-06,\n",
       "            -1.3987e-04,  2.4815e-05,  3.2879e-05,  6.4836e-05, -3.4051e-05,\n",
       "            -3.6787e-06,  1.6368e-05,  5.8999e-06,  5.3062e-06, -2.6250e-05,\n",
       "             8.5808e-05, -1.4376e-05, -2.1932e-05,  3.3663e-05, -7.4079e-05,\n",
       "             2.6368e-05,  2.4358e-05,  3.1759e-05,  1.2363e-04, -9.1058e-06,\n",
       "             2.5951e-05, -3.4068e-05,  2.9474e-05,  8.2493e-06,  7.3142e-05,\n",
       "            -2.7058e-05,  1.0366e-05, -3.2113e-05, -8.7895e-05,  4.3737e-05,\n",
       "             2.0218e-05,  1.8233e-05, -6.1512e-06, -4.1312e-06,  2.1309e-05,\n",
       "             2.9305e-05, -3.0383e-05,  7.1716e-06, -1.0595e-05, -5.0379e-05,\n",
       "             2.0686e-05,  2.0675e-06,  2.7804e-05, -5.7153e-05,  1.6213e-05,\n",
       "             5.3626e-05, -1.8688e-05,  1.9062e-05, -5.0469e-05,  1.7757e-05,\n",
       "             9.6259e-06, -9.1001e-05,  3.2174e-05,  1.0255e-05, -4.5008e-05,\n",
       "             1.1641e-04, -4.4766e-05, -8.1621e-06, -1.1059e-05,  5.3002e-05,\n",
       "            -7.7786e-06, -4.2648e-05,  1.2643e-05,  1.0070e-05, -3.2922e-05,\n",
       "            -3.5917e-05,  5.5005e-05,  4.3254e-05, -6.5798e-06, -6.3789e-06,\n",
       "            -2.6138e-06, -3.7352e-05, -9.1708e-06,  3.2591e-05, -7.6807e-05,\n",
       "            -1.4306e-05, -2.2422e-05,  4.7361e-05,  1.9126e-05,  3.8867e-05,\n",
       "             3.2185e-05,  5.5397e-05,  4.7945e-06,  1.2186e-05, -1.8361e-05,\n",
       "            -6.1822e-05,  7.5596e-05,  7.6606e-05, -1.0489e-05,  4.0228e-05,\n",
       "             3.0973e-06,  4.4411e-05,  9.9911e-05, -4.5402e-06,  5.5509e-05,\n",
       "            -1.1857e-04, -2.2396e-05, -3.2688e-05,  2.5243e-05, -6.3588e-06,\n",
       "            -6.7468e-05, -2.2620e-05,  2.4571e-06,  6.2246e-05, -1.9179e-05,\n",
       "            -1.1263e-04,  5.5337e-06,  3.6413e-05,  1.1153e-05, -1.5695e-05,\n",
       "            -6.0888e-05, -1.7309e-05, -6.3748e-06,  4.3485e-05, -1.4911e-05,\n",
       "             3.8121e-06,  3.1869e-05,  4.1863e-05, -1.9800e-05, -4.6382e-05,\n",
       "             6.4157e-06, -2.8361e-05, -1.5926e-05, -4.0130e-05,  2.1203e-06,\n",
       "            -5.5906e-05,  2.9700e-05,  1.6068e-05, -5.3944e-05, -3.1217e-05,\n",
       "             1.4165e-05,  1.5303e-06,  2.5048e-05,  2.4445e-06,  3.5633e-05,\n",
       "            -5.9952e-06, -7.0218e-07, -3.3099e-05,  3.3946e-05,  1.1093e-05,\n",
       "            -1.5933e-05,  1.8887e-05,  8.3871e-06, -5.1268e-05,  4.5197e-05,\n",
       "             2.1077e-05,  4.9738e-05,  5.0828e-06, -7.0274e-06, -1.7890e-05,\n",
       "             7.5242e-06, -2.3576e-05, -1.8762e-05,  4.3196e-06,  9.7738e-06,\n",
       "            -1.7035e-05,  1.8138e-05,  9.1280e-06, -3.1412e-05,  8.9716e-06,\n",
       "            -6.5183e-06, -4.3260e-05, -5.9763e-06,  1.8465e-05, -2.9410e-06,\n",
       "             2.1948e-05,  2.7525e-05, -2.9737e-06, -3.6394e-05, -3.2436e-06,\n",
       "            -5.3685e-06,  2.6013e-06,  3.9395e-05,  1.2614e-05, -1.0541e-04,\n",
       "             8.2247e-07, -1.6175e-05,  1.0298e-04, -3.7025e-06,  6.3559e-05,\n",
       "             6.6854e-06, -3.7010e-05,  4.8879e-06,  6.2390e-05, -4.2141e-06,\n",
       "            -1.1360e-05, -7.7066e-06, -2.4498e-05, -1.7370e-05, -7.6477e-05,\n",
       "             4.0050e-06,  3.3984e-05,  3.1017e-06, -3.2281e-05,  1.7497e-05,\n",
       "            -1.1304e-05,  1.4866e-05, -1.9482e-05,  6.0161e-06,  1.0676e-05,\n",
       "             4.2254e-05,  2.8753e-05,  4.8093e-05,  1.3101e-05, -1.7951e-05,\n",
       "             3.2308e-05, -3.3538e-06,  6.4536e-05,  2.0394e-05,  1.5244e-05,\n",
       "             2.8903e-05,  3.7148e-05, -8.2057e-06,  2.0733e-06,  2.1110e-05,\n",
       "             5.3357e-06, -4.2377e-05,  1.5536e-06, -2.6986e-05, -2.1056e-05,\n",
       "             1.2314e-06,  5.2233e-06, -9.5300e-06,  6.0221e-05,  5.2820e-06,\n",
       "             1.4143e-05, -2.1957e-05,  1.0415e-05, -1.4417e-05,  2.2578e-05,\n",
       "             2.4667e-05, -1.6535e-05, -2.0463e-05, -4.9600e-06, -1.0108e-05,\n",
       "             3.1805e-06, -2.7325e-06, -9.1033e-07, -3.6555e-06,  4.4648e-06,\n",
       "             2.0208e-05, -1.2488e-05,  9.7314e-06,  1.8619e-06,  7.9421e-05,\n",
       "             1.0022e-05, -5.5368e-05,  2.9289e-04,  1.5273e-05,  1.7074e-04,\n",
       "             5.0906e-05,  2.2079e-05,  5.3332e-06, -4.6696e-06,  3.1815e-05,\n",
       "            -2.4220e-05,  5.3517e-05, -2.9749e-04,  1.5343e-05,  1.2386e-05,\n",
       "            -4.3989e-05,  3.8955e-05,  2.0575e-05,  2.8765e-05,  2.4757e-05,\n",
       "            -9.4451e-05, -5.2289e-06,  2.5132e-05, -8.9307e-05,  6.3629e-05,\n",
       "             5.7369e-05, -1.3301e-05, -1.5111e-04, -6.6884e-05, -1.1623e-05,\n",
       "             4.9693e-05,  1.2127e-04,  1.0911e-04,  1.1772e-04, -4.3962e-05,\n",
       "            -6.4748e-05,  3.6864e-05,  3.9992e-05, -6.8923e-05,  7.0759e-05,\n",
       "             1.2944e-04, -6.7132e-05, -5.1994e-06, -1.2572e-04, -1.2447e-04,\n",
       "             1.9134e-05,  2.9156e-05, -6.0726e-05, -5.3407e-07,  3.6939e-05,\n",
       "             6.0860e-07,  1.8421e-05,  7.8960e-05, -4.2219e-05, -7.4524e-05,\n",
       "            -4.7997e-06,  1.7916e-05, -6.4635e-05, -3.3040e-05, -1.3566e-07,\n",
       "            -5.7819e-07,  7.5916e-05,  4.2702e-05, -5.9092e-05, -1.6069e-04,\n",
       "            -4.9650e-06, -1.1034e-04,  1.3621e-04,  1.5644e-04, -8.0504e-05,\n",
       "             3.0202e-05, -9.6843e-05, -2.8929e-06,  1.9508e-05,  2.6810e-06,\n",
       "             2.8917e-05, -1.5629e-04,  1.0803e-04, -3.6093e-06, -2.1428e-05,\n",
       "             4.9909e-05,  8.1070e-06,  1.8108e-05,  1.9890e-05,  6.3039e-05,\n",
       "             7.0963e-05,  1.1475e-05, -1.2336e-06, -8.8276e-05, -6.1903e-05,\n",
       "            -2.1821e-05, -6.6102e-05,  8.2980e-05, -4.1466e-05, -9.9722e-06,\n",
       "            -2.4026e-05,  1.0345e-04, -5.3692e-05,  4.2847e-05,  1.8085e-04,\n",
       "             2.6661e-05, -4.5103e-05, -1.8894e-05,  6.0809e-05, -1.1865e-05,\n",
       "            -2.5201e-05,  4.0625e-05, -8.2084e-05,  3.1313e-04,  3.0483e-05,\n",
       "             5.3912e-05, -3.3113e-05, -2.1273e-05,  2.5335e-06, -5.5269e-05,\n",
       "             4.7882e-06,  2.4595e-05,  3.9208e-05,  1.4937e-05,  1.3467e-04,\n",
       "             7.7098e-05,  6.9835e-05, -1.5461e-05,  1.1094e-05,  9.1177e-06,\n",
       "            -5.7618e-06, -8.6183e-05,  1.2973e-05,  3.4738e-05,  5.3204e-05,\n",
       "             1.1591e-05, -2.8556e-05,  5.1084e-05, -5.8559e-05,  3.4216e-05,\n",
       "            -1.0646e-04,  2.3044e-05,  5.1178e-05,  4.6336e-05, -8.6107e-05,\n",
       "             8.8907e-05,  1.1655e-04,  3.9900e-05, -3.0758e-05, -9.1847e-05,\n",
       "            -2.9844e-05,  1.1198e-05, -1.7361e-05, -1.2159e-04,  6.3311e-05,\n",
       "            -3.7167e-05, -4.1820e-05,  2.2159e-05, -7.4988e-05, -2.4979e-05,\n",
       "            -1.9073e-05,  2.6911e-05,  4.3622e-05, -2.6911e-05,  1.0921e-05,\n",
       "            -6.4522e-05,  1.8602e-05,  4.9910e-05,  2.9025e-05, -4.3499e-05,\n",
       "            -8.9626e-06, -2.5009e-05, -1.2729e-04,  3.5661e-05, -2.5389e-05,\n",
       "            -6.0430e-05,  1.1994e-05, -2.8785e-05, -9.9368e-05, -1.4519e-05,\n",
       "            -3.2305e-05,  3.2749e-05, -6.1072e-05,  6.1217e-05, -1.4201e-05,\n",
       "             1.9928e-05,  3.7073e-05, -3.7480e-05, -1.2052e-06,  7.2910e-05,\n",
       "            -4.1407e-05,  3.0122e-05, -8.9992e-07,  1.2854e-04,  1.7572e-05,\n",
       "            -6.2895e-05,  2.2635e-05,  1.0553e-05, -6.6216e-05,  2.1715e-05,\n",
       "            -3.8922e-06, -5.1650e-05,  1.6874e-05,  6.6575e-05, -3.5955e-05,\n",
       "             8.5518e-06,  6.4080e-05,  2.6009e-05, -8.6295e-05, -4.3168e-05,\n",
       "             2.5208e-05, -5.3550e-05, -3.3318e-05, -4.0763e-05,  5.2791e-05,\n",
       "            -3.6540e-05,  2.3662e-07,  1.9180e-06, -1.1378e-05, -6.4529e-05,\n",
       "             1.0675e-05,  3.8293e-05,  4.9978e-05,  6.6610e-06, -2.9221e-05,\n",
       "             3.4484e-05,  3.0840e-05,  1.6562e-06,  1.5857e-05, -3.6997e-05,\n",
       "             4.5970e-05,  2.6942e-05, -7.9673e-05,  6.0150e-05,  1.0100e-06,\n",
       "             6.6793e-06,  6.7722e-05, -2.3619e-05, -4.9009e-06, -1.1887e-04,\n",
       "             3.2072e-05,  2.2419e-05,  3.8722e-05, -5.2309e-05, -1.1870e-04,\n",
       "             4.6292e-05,  8.5176e-05,  7.0827e-05, -1.6177e-05, -3.0875e-05,\n",
       "            -2.1803e-05, -3.6700e-07, -4.0430e-06, -2.8742e-05,  1.2352e-04,\n",
       "            -3.3951e-05, -4.1888e-05,  3.3572e-05,  1.6886e-05,  1.0391e-05,\n",
       "            -1.4723e-06,  1.2422e-05]),\n",
       "    'exp_avg_sq': tensor([6.3955e-07, 3.7978e-07, 3.0784e-07, 1.5028e-07, 2.5461e-07, 6.9260e-08,\n",
       "            1.6694e-07, 1.5531e-07, 2.5209e-07, 8.4758e-08, 3.4803e-07, 4.7317e-07,\n",
       "            7.8468e-08, 8.7970e-07, 1.1745e-07, 1.3446e-08, 1.4526e-07, 2.8925e-08,\n",
       "            7.5149e-08, 8.2928e-08, 7.1743e-08, 2.5485e-08, 1.9912e-07, 1.7738e-07,\n",
       "            1.0930e-07, 9.8665e-07, 6.7232e-08, 1.3088e-07, 3.7923e-07, 9.9969e-08,\n",
       "            2.1420e-07, 2.1709e-08, 6.1774e-08, 2.0893e-06, 2.3979e-07, 3.7236e-07,\n",
       "            1.9468e-08, 5.9445e-08, 1.6173e-07, 7.8004e-08, 7.9687e-08, 1.6275e-07,\n",
       "            3.6546e-08, 2.8423e-07, 2.6996e-08, 6.1832e-08, 8.8173e-09, 5.8849e-08,\n",
       "            1.8092e-07, 4.0661e-08, 9.4388e-08, 3.4868e-08, 6.5884e-08, 1.2187e-07,\n",
       "            5.7831e-07, 3.4540e-08, 7.7642e-08, 3.1880e-08, 1.3329e-07, 1.4059e-07,\n",
       "            8.2006e-08, 3.0140e-07, 7.6436e-08, 1.2017e-07, 1.2328e-07, 2.7453e-07,\n",
       "            2.7288e-07, 8.2501e-08, 2.8409e-07, 1.1269e-07, 4.7592e-08, 1.6620e-07,\n",
       "            6.6644e-09, 6.7979e-08, 2.5356e-08, 3.4334e-07, 2.9421e-08, 2.4783e-07,\n",
       "            8.2200e-08, 5.4463e-08, 3.7465e-08, 2.6932e-07, 1.4287e-07, 1.2837e-07,\n",
       "            1.9060e-07, 2.1416e-07, 2.8581e-07, 1.3353e-07, 5.5000e-08, 4.7041e-07,\n",
       "            4.5266e-07, 1.9834e-07, 3.5207e-08, 9.3076e-08, 1.8768e-07, 1.0503e-07,\n",
       "            1.2981e-07, 1.5049e-07, 1.1651e-07, 1.9674e-08, 1.0224e-07, 4.4597e-07,\n",
       "            2.8514e-07, 1.1791e-07, 4.7761e-08, 2.2665e-07, 1.0893e-06, 8.0664e-08,\n",
       "            9.8737e-08, 3.4277e-07, 2.8952e-08, 1.0374e-07, 2.3880e-08, 7.6590e-07,\n",
       "            4.0211e-08, 9.3276e-08, 2.7597e-07, 1.6078e-07, 5.8727e-07, 3.1971e-08,\n",
       "            3.8912e-07, 6.3005e-08, 1.7242e-08, 1.0634e-07, 2.8102e-08, 1.3399e-07,\n",
       "            3.1475e-07, 4.1138e-08, 1.4874e-07, 1.2977e-07, 1.6571e-07, 5.4042e-08,\n",
       "            5.3321e-08, 1.2261e-07, 6.1919e-08, 6.7067e-08, 1.4756e-07, 2.3024e-08,\n",
       "            2.7681e-07, 1.4174e-07, 1.3387e-07, 1.2116e-07, 8.6154e-09, 1.0587e-09,\n",
       "            2.9032e-08, 2.9823e-08, 5.0034e-08, 3.1265e-08, 9.5857e-08, 5.9623e-09,\n",
       "            1.2103e-07, 7.1736e-08, 9.3449e-08, 5.9057e-07, 1.0723e-07, 1.2015e-07,\n",
       "            1.6405e-06, 3.0367e-08, 1.8315e-08, 4.8770e-09, 5.9727e-08, 1.3206e-06,\n",
       "            5.8905e-08, 5.3429e-08, 3.5540e-08, 3.0579e-08, 1.2043e-07, 2.3685e-07,\n",
       "            4.0695e-08, 1.0958e-07, 6.5745e-08, 1.3205e-07, 1.8612e-09, 9.1378e-08,\n",
       "            7.4570e-09, 2.6372e-08, 5.0411e-08, 3.6355e-08, 8.3129e-08, 2.7633e-09,\n",
       "            2.0494e-08, 2.3522e-08, 6.8041e-08, 7.6912e-09, 1.5313e-08, 3.4653e-08,\n",
       "            2.0465e-07, 5.5064e-08, 7.5694e-08, 4.1221e-08, 3.3939e-08, 2.9286e-08,\n",
       "            3.8046e-08, 2.0211e-07, 2.4255e-07, 3.1031e-08, 7.1150e-08, 4.0211e-07,\n",
       "            4.2567e-08, 2.1464e-07, 2.1715e-09, 2.0133e-08, 4.3585e-09, 1.9838e-07,\n",
       "            2.7207e-08, 6.7883e-07, 4.4040e-09, 2.2066e-08, 2.2439e-08, 2.8938e-08,\n",
       "            2.4824e-08, 3.2431e-07, 4.7521e-08, 1.2423e-07, 1.1267e-07, 9.5428e-08,\n",
       "            1.5228e-07, 9.3832e-07, 6.4991e-07, 4.6565e-08, 2.5573e-08, 1.0358e-07,\n",
       "            3.0088e-08, 6.2435e-08, 8.9015e-08, 1.0390e-07, 5.7547e-08, 1.5346e-08,\n",
       "            5.2245e-08, 5.4275e-08, 4.8280e-07, 4.8569e-08, 1.7791e-07, 1.6401e-07,\n",
       "            3.0167e-08, 9.3003e-08, 1.2847e-07, 1.4116e-07, 1.9121e-08, 1.0799e-07,\n",
       "            3.1594e-08, 5.4587e-08, 2.0264e-08, 7.4101e-08, 4.3466e-08, 7.1302e-08,\n",
       "            7.3258e-07, 6.5835e-09, 1.5741e-07, 3.1569e-08, 1.2625e-08, 4.1797e-08,\n",
       "            1.9817e-08, 1.6390e-08, 9.8219e-08, 2.5405e-08, 1.2324e-06, 3.5293e-07,\n",
       "            1.3664e-07, 1.2566e-06, 5.6536e-07, 9.1567e-08, 6.5931e-07, 8.2217e-08,\n",
       "            1.3008e-06, 2.8609e-07, 1.4386e-07, 1.6310e-06, 3.8492e-07, 3.6218e-07,\n",
       "            1.0580e-07, 8.4586e-09, 7.4250e-07, 3.3755e-08, 3.7304e-07, 7.6788e-07,\n",
       "            2.9182e-07, 8.7020e-08, 6.8377e-08, 3.0714e-07, 1.7949e-06, 6.4725e-07,\n",
       "            7.5568e-07, 1.1333e-07, 5.8949e-07, 1.3160e-07, 1.4249e-07, 7.0509e-08,\n",
       "            4.1853e-06, 2.1283e-07, 1.7937e-07, 1.1565e-06, 6.9763e-07, 2.9881e-07,\n",
       "            7.1183e-07, 1.7952e-07, 4.0769e-07, 5.8894e-08, 8.9365e-08, 1.0288e-06,\n",
       "            3.2233e-08, 2.1086e-07, 4.4506e-08, 1.6352e-07, 4.2229e-07, 4.4708e-07,\n",
       "            1.0526e-06, 2.3665e-07, 3.0595e-07, 7.3055e-07, 1.5387e-07, 1.8417e-07,\n",
       "            5.0337e-08, 1.3358e-07, 2.0593e-07, 9.4341e-07, 4.9377e-07, 4.7388e-07,\n",
       "            5.8745e-07, 4.5419e-07, 6.4936e-08, 3.2599e-07, 5.5469e-07, 7.7632e-07,\n",
       "            1.8744e-06, 8.0285e-08, 4.0247e-07, 9.7962e-08, 9.7106e-08, 2.3389e-07,\n",
       "            5.8351e-08, 1.1733e-06, 1.9902e-07, 1.5800e-06, 2.2822e-08, 4.9499e-08,\n",
       "            1.3588e-07, 3.8227e-07, 3.0392e-07, 2.5259e-06, 2.2672e-07, 3.5311e-08,\n",
       "            1.8950e-07, 1.7277e-06, 4.6864e-07, 4.3668e-07, 2.3195e-06, 1.0413e-06,\n",
       "            3.8773e-07, 5.9300e-07, 1.6952e-07, 1.2093e-07, 1.6655e-07, 4.3214e-07,\n",
       "            8.5482e-07, 1.6151e-07, 1.0715e-07, 1.5160e-07, 4.7164e-08, 2.7109e-07,\n",
       "            1.0445e-07, 8.2780e-08, 2.8533e-07, 1.2671e-06, 2.7884e-07, 1.2863e-07,\n",
       "            1.2429e-07, 4.5732e-07, 4.9347e-08, 2.5536e-07, 2.5347e-07, 6.2637e-07,\n",
       "            2.7193e-07, 1.4466e-07, 2.2290e-07, 5.7285e-08, 7.8605e-08, 5.4652e-07,\n",
       "            6.4215e-08, 9.8907e-07, 2.8764e-07, 2.2061e-07, 6.9090e-07, 2.6621e-07,\n",
       "            3.7726e-08, 8.2648e-08, 2.8444e-07, 5.6704e-07, 1.3027e-07, 1.9671e-07,\n",
       "            3.3758e-07, 3.0960e-07, 5.2407e-07, 8.2007e-08, 1.8200e-07, 3.5717e-07,\n",
       "            1.3229e-07, 1.0105e-07, 7.0677e-07, 2.9900e-07, 1.8020e-07, 2.7170e-08,\n",
       "            2.9875e-08, 1.4355e-07, 9.5880e-08, 1.3048e-07, 3.0809e-07, 3.8056e-08,\n",
       "            1.0985e-07, 7.6962e-08, 1.4700e-07, 1.0725e-07, 2.6191e-07, 1.2827e-07,\n",
       "            1.5356e-07, 2.9745e-07, 5.7519e-08, 2.8701e-07, 2.4166e-08, 2.4304e-07,\n",
       "            3.0247e-07, 3.8381e-07, 2.7718e-07, 1.0193e-07, 4.0501e-08, 1.9146e-07,\n",
       "            5.3645e-07, 2.0526e-07, 2.1195e-06, 2.0906e-07, 2.2234e-07, 1.8499e-07,\n",
       "            1.6824e-07, 2.2903e-07, 8.0437e-08, 5.9746e-07, 1.7939e-07, 1.9062e-07,\n",
       "            3.3711e-08, 2.7054e-07, 1.7957e-07, 1.2989e-07, 8.2996e-08, 2.6271e-06,\n",
       "            1.4518e-07, 3.2469e-07, 7.1303e-08, 8.3628e-08, 2.5999e-07, 4.1118e-08,\n",
       "            8.6499e-08, 7.6385e-08, 6.6466e-08, 1.7341e-07, 1.5688e-07, 1.4275e-07,\n",
       "            4.3600e-07, 4.6208e-08, 1.6977e-07, 2.2767e-07, 1.1589e-07, 3.7488e-07,\n",
       "            1.9563e-07, 2.8346e-08, 1.4724e-07, 6.2811e-08, 7.5317e-07, 1.6487e-07,\n",
       "            1.3444e-07, 1.8627e-07, 3.7207e-08, 2.7934e-07, 2.1105e-07, 3.9719e-08,\n",
       "            1.4752e-07, 8.2422e-08, 2.8612e-07, 6.6516e-08, 1.1066e-07, 1.6557e-07,\n",
       "            7.7667e-08, 3.2875e-08, 4.7476e-07, 2.0488e-07, 1.0848e-07, 1.1446e-07,\n",
       "            2.1760e-07, 6.6029e-08, 5.9336e-08, 7.1466e-07, 9.1959e-07, 2.0420e-07,\n",
       "            4.0088e-07, 8.5926e-08, 1.3793e-07, 1.1508e-07, 3.4307e-07, 4.3625e-07,\n",
       "            5.8013e-08, 9.8507e-07, 6.8842e-08, 1.3574e-07, 1.7005e-07, 9.6541e-08,\n",
       "            1.9425e-07, 3.0684e-07, 4.2993e-07, 9.3385e-08, 1.8467e-07, 2.4121e-07,\n",
       "            1.0071e-07, 9.2781e-08])},\n",
       "   8: {'step': 112320,\n",
       "    'exp_avg': tensor([ 6.0548e-05, -9.7010e-05, -8.3944e-05, -4.1320e-05, -3.7831e-05,\n",
       "             2.7451e-05, -4.8566e-05, -1.8057e-05, -2.6701e-05, -9.9660e-06,\n",
       "            -1.3987e-04,  2.4815e-05,  3.2879e-05,  6.4836e-05, -3.4051e-05,\n",
       "            -3.6787e-06,  1.6368e-05,  5.8999e-06,  5.3062e-06, -2.6250e-05,\n",
       "             8.5808e-05, -1.4376e-05, -2.1932e-05,  3.3663e-05, -7.4079e-05,\n",
       "             2.6368e-05,  2.4358e-05,  3.1759e-05,  1.2363e-04, -9.1058e-06,\n",
       "             2.5951e-05, -3.4068e-05,  2.9474e-05,  8.2493e-06,  7.3142e-05,\n",
       "            -2.7058e-05,  1.0366e-05, -3.2113e-05, -8.7895e-05,  4.3737e-05,\n",
       "             2.0218e-05,  1.8233e-05, -6.1512e-06, -4.1312e-06,  2.1309e-05,\n",
       "             2.9305e-05, -3.0383e-05,  7.1716e-06, -1.0595e-05, -5.0379e-05,\n",
       "             2.0686e-05,  2.0675e-06,  2.7804e-05, -5.7153e-05,  1.6213e-05,\n",
       "             5.3626e-05, -1.8688e-05,  1.9062e-05, -5.0469e-05,  1.7757e-05,\n",
       "             9.6259e-06, -9.1001e-05,  3.2174e-05,  1.0255e-05, -4.5008e-05,\n",
       "             1.1641e-04, -4.4766e-05, -8.1621e-06, -1.1059e-05,  5.3002e-05,\n",
       "            -7.7786e-06, -4.2648e-05,  1.2643e-05,  1.0070e-05, -3.2922e-05,\n",
       "            -3.5917e-05,  5.5005e-05,  4.3254e-05, -6.5798e-06, -6.3789e-06,\n",
       "            -2.6138e-06, -3.7352e-05, -9.1708e-06,  3.2591e-05, -7.6807e-05,\n",
       "            -1.4306e-05, -2.2422e-05,  4.7361e-05,  1.9126e-05,  3.8867e-05,\n",
       "             3.2185e-05,  5.5397e-05,  4.7945e-06,  1.2186e-05, -1.8361e-05,\n",
       "            -6.1822e-05,  7.5596e-05,  7.6606e-05, -1.0489e-05,  4.0228e-05,\n",
       "             3.0973e-06,  4.4411e-05,  9.9911e-05, -4.5402e-06,  5.5509e-05,\n",
       "            -1.1857e-04, -2.2396e-05, -3.2688e-05,  2.5243e-05, -6.3588e-06,\n",
       "            -6.7468e-05, -2.2620e-05,  2.4571e-06,  6.2246e-05, -1.9179e-05,\n",
       "            -1.1263e-04,  5.5337e-06,  3.6413e-05,  1.1153e-05, -1.5695e-05,\n",
       "            -6.0888e-05, -1.7309e-05, -6.3748e-06,  4.3485e-05, -1.4911e-05,\n",
       "             3.8121e-06,  3.1869e-05,  4.1863e-05, -1.9800e-05, -4.6382e-05,\n",
       "             6.4157e-06, -2.8361e-05, -1.5926e-05, -4.0130e-05,  2.1203e-06,\n",
       "            -5.5906e-05,  2.9700e-05,  1.6068e-05, -5.3944e-05, -3.1217e-05,\n",
       "             1.4165e-05,  1.5303e-06,  2.5048e-05,  2.4445e-06,  3.5633e-05,\n",
       "            -5.9952e-06, -7.0218e-07, -3.3099e-05,  3.3946e-05,  1.1093e-05,\n",
       "            -1.5933e-05,  1.8887e-05,  8.3872e-06, -5.1268e-05,  4.5197e-05,\n",
       "             2.1077e-05,  4.9738e-05,  5.0828e-06, -7.0274e-06, -1.7890e-05,\n",
       "             7.5242e-06, -2.3576e-05, -1.8762e-05,  4.3196e-06,  9.7738e-06,\n",
       "            -1.7035e-05,  1.8138e-05,  9.1280e-06, -3.1412e-05,  8.9716e-06,\n",
       "            -6.5183e-06, -4.3260e-05, -5.9763e-06,  1.8465e-05, -2.9410e-06,\n",
       "             2.1948e-05,  2.7525e-05, -2.9737e-06, -3.6394e-05, -3.2436e-06,\n",
       "            -5.3685e-06,  2.6013e-06,  3.9395e-05,  1.2614e-05, -1.0541e-04,\n",
       "             8.2247e-07, -1.6175e-05,  1.0298e-04, -3.7025e-06,  6.3559e-05,\n",
       "             6.6854e-06, -3.7010e-05,  4.8879e-06,  6.2390e-05, -4.2141e-06,\n",
       "            -1.1360e-05, -7.7066e-06, -2.4498e-05, -1.7370e-05, -7.6477e-05,\n",
       "             4.0050e-06,  3.3984e-05,  3.1017e-06, -3.2281e-05,  1.7497e-05,\n",
       "            -1.1304e-05,  1.4866e-05, -1.9482e-05,  6.0161e-06,  1.0676e-05,\n",
       "             4.2254e-05,  2.8753e-05,  4.8093e-05,  1.3101e-05, -1.7951e-05,\n",
       "             3.2308e-05, -3.3538e-06,  6.4536e-05,  2.0394e-05,  1.5244e-05,\n",
       "             2.8903e-05,  3.7148e-05, -8.2057e-06,  2.0733e-06,  2.1110e-05,\n",
       "             5.3357e-06, -4.2377e-05,  1.5536e-06, -2.6986e-05, -2.1056e-05,\n",
       "             1.2314e-06,  5.2233e-06, -9.5300e-06,  6.0221e-05,  5.2820e-06,\n",
       "             1.4143e-05, -2.1957e-05,  1.0415e-05, -1.4417e-05,  2.2578e-05,\n",
       "             2.4667e-05, -1.6535e-05, -2.0463e-05, -4.9600e-06, -1.0108e-05,\n",
       "             3.1805e-06, -2.7325e-06, -9.1033e-07, -3.6555e-06,  4.4648e-06,\n",
       "             2.0208e-05, -1.2488e-05,  9.7314e-06,  1.8619e-06,  7.9421e-05,\n",
       "             1.0022e-05, -5.5368e-05,  2.9289e-04,  1.5273e-05,  1.7074e-04,\n",
       "             5.0906e-05,  2.2079e-05,  5.3332e-06, -4.6696e-06,  3.1815e-05,\n",
       "            -2.4220e-05,  5.3517e-05, -2.9749e-04,  1.5343e-05,  1.2386e-05,\n",
       "            -4.3989e-05,  3.8955e-05,  2.0575e-05,  2.8765e-05,  2.4757e-05,\n",
       "            -9.4451e-05, -5.2290e-06,  2.5132e-05, -8.9307e-05,  6.3629e-05,\n",
       "             5.7369e-05, -1.3301e-05, -1.5111e-04, -6.6884e-05, -1.1623e-05,\n",
       "             4.9693e-05,  1.2127e-04,  1.0911e-04,  1.1772e-04, -4.3962e-05,\n",
       "            -6.4748e-05,  3.6864e-05,  3.9992e-05, -6.8923e-05,  7.0759e-05,\n",
       "             1.2944e-04, -6.7132e-05, -5.1994e-06, -1.2572e-04, -1.2447e-04,\n",
       "             1.9134e-05,  2.9156e-05, -6.0726e-05, -5.3406e-07,  3.6939e-05,\n",
       "             6.0859e-07,  1.8421e-05,  7.8960e-05, -4.2219e-05, -7.4524e-05,\n",
       "            -4.7997e-06,  1.7916e-05, -6.4635e-05, -3.3040e-05, -1.3567e-07,\n",
       "            -5.7821e-07,  7.5916e-05,  4.2702e-05, -5.9092e-05, -1.6069e-04,\n",
       "            -4.9650e-06, -1.1034e-04,  1.3621e-04,  1.5644e-04, -8.0504e-05,\n",
       "             3.0202e-05, -9.6843e-05, -2.8929e-06,  1.9508e-05,  2.6810e-06,\n",
       "             2.8917e-05, -1.5629e-04,  1.0803e-04, -3.6093e-06, -2.1428e-05,\n",
       "             4.9909e-05,  8.1070e-06,  1.8108e-05,  1.9890e-05,  6.3039e-05,\n",
       "             7.0963e-05,  1.1475e-05, -1.2336e-06, -8.8276e-05, -6.1903e-05,\n",
       "            -2.1821e-05, -6.6102e-05,  8.2980e-05, -4.1466e-05, -9.9722e-06,\n",
       "            -2.4026e-05,  1.0345e-04, -5.3692e-05,  4.2847e-05,  1.8085e-04,\n",
       "             2.6661e-05, -4.5103e-05, -1.8894e-05,  6.0809e-05, -1.1866e-05,\n",
       "            -2.5201e-05,  4.0625e-05, -8.2084e-05,  3.1313e-04,  3.0483e-05,\n",
       "             5.3912e-05, -3.3113e-05, -2.1273e-05,  2.5335e-06, -5.5269e-05,\n",
       "             4.7882e-06,  2.4595e-05,  3.9208e-05,  1.4937e-05,  1.3467e-04,\n",
       "             7.7098e-05,  6.9835e-05, -1.5461e-05,  1.1094e-05,  9.1178e-06,\n",
       "            -5.7618e-06, -8.6183e-05,  1.2973e-05,  3.4738e-05,  5.3204e-05,\n",
       "             1.1591e-05, -2.8556e-05,  5.1084e-05, -5.8559e-05,  3.4216e-05,\n",
       "            -1.0646e-04,  2.3044e-05,  5.1178e-05,  4.6336e-05, -8.6107e-05,\n",
       "             8.8907e-05,  1.1655e-04,  3.9900e-05, -3.0758e-05, -9.1847e-05,\n",
       "            -2.9844e-05,  1.1198e-05, -1.7361e-05, -1.2159e-04,  6.3311e-05,\n",
       "            -3.7167e-05, -4.1820e-05,  2.2159e-05, -7.4988e-05, -2.4979e-05,\n",
       "            -1.9073e-05,  2.6911e-05,  4.3622e-05, -2.6911e-05,  1.0921e-05,\n",
       "            -6.4522e-05,  1.8602e-05,  4.9910e-05,  2.9025e-05, -4.3499e-05,\n",
       "            -8.9626e-06, -2.5009e-05, -1.2729e-04,  3.5661e-05, -2.5389e-05,\n",
       "            -6.0430e-05,  1.1994e-05, -2.8785e-05, -9.9368e-05, -1.4519e-05,\n",
       "            -3.2305e-05,  3.2749e-05, -6.1072e-05,  6.1217e-05, -1.4201e-05,\n",
       "             1.9928e-05,  3.7073e-05, -3.7480e-05, -1.2052e-06,  7.2910e-05,\n",
       "            -4.1407e-05,  3.0122e-05, -8.9993e-07,  1.2854e-04,  1.7572e-05,\n",
       "            -6.2895e-05,  2.2635e-05,  1.0553e-05, -6.6216e-05,  2.1715e-05,\n",
       "            -3.8922e-06, -5.1650e-05,  1.6874e-05,  6.6575e-05, -3.5955e-05,\n",
       "             8.5518e-06,  6.4079e-05,  2.6009e-05, -8.6295e-05, -4.3168e-05,\n",
       "             2.5208e-05, -5.3550e-05, -3.3318e-05, -4.0763e-05,  5.2791e-05,\n",
       "            -3.6540e-05,  2.3662e-07,  1.9180e-06, -1.1378e-05, -6.4529e-05,\n",
       "             1.0675e-05,  3.8293e-05,  4.9978e-05,  6.6610e-06, -2.9221e-05,\n",
       "             3.4484e-05,  3.0840e-05,  1.6562e-06,  1.5857e-05, -3.6997e-05,\n",
       "             4.5970e-05,  2.6942e-05, -7.9673e-05,  6.0150e-05,  1.0100e-06,\n",
       "             6.6793e-06,  6.7722e-05, -2.3619e-05, -4.9009e-06, -1.1887e-04,\n",
       "             3.2072e-05,  2.2419e-05,  3.8722e-05, -5.2309e-05, -1.1870e-04,\n",
       "             4.6292e-05,  8.5176e-05,  7.0827e-05, -1.6177e-05, -3.0875e-05,\n",
       "            -2.1803e-05, -3.6700e-07, -4.0430e-06, -2.8742e-05,  1.2352e-04,\n",
       "            -3.3951e-05, -4.1888e-05,  3.3572e-05,  1.6886e-05,  1.0391e-05,\n",
       "            -1.4723e-06,  1.2422e-05]),\n",
       "    'exp_avg_sq': tensor([6.3955e-07, 3.7978e-07, 3.0784e-07, 1.5028e-07, 2.5461e-07, 6.9260e-08,\n",
       "            1.6694e-07, 1.5531e-07, 2.5209e-07, 8.4758e-08, 3.4803e-07, 4.7317e-07,\n",
       "            7.8468e-08, 8.7970e-07, 1.1745e-07, 1.3446e-08, 1.4526e-07, 2.8925e-08,\n",
       "            7.5149e-08, 8.2928e-08, 7.1743e-08, 2.5485e-08, 1.9912e-07, 1.7738e-07,\n",
       "            1.0930e-07, 9.8665e-07, 6.7232e-08, 1.3088e-07, 3.7923e-07, 9.9969e-08,\n",
       "            2.1420e-07, 2.1709e-08, 6.1774e-08, 2.0893e-06, 2.3979e-07, 3.7236e-07,\n",
       "            1.9468e-08, 5.9445e-08, 1.6173e-07, 7.8004e-08, 7.9687e-08, 1.6275e-07,\n",
       "            3.6546e-08, 2.8423e-07, 2.6996e-08, 6.1832e-08, 8.8173e-09, 5.8849e-08,\n",
       "            1.8092e-07, 4.0661e-08, 9.4388e-08, 3.4868e-08, 6.5884e-08, 1.2187e-07,\n",
       "            5.7831e-07, 3.4540e-08, 7.7642e-08, 3.1880e-08, 1.3329e-07, 1.4059e-07,\n",
       "            8.2006e-08, 3.0140e-07, 7.6436e-08, 1.2017e-07, 1.2328e-07, 2.7453e-07,\n",
       "            2.7288e-07, 8.2501e-08, 2.8409e-07, 1.1269e-07, 4.7592e-08, 1.6620e-07,\n",
       "            6.6644e-09, 6.7979e-08, 2.5356e-08, 3.4334e-07, 2.9421e-08, 2.4783e-07,\n",
       "            8.2200e-08, 5.4463e-08, 3.7465e-08, 2.6932e-07, 1.4287e-07, 1.2837e-07,\n",
       "            1.9061e-07, 2.1416e-07, 2.8581e-07, 1.3353e-07, 5.5000e-08, 4.7041e-07,\n",
       "            4.5266e-07, 1.9834e-07, 3.5207e-08, 9.3076e-08, 1.8768e-07, 1.0503e-07,\n",
       "            1.2981e-07, 1.5049e-07, 1.1651e-07, 1.9674e-08, 1.0224e-07, 4.4597e-07,\n",
       "            2.8514e-07, 1.1791e-07, 4.7761e-08, 2.2665e-07, 1.0893e-06, 8.0664e-08,\n",
       "            9.8737e-08, 3.4277e-07, 2.8952e-08, 1.0374e-07, 2.3880e-08, 7.6590e-07,\n",
       "            4.0211e-08, 9.3276e-08, 2.7597e-07, 1.6078e-07, 5.8727e-07, 3.1971e-08,\n",
       "            3.8912e-07, 6.3005e-08, 1.7242e-08, 1.0634e-07, 2.8102e-08, 1.3399e-07,\n",
       "            3.1475e-07, 4.1138e-08, 1.4874e-07, 1.2977e-07, 1.6571e-07, 5.4042e-08,\n",
       "            5.3321e-08, 1.2261e-07, 6.1919e-08, 6.7067e-08, 1.4756e-07, 2.3024e-08,\n",
       "            2.7681e-07, 1.4174e-07, 1.3387e-07, 1.2116e-07, 8.6154e-09, 1.0587e-09,\n",
       "            2.9032e-08, 2.9823e-08, 5.0034e-08, 3.1265e-08, 9.5857e-08, 5.9623e-09,\n",
       "            1.2103e-07, 7.1736e-08, 9.3449e-08, 5.9057e-07, 1.0723e-07, 1.2015e-07,\n",
       "            1.6405e-06, 3.0367e-08, 1.8315e-08, 4.8770e-09, 5.9727e-08, 1.3206e-06,\n",
       "            5.8905e-08, 5.3429e-08, 3.5540e-08, 3.0579e-08, 1.2043e-07, 2.3685e-07,\n",
       "            4.0695e-08, 1.0958e-07, 6.5745e-08, 1.3205e-07, 1.8612e-09, 9.1378e-08,\n",
       "            7.4570e-09, 2.6372e-08, 5.0411e-08, 3.6355e-08, 8.3129e-08, 2.7633e-09,\n",
       "            2.0494e-08, 2.3522e-08, 6.8041e-08, 7.6912e-09, 1.5313e-08, 3.4653e-08,\n",
       "            2.0465e-07, 5.5064e-08, 7.5694e-08, 4.1221e-08, 3.3939e-08, 2.9286e-08,\n",
       "            3.8046e-08, 2.0211e-07, 2.4255e-07, 3.1031e-08, 7.1150e-08, 4.0211e-07,\n",
       "            4.2567e-08, 2.1464e-07, 2.1715e-09, 2.0133e-08, 4.3585e-09, 1.9838e-07,\n",
       "            2.7207e-08, 6.7883e-07, 4.4040e-09, 2.2066e-08, 2.2439e-08, 2.8938e-08,\n",
       "            2.4824e-08, 3.2431e-07, 4.7521e-08, 1.2423e-07, 1.1267e-07, 9.5428e-08,\n",
       "            1.5228e-07, 9.3832e-07, 6.4991e-07, 4.6565e-08, 2.5573e-08, 1.0358e-07,\n",
       "            3.0088e-08, 6.2435e-08, 8.9015e-08, 1.0390e-07, 5.7547e-08, 1.5346e-08,\n",
       "            5.2245e-08, 5.4275e-08, 4.8280e-07, 4.8569e-08, 1.7791e-07, 1.6401e-07,\n",
       "            3.0167e-08, 9.3003e-08, 1.2847e-07, 1.4116e-07, 1.9121e-08, 1.0799e-07,\n",
       "            3.1594e-08, 5.4587e-08, 2.0264e-08, 7.4101e-08, 4.3466e-08, 7.1302e-08,\n",
       "            7.3258e-07, 6.5835e-09, 1.5741e-07, 3.1569e-08, 1.2625e-08, 4.1797e-08,\n",
       "            1.9817e-08, 1.6390e-08, 9.8219e-08, 2.5405e-08, 1.2324e-06, 3.5293e-07,\n",
       "            1.3664e-07, 1.2566e-06, 5.6536e-07, 9.1567e-08, 6.5931e-07, 8.2217e-08,\n",
       "            1.3008e-06, 2.8609e-07, 1.4386e-07, 1.6310e-06, 3.8492e-07, 3.6218e-07,\n",
       "            1.0580e-07, 8.4586e-09, 7.4250e-07, 3.3755e-08, 3.7304e-07, 7.6788e-07,\n",
       "            2.9182e-07, 8.7020e-08, 6.8377e-08, 3.0714e-07, 1.7949e-06, 6.4725e-07,\n",
       "            7.5568e-07, 1.1333e-07, 5.8949e-07, 1.3160e-07, 1.4249e-07, 7.0509e-08,\n",
       "            4.1853e-06, 2.1283e-07, 1.7937e-07, 1.1565e-06, 6.9763e-07, 2.9881e-07,\n",
       "            7.1183e-07, 1.7952e-07, 4.0769e-07, 5.8894e-08, 8.9365e-08, 1.0288e-06,\n",
       "            3.2233e-08, 2.1086e-07, 4.4506e-08, 1.6352e-07, 4.2229e-07, 4.4708e-07,\n",
       "            1.0526e-06, 2.3665e-07, 3.0595e-07, 7.3055e-07, 1.5387e-07, 1.8417e-07,\n",
       "            5.0337e-08, 1.3358e-07, 2.0593e-07, 9.4341e-07, 4.9377e-07, 4.7388e-07,\n",
       "            5.8745e-07, 4.5419e-07, 6.4936e-08, 3.2599e-07, 5.5469e-07, 7.7632e-07,\n",
       "            1.8744e-06, 8.0285e-08, 4.0247e-07, 9.7962e-08, 9.7106e-08, 2.3389e-07,\n",
       "            5.8351e-08, 1.1733e-06, 1.9902e-07, 1.5800e-06, 2.2822e-08, 4.9499e-08,\n",
       "            1.3588e-07, 3.8227e-07, 3.0392e-07, 2.5259e-06, 2.2672e-07, 3.5311e-08,\n",
       "            1.8950e-07, 1.7277e-06, 4.6864e-07, 4.3668e-07, 2.3195e-06, 1.0413e-06,\n",
       "            3.8773e-07, 5.9300e-07, 1.6952e-07, 1.2093e-07, 1.6655e-07, 4.3214e-07,\n",
       "            8.5482e-07, 1.6151e-07, 1.0715e-07, 1.5160e-07, 4.7164e-08, 2.7109e-07,\n",
       "            1.0445e-07, 8.2780e-08, 2.8533e-07, 1.2671e-06, 2.7884e-07, 1.2863e-07,\n",
       "            1.2429e-07, 4.5732e-07, 4.9347e-08, 2.5536e-07, 2.5347e-07, 6.2637e-07,\n",
       "            2.7193e-07, 1.4466e-07, 2.2290e-07, 5.7285e-08, 7.8605e-08, 5.4652e-07,\n",
       "            6.4215e-08, 9.8907e-07, 2.8764e-07, 2.2061e-07, 6.9090e-07, 2.6621e-07,\n",
       "            3.7726e-08, 8.2648e-08, 2.8444e-07, 5.6704e-07, 1.3027e-07, 1.9671e-07,\n",
       "            3.3758e-07, 3.0960e-07, 5.2407e-07, 8.2007e-08, 1.8200e-07, 3.5717e-07,\n",
       "            1.3229e-07, 1.0105e-07, 7.0677e-07, 2.9900e-07, 1.8020e-07, 2.7170e-08,\n",
       "            2.9875e-08, 1.4355e-07, 9.5880e-08, 1.3048e-07, 3.0809e-07, 3.8056e-08,\n",
       "            1.0985e-07, 7.6962e-08, 1.4700e-07, 1.0725e-07, 2.6191e-07, 1.2827e-07,\n",
       "            1.5356e-07, 2.9745e-07, 5.7519e-08, 2.8701e-07, 2.4166e-08, 2.4304e-07,\n",
       "            3.0247e-07, 3.8381e-07, 2.7718e-07, 1.0193e-07, 4.0501e-08, 1.9146e-07,\n",
       "            5.3645e-07, 2.0526e-07, 2.1195e-06, 2.0906e-07, 2.2234e-07, 1.8499e-07,\n",
       "            1.6824e-07, 2.2903e-07, 8.0437e-08, 5.9746e-07, 1.7939e-07, 1.9062e-07,\n",
       "            3.3711e-08, 2.7054e-07, 1.7957e-07, 1.2989e-07, 8.2996e-08, 2.6271e-06,\n",
       "            1.4518e-07, 3.2469e-07, 7.1303e-08, 8.3628e-08, 2.5999e-07, 4.1118e-08,\n",
       "            8.6499e-08, 7.6385e-08, 6.6466e-08, 1.7341e-07, 1.5688e-07, 1.4275e-07,\n",
       "            4.3600e-07, 4.6208e-08, 1.6977e-07, 2.2767e-07, 1.1589e-07, 3.7488e-07,\n",
       "            1.9563e-07, 2.8346e-08, 1.4724e-07, 6.2811e-08, 7.5317e-07, 1.6487e-07,\n",
       "            1.3444e-07, 1.8627e-07, 3.7207e-08, 2.7934e-07, 2.1105e-07, 3.9719e-08,\n",
       "            1.4752e-07, 8.2422e-08, 2.8612e-07, 6.6516e-08, 1.1066e-07, 1.6557e-07,\n",
       "            7.7667e-08, 3.2875e-08, 4.7476e-07, 2.0488e-07, 1.0848e-07, 1.1446e-07,\n",
       "            2.1760e-07, 6.6029e-08, 5.9336e-08, 7.1466e-07, 9.1959e-07, 2.0420e-07,\n",
       "            4.0088e-07, 8.5926e-08, 1.3793e-07, 1.1508e-07, 3.4307e-07, 4.3625e-07,\n",
       "            5.8013e-08, 9.8507e-07, 6.8842e-08, 1.3574e-07, 1.7005e-07, 9.6541e-08,\n",
       "            1.9425e-07, 3.0684e-07, 4.2993e-07, 9.3385e-08, 1.8467e-07, 2.4121e-07,\n",
       "            1.0071e-07, 9.2781e-08])},\n",
       "   9: {'step': 112320,\n",
       "    'exp_avg': tensor([[-5.6052e-45, -5.6052e-45, -5.6052e-45,  ...,  5.6052e-45,\n",
       "             -5.6052e-45,  5.6052e-45],\n",
       "            [ 1.4458e-05, -2.8310e-06, -1.7493e-05,  ..., -3.7849e-05,\n",
       "             -7.9887e-06, -1.7273e-06],\n",
       "            [-2.3920e-04,  9.1273e-05,  3.5507e-05,  ...,  9.5276e-05,\n",
       "              1.0602e-04,  6.4173e-04],\n",
       "            ...,\n",
       "            [-1.8500e-07, -6.3379e-07,  4.8341e-06,  ..., -2.8779e-06,\n",
       "             -5.8740e-06, -9.2524e-06],\n",
       "            [-2.7787e-04,  1.1159e-05, -4.1800e-04,  ..., -2.3599e-05,\n",
       "              2.6689e-04, -2.4374e-04],\n",
       "            [-1.6816e-44, -5.3249e-44, -1.2612e-44,  ..., -5.6052e-45,\n",
       "             -5.6052e-45,  8.4078e-45]]),\n",
       "    'exp_avg_sq': tensor([[7.0065e-43, 7.0065e-43, 7.0065e-43,  ..., 7.0065e-43, 7.0065e-43,\n",
       "             7.0065e-43],\n",
       "            [1.6373e-08, 1.1351e-08, 2.8606e-08,  ..., 2.0458e-08, 4.0828e-08,\n",
       "             6.8007e-08],\n",
       "            [7.7213e-07, 4.2688e-07, 6.3236e-07,  ..., 3.9842e-07, 8.2345e-07,\n",
       "             4.4101e-07],\n",
       "            ...,\n",
       "            [7.6578e-09, 6.2952e-09, 8.6057e-09,  ..., 6.9564e-09, 1.4934e-08,\n",
       "             4.8829e-09],\n",
       "            [1.1447e-07, 9.9530e-07, 5.1645e-07,  ..., 1.2555e-06, 2.7451e-07,\n",
       "             1.6349e-07],\n",
       "            [2.4472e-15, 5.3073e-14, 3.2541e-18,  ..., 1.9022e-17, 6.1121e-17,\n",
       "             1.1637e-18]])},\n",
       "   10: {'step': 112320,\n",
       "    'exp_avg': tensor([[ 3.9221e-06, -5.0398e-07, -4.4145e-07,  ...,  3.5323e-06,\n",
       "              5.9817e-06, -1.4413e-06],\n",
       "            [ 1.4193e-06,  3.8893e-06,  4.3408e-07,  ...,  1.5716e-06,\n",
       "              6.7921e-06,  6.5805e-07],\n",
       "            [ 2.9501e-06,  1.0287e-06, -6.3096e-06,  ...,  1.5501e-06,\n",
       "             -5.4956e-07, -3.4897e-06],\n",
       "            ...,\n",
       "            [-1.8518e-05, -3.8970e-06, -4.0610e-06,  ..., -1.5617e-06,\n",
       "             -3.9872e-06,  2.3263e-07],\n",
       "            [ 1.0005e-05,  1.0653e-05,  2.2193e-07,  ...,  1.8324e-07,\n",
       "              8.5037e-06, -8.2800e-06],\n",
       "            [-6.8217e-07, -2.0270e-05,  1.3206e-05,  ..., -4.8152e-07,\n",
       "              3.9914e-07,  3.6199e-06]]),\n",
       "    'exp_avg_sq': tensor([[1.1707e-09, 1.2490e-09, 5.8972e-10,  ..., 9.8908e-10, 2.1796e-09,\n",
       "             2.2418e-10],\n",
       "            [5.9477e-10, 2.5521e-10, 3.3419e-10,  ..., 4.5325e-10, 1.6502e-09,\n",
       "             7.5603e-10],\n",
       "            [1.9293e-09, 1.8959e-09, 1.5043e-09,  ..., 1.0386e-09, 6.2096e-08,\n",
       "             1.7046e-08],\n",
       "            ...,\n",
       "            [7.1537e-09, 8.2524e-09, 4.5514e-09,  ..., 1.3344e-09, 4.6376e-08,\n",
       "             2.9768e-09],\n",
       "            [1.8358e-09, 2.8784e-09, 1.6658e-09,  ..., 1.2059e-08, 1.8559e-08,\n",
       "             5.3899e-09],\n",
       "            [6.5443e-10, 1.3433e-09, 1.1848e-09,  ..., 1.3480e-09, 3.9345e-09,\n",
       "             9.2872e-10]])},\n",
       "   11: {'step': 112320,\n",
       "    'exp_avg': tensor([[-7.3270e-08,  1.7224e-05, -2.6086e-06,  ..., -2.2878e-05,\n",
       "              1.1988e-05, -1.1120e-05],\n",
       "            [-4.2222e-06, -6.7619e-06, -9.8196e-06,  ..., -1.5844e-06,\n",
       "              2.2522e-06,  2.5158e-07],\n",
       "            [ 1.1274e-05, -1.5569e-06, -1.3054e-05,  ..., -5.3959e-06,\n",
       "             -1.3089e-05,  1.0966e-05],\n",
       "            ...,\n",
       "            [-1.1086e-05, -2.8539e-07,  1.0221e-05,  ...,  4.7868e-06,\n",
       "             -1.1080e-06, -3.3009e-05],\n",
       "            [ 2.9153e-06, -1.7856e-05,  7.1806e-06,  ...,  3.6409e-07,\n",
       "             -7.9420e-06,  2.3521e-06],\n",
       "            [-1.5423e-05, -1.3771e-05,  1.7548e-05,  ..., -4.5380e-06,\n",
       "              9.1007e-06,  4.0261e-05]]),\n",
       "    'exp_avg_sq': tensor([[3.1382e-09, 2.8137e-09, 7.2635e-09,  ..., 5.4766e-09, 1.2013e-09,\n",
       "             8.7841e-09],\n",
       "            [6.5398e-10, 5.4347e-10, 6.9601e-10,  ..., 1.0659e-10, 1.4047e-10,\n",
       "             7.9465e-11],\n",
       "            [1.5275e-09, 3.2283e-09, 3.2827e-08,  ..., 7.9535e-10, 1.2917e-09,\n",
       "             1.9018e-09],\n",
       "            ...,\n",
       "            [6.0873e-09, 2.4300e-08, 1.1919e-08,  ..., 1.1980e-08, 4.4019e-09,\n",
       "             1.5065e-08],\n",
       "            [2.0064e-09, 1.8570e-08, 8.2949e-09,  ..., 4.5287e-10, 8.1779e-09,\n",
       "             1.4274e-09],\n",
       "            [2.7306e-09, 3.6248e-09, 4.0233e-09,  ..., 2.6609e-09, 1.4347e-09,\n",
       "             4.6954e-09]])},\n",
       "   12: {'step': 112320,\n",
       "    'exp_avg': tensor([ 3.8900e-05,  4.2658e-06,  5.2999e-06,  ..., -1.5869e-05,\n",
       "            -5.4226e-05,  5.0745e-05]),\n",
       "    'exp_avg_sq': tensor([5.1501e-08, 9.5239e-09, 3.3903e-07,  ..., 1.7379e-07, 6.0938e-08,\n",
       "            4.1918e-08])},\n",
       "   13: {'step': 112320,\n",
       "    'exp_avg': tensor([ 3.8900e-05,  4.2658e-06,  5.2999e-06,  ..., -1.5869e-05,\n",
       "            -5.4226e-05,  5.0745e-05]),\n",
       "    'exp_avg_sq': tensor([5.1501e-08, 9.5239e-09, 3.3903e-07,  ..., 1.7379e-07, 6.0938e-08,\n",
       "            4.1918e-08])},\n",
       "   14: {'step': 112320,\n",
       "    'exp_avg': tensor([[-1.2022e-05, -3.6811e-06,  1.1051e-05,  ...,  3.2198e-06,\n",
       "             -3.1543e-05,  7.7045e-07],\n",
       "            [-8.7234e-06,  3.8874e-06,  1.5527e-06,  ...,  1.7134e-06,\n",
       "              2.2800e-06, -9.0340e-07],\n",
       "            [ 8.6751e-06, -1.8707e-05,  5.2355e-06,  ...,  6.3021e-06,\n",
       "             -3.6516e-07, -2.7649e-06],\n",
       "            ...,\n",
       "            [ 2.2036e-06, -2.4840e-06, -1.3231e-07,  ..., -9.7059e-07,\n",
       "              3.1741e-06,  1.4158e-06],\n",
       "            [-2.0884e-06,  1.6375e-06,  1.1166e-05,  ...,  2.9328e-06,\n",
       "              1.3111e-06,  2.8551e-08],\n",
       "            [ 1.3608e-06,  5.8837e-06,  1.4801e-06,  ...,  5.1620e-06,\n",
       "             -3.2227e-06, -3.0474e-07]]),\n",
       "    'exp_avg_sq': tensor([[1.1561e-08, 2.1867e-09, 1.0134e-08,  ..., 3.6452e-09, 2.5739e-08,\n",
       "             6.2247e-09],\n",
       "            [6.8552e-08, 9.1451e-08, 1.0753e-07,  ..., 4.0784e-08, 1.6467e-07,\n",
       "             1.5117e-08],\n",
       "            [3.2803e-07, 2.8658e-08, 3.7010e-07,  ..., 9.7546e-08, 7.0506e-07,\n",
       "             8.1806e-09],\n",
       "            ...,\n",
       "            [3.3030e-09, 8.5258e-09, 4.3891e-09,  ..., 2.9854e-10, 5.9915e-09,\n",
       "             1.2793e-09],\n",
       "            [8.1413e-09, 3.5291e-09, 6.3720e-09,  ..., 4.8794e-09, 1.2048e-08,\n",
       "             2.6545e-09],\n",
       "            [6.2220e-09, 2.2451e-09, 6.2167e-09,  ..., 2.2593e-09, 8.4804e-09,\n",
       "             4.6548e-09]])},\n",
       "   15: {'step': 112320,\n",
       "    'exp_avg': tensor([ 3.1302e-11, -1.1405e-13, -5.0033e-11,  5.5718e-12, -2.4050e-11,\n",
       "             3.7536e-12,  2.6828e-11,  2.4045e-10,  4.1660e-11, -5.3284e-11,\n",
       "            -1.3901e-11,  1.3937e-11, -1.7344e-11,  8.2338e-11, -6.0258e-11,\n",
       "             2.0887e-11, -1.0331e-11,  1.9259e-11,  6.5132e-11, -5.7368e-12,\n",
       "            -4.4613e-11,  1.0964e-11, -3.5926e-12,  1.1618e-11, -1.4906e-11,\n",
       "             6.4756e-11, -2.0190e-11,  9.9675e-11, -3.3078e-11,  1.6135e-11,\n",
       "             2.5571e-11,  1.0293e-10, -4.8096e-11,  2.1736e-11, -6.0628e-11,\n",
       "            -2.6700e-11,  4.2061e-11,  2.0426e-11,  1.6946e-12, -1.6528e-11,\n",
       "             2.1717e-11, -6.0624e-12,  1.8766e-11, -6.6106e-12,  8.0883e-11,\n",
       "             3.1242e-12, -7.2593e-12, -1.1425e-11, -1.2429e-10,  4.5665e-11,\n",
       "            -2.3814e-11,  4.8084e-12, -4.1092e-11, -2.3151e-12,  5.8820e-12,\n",
       "             2.9707e-11,  1.8222e-11,  4.5394e-12, -7.8936e-12, -6.1867e-13,\n",
       "            -1.9211e-11, -4.2731e-11, -2.1850e-11, -3.2004e-11, -5.5311e-11,\n",
       "             3.4895e-11, -3.7870e-11, -2.8069e-12,  3.8533e-11,  7.6875e-12,\n",
       "             3.0110e-11, -4.3381e-11, -2.4062e-12, -1.2051e-11,  2.5646e-11,\n",
       "            -1.4519e-11,  1.7754e-11, -8.9729e-12, -1.0495e-10,  1.1285e-12,\n",
       "             1.1095e-11,  2.2789e-10, -1.1278e-12, -5.6679e-12,  2.3749e-12,\n",
       "             4.2733e-11,  4.6562e-11, -5.4082e-11, -4.6426e-11,  1.0860e-12,\n",
       "             3.1848e-12, -1.6923e-12, -2.0452e-11, -2.3810e-11,  2.1531e-11,\n",
       "            -8.4372e-12,  7.3115e-12, -1.0047e-12, -4.5299e-11, -7.0156e-11,\n",
       "            -4.9625e-11,  1.9344e-11, -4.3759e-11,  1.3817e-11,  1.0549e-10,\n",
       "             8.0734e-12, -2.7249e-11,  2.7923e-11, -4.5095e-12, -4.4471e-15,\n",
       "            -3.6961e-11, -3.1056e-11, -3.6389e-11, -4.1329e-11,  2.3776e-11,\n",
       "             3.0814e-11, -2.4124e-11,  4.1036e-11, -1.1946e-11,  1.7686e-12,\n",
       "            -2.6444e-11,  1.6724e-10,  4.0890e-11, -2.0015e-11, -1.1850e-10,\n",
       "            -8.7580e-11, -2.5959e-11,  2.2131e-11, -1.8550e-11, -4.2374e-11,\n",
       "            -2.1446e-11,  1.1193e-11, -1.6379e-11, -5.5610e-11,  4.8538e-12,\n",
       "             4.5524e-12, -2.8271e-11,  1.7961e-11,  4.5269e-12,  2.4632e-11,\n",
       "            -3.7841e-11,  1.2040e-11,  7.0558e-13, -1.2804e-11,  3.5205e-12,\n",
       "             1.3184e-11,  3.0051e-11,  2.6942e-11, -3.6542e-11,  1.1850e-12,\n",
       "            -1.6997e-11,  8.6065e-12, -1.6833e-10, -2.8173e-11,  2.0577e-11,\n",
       "            -2.0324e-11, -1.8701e-12, -4.4966e-12,  3.7821e-11,  2.8534e-11,\n",
       "             1.2667e-11,  1.2473e-11,  1.1296e-11, -6.2668e-12,  3.2000e-11,\n",
       "             8.9466e-12, -1.1909e-11,  2.4813e-11, -2.9566e-11, -1.8950e-11,\n",
       "             2.6130e-12, -7.9038e-12,  9.5154e-11, -6.4393e-11, -5.7785e-11,\n",
       "            -1.5185e-11,  2.0157e-11, -4.9778e-11, -5.9441e-12, -1.9103e-11,\n",
       "             1.0705e-10, -9.8090e-12,  1.4441e-11, -4.6096e-11,  6.2967e-11,\n",
       "             4.4052e-11, -5.1826e-11, -3.0794e-11,  2.2576e-11,  2.5468e-11,\n",
       "            -1.6761e-11, -6.1128e-12, -2.5302e-11,  8.8012e-12, -2.8610e-11,\n",
       "            -3.7751e-11,  1.9773e-11, -8.8331e-12, -2.8509e-11, -1.9395e-12,\n",
       "             2.7651e-10, -2.5348e-11,  1.7037e-11,  2.1302e-11, -9.1215e-12,\n",
       "             1.2668e-12,  1.6997e-10,  2.3237e-11,  2.4922e-11, -1.3012e-11,\n",
       "            -2.9848e-11, -3.8896e-11, -2.5239e-11, -1.0343e-10, -5.9800e-12,\n",
       "             3.5184e-12, -1.3044e-10,  1.5758e-11, -2.3402e-10, -5.6948e-12,\n",
       "            -1.5579e-11, -7.9816e-13,  2.0845e-11,  3.2256e-11, -1.0227e-11,\n",
       "            -8.9418e-11,  1.5657e-10, -6.5080e-11, -2.1838e-11,  1.1498e-10,\n",
       "            -8.5772e-12,  1.8459e-11,  3.2247e-11,  1.1679e-11,  3.7214e-12,\n",
       "             4.5602e-13,  2.4251e-11, -2.3431e-11, -1.9020e-11, -2.7132e-11,\n",
       "            -4.7892e-11,  1.2784e-10, -3.4686e-11, -5.5596e-11,  3.1996e-11,\n",
       "             1.4364e-11,  2.2071e-11,  3.4146e-11, -3.8183e-12, -5.3021e-12,\n",
       "            -3.2883e-11,  2.2378e-11, -3.5536e-12,  3.2781e-11,  6.2236e-11,\n",
       "             2.2398e-11]),\n",
       "    'exp_avg_sq': tensor([5.2158e-20, 6.1430e-20, 4.1015e-20, 1.9670e-20, 2.0291e-20, 1.4290e-19,\n",
       "            3.3671e-20, 6.3486e-20, 4.3820e-20, 9.4787e-20, 2.6180e-20, 1.6548e-20,\n",
       "            1.9526e-20, 1.9936e-20, 1.0414e-19, 7.8583e-20, 5.3641e-20, 1.0248e-20,\n",
       "            2.9202e-20, 2.5336e-20, 2.7811e-20, 2.4134e-20, 1.1574e-19, 2.4032e-20,\n",
       "            3.2414e-20, 3.0958e-20, 1.7583e-20, 7.9867e-20, 5.3478e-20, 2.6815e-20,\n",
       "            3.2144e-20, 7.8582e-20, 3.5849e-20, 6.6984e-20, 2.0656e-20, 7.8600e-20,\n",
       "            6.3054e-20, 9.7136e-20, 1.5913e-20, 4.2461e-20, 1.3866e-20, 1.2338e-20,\n",
       "            1.3463e-19, 2.3036e-20, 6.8871e-20, 3.1345e-20, 2.9155e-20, 2.4761e-20,\n",
       "            6.6985e-20, 1.6362e-19, 1.1210e-20, 8.5149e-20, 1.6735e-19, 4.2274e-21,\n",
       "            1.5217e-20, 2.2233e-20, 1.8171e-20, 9.4460e-20, 1.8570e-20, 1.3564e-20,\n",
       "            4.0061e-20, 6.4042e-21, 2.8429e-20, 3.9682e-20, 2.1206e-19, 6.9305e-20,\n",
       "            3.9004e-20, 3.7350e-21, 1.8310e-20, 1.7027e-20, 3.6932e-20, 2.2355e-20,\n",
       "            2.1034e-20, 2.0080e-20, 2.3124e-20, 3.1867e-20, 7.4532e-20, 5.3306e-20,\n",
       "            4.2131e-20, 1.7232e-19, 8.4622e-21, 5.2454e-19, 2.7574e-20, 9.9128e-21,\n",
       "            8.8914e-21, 5.4706e-20, 2.9728e-20, 9.9021e-21, 3.9103e-20, 1.1200e-20,\n",
       "            6.6400e-20, 1.7436e-19, 1.1302e-19, 1.3992e-20, 6.3368e-20, 8.2468e-21,\n",
       "            1.5515e-20, 8.3695e-20, 1.9607e-20, 3.1183e-20, 1.5485e-20, 9.0784e-20,\n",
       "            4.1436e-20, 3.6980e-21, 4.1605e-20, 9.1568e-20, 2.8802e-20, 7.8030e-20,\n",
       "            2.6613e-20, 1.1099e-20, 4.4706e-20, 8.7330e-20, 3.6433e-20, 2.5656e-20,\n",
       "            5.6111e-20, 1.9157e-20, 2.9212e-20, 6.2494e-20, 2.5686e-20, 3.8989e-20,\n",
       "            2.2656e-20, 8.3742e-20, 1.5418e-19, 8.1640e-20, 6.2951e-20, 7.4939e-20,\n",
       "            5.4240e-20, 1.8615e-19, 4.6915e-20, 2.1306e-20, 6.7733e-20, 1.9529e-20,\n",
       "            1.5098e-20, 1.0077e-19, 1.3030e-20, 4.2607e-20, 1.0602e-19, 1.0196e-20,\n",
       "            1.6670e-20, 1.2420e-19, 4.4549e-20, 1.5268e-20, 6.0842e-20, 1.8323e-20,\n",
       "            4.3510e-21, 7.4461e-20, 3.9471e-20, 3.7087e-20, 4.8886e-20, 7.5995e-21,\n",
       "            1.4244e-20, 2.3114e-20, 6.6941e-20, 5.3765e-20, 6.1247e-21, 2.0414e-20,\n",
       "            1.7588e-20, 2.3986e-20, 4.4038e-20, 3.4173e-20, 3.8587e-20, 1.9373e-20,\n",
       "            2.4237e-20, 1.1427e-20, 1.1958e-20, 1.3241e-20, 2.1053e-20, 5.8544e-20,\n",
       "            8.4495e-21, 2.5213e-20, 3.4123e-20, 7.6107e-20, 2.5500e-19, 8.6779e-20,\n",
       "            6.5656e-20, 7.3097e-21, 2.3280e-20, 9.1483e-20, 1.8785e-20, 4.7999e-20,\n",
       "            5.8501e-20, 2.3444e-20, 2.9763e-20, 5.9920e-20, 1.5465e-19, 2.7249e-20,\n",
       "            2.9685e-20, 2.0763e-20, 4.3631e-20, 5.1075e-20, 8.6774e-20, 4.7175e-20,\n",
       "            7.2981e-20, 8.9256e-20, 2.1826e-20, 1.3240e-20, 4.2725e-20, 7.8193e-20,\n",
       "            5.9373e-20, 1.1739e-20, 5.6394e-19, 1.5752e-20, 3.7481e-20, 9.5051e-21,\n",
       "            7.3058e-20, 3.0562e-20, 1.2812e-19, 5.5677e-20, 2.8832e-20, 1.0839e-20,\n",
       "            3.7702e-20, 1.0640e-20, 1.3875e-20, 3.5006e-20, 3.5171e-20, 4.4576e-20,\n",
       "            7.6545e-20, 2.3049e-20, 3.8177e-20, 1.1389e-20, 2.2477e-20, 5.9075e-20,\n",
       "            7.9588e-21, 2.8202e-20, 2.0529e-20, 4.6383e-20, 2.3293e-20, 6.9111e-20,\n",
       "            1.5666e-20, 5.5786e-20, 1.9484e-20, 2.9249e-20, 9.6035e-21, 1.1006e-20,\n",
       "            2.9924e-20, 1.6981e-20, 1.4915e-19, 3.3816e-20, 3.8553e-20, 1.4059e-19,\n",
       "            1.0127e-19, 2.0199e-20, 4.5054e-20, 1.5807e-19, 3.9721e-20, 2.5676e-20,\n",
       "            2.9622e-20, 2.9860e-20, 5.4003e-20, 3.8040e-20, 6.5918e-20, 1.8662e-20,\n",
       "            9.8701e-20, 7.0709e-21, 3.7614e-20, 2.7399e-20])},\n",
       "   16: {'step': 112320,\n",
       "    'exp_avg': tensor([[-3.7959e-12,  3.4398e-13,  2.0259e-12,  ..., -8.6933e-12,\n",
       "             -4.9398e-13,  3.0542e-12],\n",
       "            [-3.7734e-12,  3.2406e-13,  2.0372e-12,  ..., -8.6580e-12,\n",
       "             -5.0137e-13,  3.0549e-12],\n",
       "            [-2.5589e-04, -2.6631e-04,  3.5744e-04,  ..., -2.7299e-04,\n",
       "             -2.7986e-05,  4.3303e-04],\n",
       "            ...,\n",
       "            [ 1.6426e-07, -1.2577e-08,  1.5526e-07,  ..., -8.2692e-08,\n",
       "             -3.3455e-08, -3.5690e-08],\n",
       "            [-4.8007e-07, -5.6863e-08,  7.4449e-07,  ..., -1.1423e-06,\n",
       "              2.5158e-07,  2.2255e-06],\n",
       "            [-2.7643e-10,  7.1683e-11,  4.2466e-10,  ..., -1.2167e-09,\n",
       "              5.6400e-12, -3.2598e-11]]),\n",
       "    'exp_avg_sq': tensor([[7.2328e-23, 9.7024e-23, 2.0079e-23,  ..., 2.2882e-22, 2.2004e-23,\n",
       "             1.1440e-21],\n",
       "            [7.1894e-23, 9.3039e-23, 1.9362e-23,  ..., 2.1985e-22, 2.3306e-23,\n",
       "             1.1878e-21],\n",
       "            [5.1345e-07, 1.4827e-07, 2.9473e-07,  ..., 6.6823e-07, 1.2297e-07,\n",
       "             1.0079e-06],\n",
       "            ...,\n",
       "            [4.5406e-11, 2.5253e-11, 3.3781e-12,  ..., 4.9049e-11, 4.7820e-12,\n",
       "             1.5411e-11],\n",
       "            [9.7251e-11, 3.7226e-11, 5.1970e-11,  ..., 6.7118e-11, 6.7436e-12,\n",
       "             8.7065e-11],\n",
       "            [1.9226e-14, 2.2493e-15, 1.5067e-15,  ..., 2.9897e-14, 4.9868e-17,\n",
       "             8.7012e-17]])},\n",
       "   17: {'step': 112320,\n",
       "    'exp_avg': tensor([ 4.7852e-11,  4.7808e-11,  9.3833e-04, -3.1162e-04, -1.2522e-03,\n",
       "             1.1011e-04,  2.1344e-04,  1.9960e-04, -1.6693e-04, -1.7083e-04,\n",
       "            -4.7626e-04, -1.2910e-04,  3.7449e-05,  4.8998e-04, -1.4090e-04,\n",
       "             5.3400e-04,  2.0682e-04, -4.2936e-04,  1.1153e-04,  2.7167e-04,\n",
       "             3.0925e-05,  1.3485e-06,  1.7428e-04, -2.6895e-04,  2.1023e-05,\n",
       "             5.7817e-06,  3.2566e-08,  1.6664e-07, -1.3588e-05,  2.4303e-06,\n",
       "             4.0350e-06,  1.1092e-06,  2.4004e-06,  3.6736e-06,  1.4834e-06,\n",
       "             2.1566e-09,  7.9101e-07, -2.6603e-06,  5.3280e-09]),\n",
       "    'exp_avg_sq': tensor([2.0816e-20, 2.1508e-20, 3.7509e-06, 1.4627e-06, 3.4262e-06, 1.2228e-06,\n",
       "            9.1883e-07, 1.1581e-06, 1.3052e-07, 2.4055e-07, 1.1543e-06, 2.8577e-06,\n",
       "            1.6051e-06, 2.0792e-06, 9.4546e-07, 2.0344e-06, 2.0675e-06, 1.1165e-06,\n",
       "            9.1178e-07, 5.5565e-07, 4.4839e-07, 2.8818e-07, 6.8612e-07, 3.3180e-07,\n",
       "            3.6686e-08, 1.6880e-09, 1.6846e-11, 3.7961e-11, 1.6108e-09, 3.2616e-10,\n",
       "            7.3830e-10, 2.4625e-10, 4.5900e-10, 4.1026e-10, 2.4164e-10, 5.1504e-13,\n",
       "            1.4254e-10, 4.0904e-10, 3.1034e-13])}},\n",
       "  'param_groups': [{'lr': 0.001,\n",
       "    'betas': (0.9, 0.999),\n",
       "    'eps': 1e-08,\n",
       "    'weight_decay': 0,\n",
       "    'amsgrad': False,\n",
       "    'maximize': False,\n",
       "    'params': [0,\n",
       "     1,\n",
       "     2,\n",
       "     3,\n",
       "     4,\n",
       "     5,\n",
       "     6,\n",
       "     7,\n",
       "     8,\n",
       "     9,\n",
       "     10,\n",
       "     11,\n",
       "     12,\n",
       "     13,\n",
       "     14,\n",
       "     15,\n",
       "     16,\n",
       "     17]}]},\n",
       " 'char_to_ix': {'<PAD>': 0,\n",
       "  '<UNK>': 1,\n",
       "  '<start>': 2,\n",
       "  '<end>': 3,\n",
       "  'เ': 4,\n",
       "  'ก': 5,\n",
       "  'ษ': 6,\n",
       "  'ม': 7,\n",
       "  ' ': 8,\n",
       "  'บ': 9,\n",
       "  'ุ': 10,\n",
       "  'ญ': 11,\n",
       "  'า': 12,\n",
       "  'ห': 13,\n",
       "  'ั': 14,\n",
       "  'น': 15,\n",
       "  'ิ': 16,\n",
       "  'โ': 17,\n",
       "  'ร': 18,\n",
       "  'ธ': 19,\n",
       "  'ศ': 20,\n",
       "  'ล': 21,\n",
       "  'พ': 22,\n",
       "  '่': 23,\n",
       "  'ท': 24,\n",
       "  'ย': 25,\n",
       "  '์': 26,\n",
       "  'ี': 27,\n",
       "  'ต': 28,\n",
       "  'อ': 29,\n",
       "  '็': 30,\n",
       "  'ง': 31,\n",
       "  'แ': 32,\n",
       "  'ซ': 33,\n",
       "  'ส': 34,\n",
       "  'ว': 35,\n",
       "  'ะ': 36,\n",
       "  'ำ': 37,\n",
       "  'ด': 38,\n",
       "  'จ': 39,\n",
       "  'ค': 40,\n",
       "  'ณ': 41,\n",
       "  'ฑ': 42,\n",
       "  'ึ': 43,\n",
       "  '้': 44,\n",
       "  'ู': 45,\n",
       "  'ฤ': 46,\n",
       "  'ฐ': 47,\n",
       "  'ข': 48,\n",
       "  'ฒ': 49,\n",
       "  'ฎ': 50,\n",
       "  'ป': 51,\n",
       "  'ไ': 52,\n",
       "  'ช': 53,\n",
       "  'ฏ': 54,\n",
       "  'ภ': 55,\n",
       "  'ฆ': 56,\n",
       "  'ฟ': 57,\n",
       "  'ฮ': 58,\n",
       "  'ื': 59,\n",
       "  'ผ': 60,\n",
       "  '๋': 61,\n",
       "  'ใ': 62,\n",
       "  '๊': 63,\n",
       "  'ถ': 64,\n",
       "  'ฌ': 65,\n",
       "  'ฉ': 66,\n",
       "  'ฝ': 67,\n",
       "  'ฬ': 68,\n",
       "  '.': 69,\n",
       "  'ฦ': 70,\n",
       "  '(': 71,\n",
       "  ')': 72,\n",
       "  'ฯ': 73,\n",
       "  '-': 74,\n",
       "  'ฃ': 75,\n",
       "  'ๆ': 76,\n",
       "  '2': 77,\n",
       "  'ๅ': 78,\n",
       "  'ฅ': 79,\n",
       "  'ฺ': 80,\n",
       "  'ํ': 81,\n",
       "  '5': 82,\n",
       "  '3': 83,\n",
       "  '6': 84,\n",
       "  '4': 85,\n",
       "  '7': 86,\n",
       "  '1': 87,\n",
       "  '0': 88,\n",
       "  '!': 89,\n",
       "  '9': 90,\n",
       "  '8': 91,\n",
       "  '\"': 92,\n",
       "  '๙': 93},\n",
       " 'ix_to_char': {0: '<PAD>',\n",
       "  1: '<UNK>',\n",
       "  2: '<start>',\n",
       "  3: '<end>',\n",
       "  4: 'เ',\n",
       "  5: 'ก',\n",
       "  6: 'ษ',\n",
       "  7: 'ม',\n",
       "  8: ' ',\n",
       "  9: 'บ',\n",
       "  10: 'ุ',\n",
       "  11: 'ญ',\n",
       "  12: 'า',\n",
       "  13: 'ห',\n",
       "  14: 'ั',\n",
       "  15: 'น',\n",
       "  16: 'ิ',\n",
       "  17: 'โ',\n",
       "  18: 'ร',\n",
       "  19: 'ธ',\n",
       "  20: 'ศ',\n",
       "  21: 'ล',\n",
       "  22: 'พ',\n",
       "  23: '่',\n",
       "  24: 'ท',\n",
       "  25: 'ย',\n",
       "  26: '์',\n",
       "  27: 'ี',\n",
       "  28: 'ต',\n",
       "  29: 'อ',\n",
       "  30: '็',\n",
       "  31: 'ง',\n",
       "  32: 'แ',\n",
       "  33: 'ซ',\n",
       "  34: 'ส',\n",
       "  35: 'ว',\n",
       "  36: 'ะ',\n",
       "  37: 'ำ',\n",
       "  38: 'ด',\n",
       "  39: 'จ',\n",
       "  40: 'ค',\n",
       "  41: 'ณ',\n",
       "  42: 'ฑ',\n",
       "  43: 'ึ',\n",
       "  44: '้',\n",
       "  45: 'ู',\n",
       "  46: 'ฤ',\n",
       "  47: 'ฐ',\n",
       "  48: 'ข',\n",
       "  49: 'ฒ',\n",
       "  50: 'ฎ',\n",
       "  51: 'ป',\n",
       "  52: 'ไ',\n",
       "  53: 'ช',\n",
       "  54: 'ฏ',\n",
       "  55: 'ภ',\n",
       "  56: 'ฆ',\n",
       "  57: 'ฟ',\n",
       "  58: 'ฮ',\n",
       "  59: 'ื',\n",
       "  60: 'ผ',\n",
       "  61: '๋',\n",
       "  62: 'ใ',\n",
       "  63: '๊',\n",
       "  64: 'ถ',\n",
       "  65: 'ฌ',\n",
       "  66: 'ฉ',\n",
       "  67: 'ฝ',\n",
       "  68: 'ฬ',\n",
       "  69: '.',\n",
       "  70: 'ฦ',\n",
       "  71: '(',\n",
       "  72: ')',\n",
       "  73: 'ฯ',\n",
       "  74: '-',\n",
       "  75: 'ฃ',\n",
       "  76: 'ๆ',\n",
       "  77: '2',\n",
       "  78: 'ๅ',\n",
       "  79: 'ฅ',\n",
       "  80: 'ฺ',\n",
       "  81: 'ํ',\n",
       "  82: '5',\n",
       "  83: '3',\n",
       "  84: '6',\n",
       "  85: '4',\n",
       "  86: '7',\n",
       "  87: '1',\n",
       "  88: '0',\n",
       "  89: '!',\n",
       "  90: '9',\n",
       "  91: '8',\n",
       "  92: '\"',\n",
       "  93: '๙'},\n",
       " 'target_char_to_ix': {'<PAD>': 0,\n",
       "  '<start>': 1,\n",
       "  '<end>': 2,\n",
       "  'k': 3,\n",
       "  'a': 4,\n",
       "  's': 5,\n",
       "  'e': 6,\n",
       "  'm': 7,\n",
       "  ' ': 8,\n",
       "  'b': 9,\n",
       "  'u': 10,\n",
       "  'n': 11,\n",
       "  'h': 12,\n",
       "  'i': 13,\n",
       "  'r': 14,\n",
       "  'o': 15,\n",
       "  't': 16,\n",
       "  'p': 17,\n",
       "  'g': 18,\n",
       "  'w': 19,\n",
       "  'l': 20,\n",
       "  'd': 21,\n",
       "  'c': 22,\n",
       "  'y': 23,\n",
       "  'f': 24,\n",
       "  '1': 25,\n",
       "  '(': 26,\n",
       "  ')': 27,\n",
       "  '2': 28,\n",
       "  '5': 29,\n",
       "  '3': 30,\n",
       "  '6': 31,\n",
       "  '4': 32,\n",
       "  '7': 33,\n",
       "  '0': 34,\n",
       "  '!': 35,\n",
       "  '9': 36,\n",
       "  '8': 37,\n",
       "  '\"': 38,\n",
       "  'v': 3,\n",
       "  'x': 4,\n",
       "  'z': 5,\n",
       "  'é': 6,\n",
       "  'q': 7,\n",
       "  \"'\": 8,\n",
       "  'j': 9,\n",
       "  'ฺ': 10,\n",
       "  'è': 11,\n",
       "  '岸': 12,\n",
       "  '田': 13,\n",
       "  '文': 14,\n",
       "  '雄': 15,\n",
       "  '.': 16,\n",
       "  '–': 17},\n",
       " 'ix_to_target_char': {0: '<PAD>',\n",
       "  1: '<start>',\n",
       "  2: '<end>',\n",
       "  3: 'v',\n",
       "  4: 'x',\n",
       "  5: 'z',\n",
       "  6: 'é',\n",
       "  7: 'q',\n",
       "  8: \"'\",\n",
       "  9: 'j',\n",
       "  10: 'ฺ',\n",
       "  11: 'è',\n",
       "  12: '岸',\n",
       "  13: '田',\n",
       "  14: '文',\n",
       "  15: '雄',\n",
       "  16: '.',\n",
       "  17: '–',\n",
       "  18: 'g',\n",
       "  19: 'w',\n",
       "  20: 'l',\n",
       "  21: 'd',\n",
       "  22: 'c',\n",
       "  23: 'y',\n",
       "  24: 'f',\n",
       "  25: '1',\n",
       "  26: '(',\n",
       "  27: ')',\n",
       "  28: '2',\n",
       "  29: '5',\n",
       "  30: '3',\n",
       "  31: '6',\n",
       "  32: '4',\n",
       "  33: '7',\n",
       "  34: '0',\n",
       "  35: '!',\n",
       "  36: '9',\n",
       "  37: '8',\n",
       "  38: '\"'},\n",
       " 'encoder_params': (94, 128, 256, 0.5),\n",
       " 'decoder_params': (39, 128, 256, 0.5)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "228b9ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['epoch', 'loss', 'model_state_dict', 'optimizer_state_dict', 'char_to_ix', 'ix_to_char', 'target_char_to_ix', 'ix_to_target_char', 'encoder_params', 'decoder_params'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ffda1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7568e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer.load_state_dict(data['optimizer_state_dict'])\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5aa34fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_ix = data['char_to_ix']\n",
    "ix_to_char = data['ix_to_char'] \n",
    "target_char_to_ix = data['target_char_to_ix']\n",
    "ix_to_target_char = data['ix_to_target_char']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6d59c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0bbf0f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68ba5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.config = {\"learning_rate\": learning_rate, \"epochs\": N_EPOCHS, \"batch_size\": BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fffaa1b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.watch(model, log_freq=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f0db625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip, teacher_forcing_ratio=0.5):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    for i, batch in tqdm(enumerate(iterator), total = len(iterator)):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        source_seq, source_seq_len = batch['input_tensor'], batch['input_length']\n",
    "        batch_size = source_seq.size(0)\n",
    "        \n",
    "        # target_seq: (batch_size , MAX_LENGTH)\n",
    "        # output: (MAX_LENGTH , batch_size , target_vocab_size)\n",
    "        target_seq = batch['target_tensor']\n",
    "\n",
    "        output = model(source_seq, source_seq_len, target_seq, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "        \n",
    "        # target_seq -> (MAX_LENGTH , batch_size)\n",
    "        target_seq = target_seq.transpose(0, 1)\n",
    "\n",
    "        # target_seq -> ((MAX_LENGTH - 1) * batch_size)\n",
    "        target_seq = target_seq[1:].contiguous().view(-1)\n",
    "\n",
    "        # output -> ((MAX_LENGTH -1) * batch_size, target_vocab_size)        \n",
    "        output = output[1:].view(-1, output.shape[-1])\n",
    "\n",
    "        loss = criterion(output, target_seq)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        wandb.log({\"train_loss\":loss.item()})\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "876ee1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char = thai_romanization_dataset.lang_th.char2index ,thai_romanization_dataset.lang_th.index2char , thai_romanization_dataset.lang_th_romanized.char2index ,  thai_romanization_dataset.lang_th_romanized.index2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ca115d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for i, batch in enumerate(iterator):\n",
    "\n",
    "            source_seq, source_seq_len = batch['input_tensor'], batch['input_length']\n",
    "            batch_size = source_seq.size(0)\n",
    "\n",
    "            # target_seq: (batch_size , MAX_LENGTH)\n",
    "            # output: (MAX_LENGTH , batch_size , target_vocab_size)\n",
    "            target_seq = batch['target_tensor']\n",
    "            output = model(source_seq, source_seq_len, target_seq)\n",
    "        \n",
    "            # target_seq -> (MAX_LENGTH , batch_size)\n",
    "            target_seq = target_seq.transpose(0, 1)\n",
    "\n",
    "            # target_seq -> ((MAX_LENGTH - 1) * batch_size)\n",
    "            target_seq = target_seq[1:].contiguous().view(-1)\n",
    "\n",
    "            # output -> ((MAX_LENGTH -1) * batch_size, target_vocab_size)        \n",
    "            output = output[1:].view(-1, output.shape[-1])\n",
    "\n",
    "            loss = criterion(output, target_seq)\n",
    "            wandb.log({\"evaluate_loss\":loss.item()})\n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "50ec5088",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, text, char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char):\n",
    "    model.eval()\n",
    "\n",
    "    input_seq =  [ch for ch in text] +  ['<end>']\n",
    "    numericalized = [char_2_ix[ch] for ch in input_seq] \n",
    "    \n",
    "#     print('input ',numericalized)\n",
    "    sentence_length = [len(numericalized)]\n",
    "\n",
    "    tensor = torch.LongTensor(numericalized).view(1, -1)\n",
    "    \n",
    "#     print(tensor)\n",
    "    translation_tensor_logits = model(tensor, sentence_length, None, 0) \n",
    "#     print(translation_tensor_logits)\n",
    "    try:\n",
    "        translation_tensor = torch.argmax(translation_tensor_logits.squeeze(1), 1).cpu().numpy()\n",
    "        translation_indices = [t for t in translation_tensor]\n",
    "        \n",
    "#         print('translation_tensor', translation_tensor)\n",
    "        translation = [ix_to_target_char[t] for t in translation_tensor]\n",
    "    except:\n",
    "        translation_indices = [0]\n",
    "        translation = ['<pad>']\n",
    "    return ''.join(translation), translation_indices\n",
    "\n",
    "def show_inference_example(model, input_texts, target_texts, char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char):\n",
    "    for index, input_text in enumerate(input_texts):\n",
    "        prediction, indices = inference(model, input_text, char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char)\n",
    "        print('groundtruth: {}'.format(target_texts[index]))\n",
    "        print(' prediction: {} {}\\n'.format(prediction, indices))\n",
    "        \n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "db295d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "groundtruth: \n",
      " prediction: 8xè [37, 4, 11]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_inference_example(model, ['การ'], [''], char_2_ix, ix_2_char, target_char_to_ix, ix_to_target_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "29a97d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIP = 5\n",
    "\n",
    "best_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f6339ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(1, N_EPOCHS+1)):\n",
    "#  model.train()\n",
    "#  start_time = time.time()\n",
    "#  train_loss = train(model, train_dataset_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=0.5)\n",
    "#  #valid_loss = evaluate(model, val_dataset_loader, criterion)\n",
    "#  end_time = time.time()\n",
    "#  save_model('new_model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)\n",
    "#  wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a43cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "45420ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model('new_model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a147a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ca7e70c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e1b373ec31e49fe875736643052f276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b12fa20703684e3aaee775b63ec2c082",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693868ac71494b0aa7363a6a4e4489bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e651ee54f7254fb48f1ce46be6ac5332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfcdee081844f24b6395a90018b0a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2356edf209a744948df9b9dbcf4e20e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save model at epoch  5\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, N_EPOCHS+1)):\n",
    " model.train()\n",
    " start_time = time.time()\n",
    " train_loss = train(model, train_dataset_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=0.5)\n",
    " #valid_loss = evaluate(model, val_dataset_loader, criterion)\n",
    " end_time = time.time()\n",
    " save_model('model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)\n",
    " wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cbd1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in tqdm(range(6,10+1)):\n",
    "#  model.train()\n",
    "#  start_time = time.time()\n",
    "#  train_loss = train(model, train_dataset_loader, optimizer, criterion, CLIP, teacher_forcing_ratio=0.5)\n",
    "#  #valid_loss = evaluate(model, val_dataset_loader, criterion)\n",
    "#  end_time = time.time()\n",
    "#  save_model('new_model/thai2rom-pytorch-%s.attn.v6' % str(epoch), epoch, train_loss, model)\n",
    "#  wandb.log({\"epoch\": epoch,\"epoch_train_loss\": train_loss,\"epoch_time\":end_time-start_time})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f2fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
